{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V3'\n","CODE_ENV = 'local' #'kaggle', 'aws', 'local'\n","TEST_END  = 1941 #1969 "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.393140Z","iopub.status.busy":"2024-01-22T13:02:34.392833Z","iopub.status.idle":"2024-01-22T13:02:47.056364Z","shell.execute_reply":"2024-01-22T13:02:47.055528Z","shell.execute_reply.started":"2024-01-22T13:02:34.393114Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Masking, RepeatVector, Dropout, Reshape\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","from keras import backend as K\n","from keras.callbacks import Callback\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n","False\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.373809Z","iopub.status.busy":"2024-01-22T13:02:34.373363Z","iopub.status.idle":"2024-01-22T13:02:34.379504Z","shell.execute_reply":"2024-01-22T13:02:34.378489Z","shell.execute_reply.started":"2024-01-22T13:02:34.373780Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v3/'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.381096Z","iopub.status.busy":"2024-01-22T13:02:34.380802Z","iopub.status.idle":"2024-01-22T13:02:34.390571Z","shell.execute_reply":"2024-01-22T13:02:34.389570Z","shell.execute_reply.started":"2024-01-22T13:02:34.381070Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","BASE      = prc_dir +'df_1.pkl'\n","CALENDAR  = prc_dir +'df_2.pkl'\n","NUM_ITEMS = 30490 # Number of items per each day\n","# Set time_steps for defining test, train and validation sets\n","DAYS_PER_SEQUENCE = 7  # Length of the sequence\n","target_col = 'sales_amount'"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:47.058053Z","iopub.status.busy":"2024-01-22T13:02:47.057502Z","iopub.status.idle":"2024-01-22T13:03:01.455370Z","shell.execute_reply":"2024-01-22T13:03:01.454363Z","shell.execute_reply.started":"2024-01-22T13:02:47.058028Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:01.457014Z","iopub.status.busy":"2024-01-22T13:03:01.456690Z","iopub.status.idle":"2024-01-22T13:03:09.962427Z","shell.execute_reply":"2024-01-22T13:03:09.961481Z","shell.execute_reply.started":"2024-01-22T13:03:01.456986Z"},"trusted":true},"outputs":[],"source":["# Define categorical and numerical columns\n","categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n","numerical_cols = ['sell_price']\n","\n","# Convert categorical columns to category dtype and encode with cat.codes\n","for col in categorical_cols:\n","    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","# Normalize numerical columns\n","scaler_numerical = MinMaxScaler()\n","df_all_data[numerical_cols] = scaler_numerical.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","\n","scaler_target = MinMaxScaler()\n","df_all_data[target_col] = scaler_target.fit_transform(df_all_data[[target_col]].astype(np.float64))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:09.970478Z","iopub.status.busy":"2024-01-22T13:03:09.970194Z","iopub.status.idle":"2024-01-22T13:03:18.370792Z","shell.execute_reply":"2024-01-22T13:03:18.369916Z","shell.execute_reply.started":"2024-01-22T13:03:09.970453Z"},"trusted":true},"outputs":[],"source":["# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation \n","\n","df_train = df_all_data[df_all_data['d'] < TRAIN_END].reset_index(drop=True)\n","df_val   = df_all_data[(df_all_data['d'] >= TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] < VAL_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","df_test  = df_all_data[(df_all_data['d'] >= VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] < TEST_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","\n","# Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","del df_all_data"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.372402Z","iopub.status.busy":"2024-01-22T13:03:18.372106Z","iopub.status.idle":"2024-01-22T13:03:18.386039Z","shell.execute_reply":"2024-01-22T13:03:18.385156Z","shell.execute_reply.started":"2024-01-22T13:03:18.372378Z"},"trusted":true},"outputs":[],"source":["def lstm_data_generator(df, num_features, target, days_sliding_window, batch_size, once_only_features, repeated_features):\n","    length_days = len(df) // NUM_ITEMS  # 1941 days\n","    while True:\n","        for i in range(0, length_days - days_sliding_window):\n","            start_ind = i * NUM_ITEMS\n","            end_ind = start_ind + NUM_ITEMS * (days_sliding_window)  # predict the next day after the sequence\n","\n","            # Extract once-only features for all days in the sequence at once\n","            once_features = df.iloc[start_ind:end_ind:NUM_ITEMS][once_only_features].to_numpy()\n","            # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","            # Extract repeated features for all items and days at once\n","            repeated_features_stack = df.iloc[start_ind:end_ind][repeated_features].to_numpy() # 210,000 items, 10 features\n","\n","            # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","            reshaped_3d = repeated_features_stack.reshape(days_sliding_window, NUM_ITEMS, len(repeated_features))\n","\n","            # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","            final_array = reshaped_3d.reshape(days_sliding_window, -1)\n","\n","            # Combine once-only and repeated features\n","            batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","            # Reshape batch_sequences to match LSTM input shape\n","            batch_sequences = batch_sequences.reshape(1, days_sliding_window, -1)\n","\n","            # Extract targets\n","            batch_targets = df.iloc[end_ind:end_ind + NUM_ITEMS][target].to_numpy()\n","\n","            # Yield the batch\n","            yield batch_sequences, batch_targets"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Initialize the generator\n","# repeated_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","repeated_features = ['sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","# once_only_features = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","once_only_features = ['snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","num_features = len(once_only_features) + len(repeated_features) * NUM_ITEMS # Calculate the number of features\n","\n","train_generator = lstm_data_generator(df_train, num_features, target_col, days_sliding_window=DAYS_PER_SEQUENCE, batch_size=1, once_only_features=once_only_features, repeated_features=repeated_features)\n","val_generator = lstm_data_generator(df_val, num_features, target_col, days_sliding_window=DAYS_PER_SEQUENCE, batch_size=1, once_only_features=once_only_features, repeated_features=repeated_features)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# For testing purposes: check how large on batch is\n","# next train_generator\n","# x, y = next(train_generator)\n","# # size of memory in mb of x and y\n","# print(x.nbytes / 1e6)\n","# print(y.nbytes / 1e6)\n","\n","# print(x.shape)\n","# print(y.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# # list all columns in df_train\n","# df_train.columns\n","\n","# # call head of df_train displaying all columns without truncation\n","# pd.set_option('display.max_columns', None)\n","# df_train.head()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.389636Z","iopub.status.busy":"2024-01-22T13:03:18.389317Z","iopub.status.idle":"2024-01-22T13:03:18.416778Z","shell.execute_reply":"2024-01-22T13:03:18.415805Z","shell.execute_reply.started":"2024-01-22T13:03:18.389597Z"},"trusted":true},"outputs":[],"source":["# Model parameters\n","epochs = 2\n","batch_size = 1"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.428419Z","iopub.status.busy":"2024-01-22T13:03:18.428151Z","iopub.status.idle":"2024-01-22T13:03:18.437168Z","shell.execute_reply":"2024-01-22T13:03:18.436284Z","shell.execute_reply.started":"2024-01-22T13:03:18.428395Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.438535Z","iopub.status.busy":"2024-01-22T13:03:18.438246Z","iopub.status.idle":"2024-01-22T13:03:19.715259Z","shell.execute_reply":"2024-01-22T13:03:19.714216Z","shell.execute_reply.started":"2024-01-22T13:03:18.438511Z"},"trusted":true},"outputs":[],"source":["# This is a sequence-to-sequence model: errors can propagate through the sequence\n","# model = Sequential()\n","\n","# model.add(LSTM(units=30,\n","#                activation='tanh', #relu\n","#                return_sequences=False,\n","#                stateful=True))\n","\n","# model.add(RepeatVector(28))\n","\n","# model.add(LSTM(units=30, \n","#                activation='tanh', \n","#                return_sequences=True, \n","#                stateful=True))\n","\n","# model.add(Dense(units=1))\n","\n","# model.compile(optimizer='adam', loss='mean_squared_error')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-03 18:16:54.874323: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-02-03 18:16:54.875110: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-02-03 18:16:54.875862: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n","2024-02-03 18:16:54.957905: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-02-03 18:16:54.958377: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-02-03 18:16:54.958827: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"]}],"source":["# Neu: Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","model = Sequential()\n","\n","# First LSTM layer with more units and return sequences\n","model.add(LSTM(units=50, \n","               activation='tanh', \n","               return_sequences=True, \n","               stateful=True,\n","               input_shape=(DAYS_PER_SEQUENCE, num_features),\n","               batch_input_shape=(batch_size, DAYS_PER_SEQUENCE, num_features)))\n","model.add(Dropout(0.2))\n","\n","# Additional LSTM Layer\n","model.add(LSTM(units=30, \n","               activation='tanh', \n","               return_sequences=False, \n","               stateful=True))\n","model.add(Dropout(0.2))\n","\n","# Dense layer\n","model.add(Dense(units=80, \n","                activation='relu'))\n","\n","# Final Dense layer for output\n","model.add(Dense(units=NUM_ITEMS))\n","\n","# Reshape the output to be (number of items)\n","model.add(Reshape((NUM_ITEMS,))) # Eigentlich müsste das vorherige Layer bereits die richtige Form haben\n","\n","model.compile(optimizer='adam', \n","              loss='mse', \n","              metrics=[RootMeanSquaredError()])"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (1, 7, 50)                18304800  \n","                                                                 \n"," dropout (Dropout)           (1, 7, 50)                0         \n","                                                                 \n"," lstm_1 (LSTM)               (1, 30)                   9720      \n","                                                                 \n"," dropout_1 (Dropout)         (1, 30)                   0         \n","                                                                 \n"," dense (Dense)               (1, 80)                   2480      \n","                                                                 \n"," dense_1 (Dense)             (1, 30490)                2469690   \n","                                                                 \n"," reshape (Reshape)           (1, 30490)                0         \n","                                                                 \n","=================================================================\n","Total params: 20,786,690\n","Trainable params: 20,786,690\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# For tracking purposes: check the models parameters\n","model.summary()\n","\n","# Print input shape of the layers\n","# for layer in model.layers:\n","#     print(layer.input_shape)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:19.716971Z","iopub.status.busy":"2024-01-22T13:03:19.716580Z","iopub.status.idle":"2024-01-22T13:03:19.722552Z","shell.execute_reply":"2024-01-22T13:03:19.721412Z","shell.execute_reply.started":"2024-01-22T13:03:19.716930Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T21:41:47.956270Z","iopub.status.busy":"2024-01-22T21:41:47.955997Z","iopub.status.idle":"2024-01-22T21:41:48.300127Z","shell.execute_reply":"2024-01-22T21:41:48.298879Z","shell.execute_reply.started":"2024-01-22T21:41:47.956244Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["2024-02-03 18:16:55.040063: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n","\t [[{{node Placeholder/_0}}]]\n","2024-02-03 18:16:55.064405: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n","2024-02-03 18:16:55.211099: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-02-03 18:16:55.211980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-02-03 18:16:55.212707: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n","2024-02-03 18:16:55.309476: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-02-03 18:16:55.310638: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-02-03 18:16:55.311559: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n","2024-02-03 18:16:55.670563: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-02-03 18:16:55.671318: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-02-03 18:16:55.672011: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n","2024-02-03 18:16:55.759518: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-02-03 18:16:55.760669: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-02-03 18:16:55.761302: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"]},{"name":"stdout","output_type":"stream","text":[" 166/1878 [=>............................] - ETA: 1:49:54 - loss: 2.4242e-05 - root_mean_squared_error: 0.0049"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mtrain_generator,\n\u001b[1;32m      3\u001b[0m           steps_per_epoch\u001b[38;5;241m=\u001b[39mTRAIN_DUR,  \u001b[38;5;66;03m# total number of sequences in the training set\u001b[39;00m\n\u001b[1;32m      4\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39mval_generator,\n\u001b[1;32m      5\u001b[0m           validation_steps\u001b[38;5;241m=\u001b[39mVAL_DUR,  \u001b[38;5;66;03m# total number of sequences in the validation set\u001b[39;00m\n\u001b[1;32m      6\u001b[0m           epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m      7\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39m[ResetStatesCallback()])\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Training the model\n","history = model.fit(x=train_generator,\n","          steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","          validation_data=val_generator,\n","          validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","          epochs=epochs,\n","          callbacks=[ResetStatesCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train and validation df not needed anymore\n","del df_train\n","del df_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:30:39.354772Z","iopub.status.busy":"2024-01-18T14:30:39.353690Z","iopub.status.idle":"2024-01-18T14:30:39.384792Z","shell.execute_reply":"2024-01-18T14:30:39.383711Z","shell.execute_reply.started":"2024-01-18T14:30:39.354715Z"},"trusted":true},"outputs":[],"source":["# Save the model to a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')\n","    \n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model.save('/kaggle/working/' + MODEL_NAME + '.h5')\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Start from here if you want to load the model\n","from keras.models import load_model\n","\n","# Load the model from a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model = load_model(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model = load_model('/kaggle/input/v1-model/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:00.674762Z","iopub.status.busy":"2024-01-18T14:34:00.673913Z","iopub.status.idle":"2024-01-18T14:34:00.949572Z","shell.execute_reply":"2024-01-18T14:34:00.948466Z","shell.execute_reply.started":"2024-01-18T14:34:00.674717Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# now we need a generator for the test set\n","def lstm_data_generator_test(df, num_features, target, days_sliding_window, batch_size, once_only_features, repeated_features):\n","    length_days = len(df) // NUM_ITEMS  # 35 days\n","    while True:\n","        for i in range(0, length_days - days_sliding_window): # Stop after 28 days because we start from the 7th day (because of the sliding window)\n","            start_ind = i * NUM_ITEMS\n","            end_ind = start_ind + NUM_ITEMS * (days_sliding_window)  # predict the next day after the sequence\n","\n","            # Extract once-only features for all days in the sequence at once\n","            once_features = df.iloc[start_ind:end_ind:NUM_ITEMS][once_only_features].to_numpy()\n","            # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","            # Extract repeated features for all items and days at once\n","            repeated_features_stack = df.iloc[start_ind:end_ind][repeated_features].to_numpy() # 210,000 items, 10 features\n","\n","            # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","            reshaped_3d = repeated_features_stack.reshape(days_sliding_window, NUM_ITEMS, len(repeated_features))\n","\n","            # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","            final_array = reshaped_3d.reshape(days_sliding_window, -1)\n","\n","            # Combine once-only and repeated features\n","            batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","            # Reshape batch_sequences to match LSTM input shape\n","            batch_sequences = batch_sequences.reshape(1, days_sliding_window, -1)\n","\n","            # Extract targets\n","            batch_targets = df.iloc[end_ind:end_ind + NUM_ITEMS][target].to_numpy()\n","\n","            # Yield the batch\n","            yield batch_sequences, batch_targets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x, y = next(train_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction_normalized = model.predict(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction_normalized"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction_original = scaler_target.inverse_transform(prediction_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction_original"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict_next_day(model, last_window_data, num_features):\n","    # Predict the next day\n","    next_day_prediction = model.predict(last_window_data.reshape(1, 7, num_features))\n","    return next_day_prediction\n","\n","# Assuming you have a function to update your data with new predictions\n","def update_data_with_prediction(data, new_prediction):\n","    # Logic to update your dataset with the new_prediction\n","    # This could involve shifting the window and inserting the new prediction\n","    pass\n","\n","# Starting with actual historical data\n","last_window_data = get_last_window_of_actual_data()  # Shape: (7, num_features)\n","\n","for i in range(num_future_days):\n","    # Predict the next day\n","    next_day_prediction = predict_next_day(model, last_window_data, num_features)\n","\n","    # Update your data with this new prediction\n","    last_window_data = update_data_with_prediction(last_window_data, next_day_prediction)\n","\n","    # Now last_window_data contains the most recent prediction, which will be used in the next iteration\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def prepare_forecast_input(df, DAYS_PER_SEQUENCE, num_items):\n","#     #df_test starts at 1942-7 which we need take into account\n","#     # Prepare input data for forecasting\n","#     forecast_input = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx].drop('sales_amount', axis=1).to_numpy()\n","#         forecast_input.append(sequence)\n","#     return np.array(forecast_input)\n","\n","\n","# Custom function for input to prepare forecasts input for model\n","# def prepare_forecast_input(df, target, model, DAYS_PER_SEQUENCE, num_items):\n","#     forecast_output = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx, : ].drop(target, axis=1).to_numpy()\n","#         # forecast_output.append(model.predict(sequence))\n","#         forecast_output.append(model.predict(sequence.reshape(1, sequence.shape[0], sequence.shape[1])))\n","#     return np.array(forecast_output)#.reshape(-1, 1)\n","# forecast_output = prepare_forecast_input(df_test, target_col, model, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","#forecasts_original = scaler.inverse_transform(forecast_output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming df_all_data contains all data up to day 1941\n","# forecast_input = prepare_forecast_input(df_test, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","\n","# Generate forecasts\n","# forecasts = model.predict(forecast_input)\n","# forecasts_original = scaler.inverse_transform(forecasts)\n","\n","# forecasts_original now contains the predicted sales amounts for days 1942 to 1969\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:26.857391Z","iopub.status.busy":"2024-01-18T14:34:26.857016Z","iopub.status.idle":"2024-01-18T14:34:27.273805Z","shell.execute_reply":"2024-01-18T14:34:27.272980Z","shell.execute_reply.started":"2024-01-18T14:34:26.857362Z"},"trusted":true},"outputs":[],"source":["# Prepare input for forecasts\n","# I cannot use the custom lstm_data_generator\n","# Prepare 7 day slices each shifted by one day\n","def prepare_forecast_input(df, DAYS_PER_SEQUENCE, target_col):\n","    forecast_input = []\n","    for i in range(0, len(df)//NUM_ITEMS): #i=0; 1, 2, 3, ..., 35?\n","        if i + DAYS_PER_SEQUENCE < (len(df)-1)//NUM_ITEMS: #7, 8, 9, 10, ...\n","            start_idx = i*NUM_ITEMS\n","            end_idx   = start_idx + NUM_ITEMS * DAYS_PER_SEQUENCE\n","            sequence  = df.iloc[start_idx : end_idx, :].drop(target_col, axis=1).to_numpy()\n","            forecast_input.append(sequence)\n","    return np.array(forecast_input)\n","\n","predict_array = prepare_forecast_input(df=df_test, DAYS_PER_SEQUENCE=DAYS_PER_SEQUENCE, target_col=target_col)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:29.975040Z","iopub.status.busy":"2024-01-18T14:34:29.974179Z","iopub.status.idle":"2024-01-18T14:34:29.981976Z","shell.execute_reply":"2024-01-18T14:34:29.981017Z","shell.execute_reply.started":"2024-01-18T14:34:29.974993Z"},"trusted":true},"outputs":[],"source":["predict_array.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:35:58.035250Z","iopub.status.busy":"2024-01-18T14:35:58.034255Z","iopub.status.idle":"2024-01-18T14:36:03.405105Z","shell.execute_reply":"2024-01-18T14:36:03.404120Z","shell.execute_reply.started":"2024-01-18T14:35:58.035208Z"},"trusted":true},"outputs":[],"source":["forecast_normalized = model.predict(predict_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:36:45.205555Z","iopub.status.busy":"2024-01-18T14:36:45.205169Z","iopub.status.idle":"2024-01-18T14:36:45.210651Z","shell.execute_reply":"2024-01-18T14:36:45.209587Z","shell.execute_reply.started":"2024-01-18T14:36:45.205524Z"},"trusted":true},"outputs":[],"source":["forecasts_original = scaler.inverse_transform(forecast_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:37:28.194159Z","iopub.status.busy":"2024-01-18T14:37:28.193718Z","iopub.status.idle":"2024-01-18T14:37:28.200519Z","shell.execute_reply":"2024-01-18T14:37:28.199591Z","shell.execute_reply.started":"2024-01-18T14:37:28.194120Z"},"trusted":true},"outputs":[],"source":["forecasts_original.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, n):\n","    test_gen = lstm_data_generator(df_test, target_col, DAYS_PER_SEQUENCE, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","# evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, VAL_END)"]},{"cell_type":"markdown","metadata":{},"source":["- TPU nutzen und direkt aufrufen\n","- mutiprocessing\n","- use tensorflow dataset\n","- gpu nutzen (CUDA aufrufen)\n","- ConvLSTM1D layer: https://keras.io/api/layers/recurrent_layers/conv_lstm1d/\n","- https://www.kaggle.com/code/li325040229/eda-and-an-encoder-decoder-lstm-with-9-features/notebook#Build-a-LSTM-Model-\n","- Wie zum laufen bekommen?:\n","-   <b>Encoder-Decoder Model</b> --> https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243\n","-   https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243 --> 30490 als batch input nutzen, dann aber Problem, dass scheinbar nur Abhängigkeiten von einem auf den anderen Tag getrackt werden und keine Muster zwischen Zeitsequenzen gefunden werden können\n","- Herangehensweise:\n","    - Develop one model per site.\n","    -  Develop one model per group of sites.\n","    -  Develop one model for all sites.\n","\n","\n","<br>\n","\n","- Progress bars mit tqdm anzeigen\n","- Test, Validierung und Trainingzeitraum sollten sich nicht überlappen, ist aber ggf. der Fall?\n","- Ggf. zu float16 konvertieren checken, ob finaler df mit time slices dann deutlich kleiner und performance testen\n","- column 'd' in training df löschen?\n","- paralletl computing einstellen\n","- use_multiprocessing in keras auf true setzen (model.fit agument)\n","- Cross validation?\n","- Ensemble learning?\n","- brauche ich one-hot encoding für categorical features?\n","- Things to consider:\n","- dropout\n","- seed\n","- learning rate\n","- loss function\n","- optimizer\n","- metrics\n","- batch size\n","- epochs\n","- Add CNN layer\n","- model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(DAYS_PER_SEQUENCE, num_features)))\n","- model.add(MaxPooling1D(pool_size=2))\n","- model.add(Flatten())\n","- model.add(LSTM(50, activation='relu'))\n","- model.add(Dense(1)) / or more layers as needed\n","- model.compile()"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
