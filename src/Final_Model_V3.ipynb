{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V3'\n","CODE_ENV = 'aws' #'kaggle', 'aws', 'local'\n","TEST_END  = 1941 #1969 "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.393140Z","iopub.status.busy":"2024-01-22T13:02:34.392833Z","iopub.status.idle":"2024-01-22T13:02:47.056364Z","shell.execute_reply":"2024-01-22T13:02:47.055528Z","shell.execute_reply.started":"2024-01-22T13:02:34.393114Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_13248/3405039513.py:2: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd\n","2024-02-02 20:35:43.826586: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-02-02 20:35:44.782138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Masking, RepeatVector, Dropout, Reshape\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","from keras import backend as K\n","from keras.callbacks import Callback\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n","True\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.373809Z","iopub.status.busy":"2024-01-22T13:02:34.373363Z","iopub.status.idle":"2024-01-22T13:02:34.379504Z","shell.execute_reply":"2024-01-22T13:02:34.378489Z","shell.execute_reply.started":"2024-01-22T13:02:34.373780Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v3/'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.381096Z","iopub.status.busy":"2024-01-22T13:02:34.380802Z","iopub.status.idle":"2024-01-22T13:02:34.390571Z","shell.execute_reply":"2024-01-22T13:02:34.389570Z","shell.execute_reply.started":"2024-01-22T13:02:34.381070Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","BASE      = prc_dir +'df_1.pkl'\n","CALENDAR  = prc_dir +'df_2.pkl'\n","NUM_ITEMS = 30490 # Number of items per each day\n","# Set time_steps for defining test, train and validation sets\n","DAYS_PER_SEQUENCE = 7  # Length of the sequence\n","target_col = 'sales_amount'"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:47.058053Z","iopub.status.busy":"2024-01-22T13:02:47.057502Z","iopub.status.idle":"2024-01-22T13:03:01.455370Z","shell.execute_reply":"2024-01-22T13:03:01.454363Z","shell.execute_reply.started":"2024-01-22T13:02:47.058028Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:01.457014Z","iopub.status.busy":"2024-01-22T13:03:01.456690Z","iopub.status.idle":"2024-01-22T13:03:09.962427Z","shell.execute_reply":"2024-01-22T13:03:09.961481Z","shell.execute_reply.started":"2024-01-22T13:03:01.456986Z"},"trusted":true},"outputs":[],"source":["# Define categorical and numerical columns\n","categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n","numerical_cols = ['sell_price']\n","\n","# Convert categorical columns to category dtype and encode with cat.codes\n","for col in categorical_cols:\n","    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","# Normalize numerical columns\n","scaler = MinMaxScaler()\n","df_all_data[numerical_cols] = scaler.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","df_all_data[target_col] = scaler.fit_transform(df_all_data[[target_col]].astype(np.float32))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:09.970478Z","iopub.status.busy":"2024-01-22T13:03:09.970194Z","iopub.status.idle":"2024-01-22T13:03:18.370792Z","shell.execute_reply":"2024-01-22T13:03:18.369916Z","shell.execute_reply.started":"2024-01-22T13:03:09.970453Z"},"trusted":true},"outputs":[],"source":["# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation \n","\n","df_train = df_all_data[df_all_data['d'] < TRAIN_END].reset_index(drop=True)\n","df_val   = df_all_data[(df_all_data['d'] >= TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] < VAL_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","df_test  = df_all_data[(df_all_data['d'] >= VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] < TEST_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","\n","# Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","del df_all_data"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.372402Z","iopub.status.busy":"2024-01-22T13:03:18.372106Z","iopub.status.idle":"2024-01-22T13:03:18.386039Z","shell.execute_reply":"2024-01-22T13:03:18.385156Z","shell.execute_reply.started":"2024-01-22T13:03:18.372378Z"},"trusted":true},"outputs":[],"source":["# def lstm_data_generator(df, num_features, target, days_sliding_window, batch_size, once_only_features, repeated_features):\n","#     while True:\n","#         length_days = len(df) // NUM_ITEMS #1941 days\n","#         for i in range(0, length_days - days_sliding_window):\n","#             batch_sequences = []\n","#             batch_targets = []\n","\n","#             start_ind = i * NUM_ITEMS\n","#             end_ind = start_ind + NUM_ITEMS * (days_sliding_window + 1) # predict the next day after the sequence\n","\n","#             # Initialize an array to hold the features for each day\n","#             day_sequences = np.zeros((days_sliding_window, num_features))\n","\n","#             # Loop over each day in the sequence\n","#             for day_idx in range(days_sliding_window):\n","#                 # Extract once-only features (assumed to be the same for all items)\n","#                 once_features = df.iloc[start_ind + day_idx * NUM_ITEMS][once_only_features].to_numpy() #müsste damit 7 Einträge haben\n","\n","#                 # Start filling the day_sequences with once-only features\n","#                 day_sequences[day_idx, :len(once_only_features)] = once_features\n","\n","#                 # Loop over each item for repeated features\n","#                 for item_idx in range(NUM_ITEMS):\n","#                     item_start_ind = start_ind + item_idx + day_idx * NUM_ITEMS\n","#                     item_features = df.iloc[item_start_ind:item_start_ind + 1][repeated_features].to_numpy().flatten()\n","\n","#                     # Calculate the start index in the day_sequences array for this item's features\n","#                     features_start_idx = len(once_only_features) + item_idx * len(repeated_features)\n","\n","#                     # Place the item's features in the day_sequences array\n","#                     day_sequences[day_idx, features_start_idx:features_start_idx + len(repeated_features)] = item_features\n","\n","#             # Add the sequence for this window to batch_sequences\n","#             batch_sequences.append(day_sequences)\n","\n","#             # Extract and add the target for this window\n","#             batch_target = df.iloc[end_ind:end_ind + NUM_ITEMS][target].to_numpy().flatten()\n","#             batch_targets.append(batch_target)\n","\n","#             # Yield the batch\n","#             yield np.array(batch_sequences), np.array(batch_targets)\n","\n","\n","\n","def lstm_data_generator(df, num_features, target, days_sliding_window, batch_size, once_only_features, repeated_features):\n","    length_days = len(df) // NUM_ITEMS  # 1941 days\n","    while True:\n","        for i in range(0, length_days - days_sliding_window):\n","            start_ind = i * NUM_ITEMS\n","            end_ind = start_ind + NUM_ITEMS * (days_sliding_window)  # predict the next day after the sequence\n","\n","            # Extract once-only features for all days in the sequence at once\n","            once_features = df.iloc[start_ind:end_ind:NUM_ITEMS][once_only_features].to_numpy()\n","            # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","            # Extract repeated features for all items and days at once\n","            repeated_features_stack = df.iloc[start_ind:end_ind][repeated_features].to_numpy() # 210,000 items, 10 features\n","\n","            # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","            reshaped_3d = repeated_features_stack.reshape(days_sliding_window, NUM_ITEMS, len(repeated_features))\n","\n","            # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","            final_array = reshaped_3d.reshape(days_sliding_window, -1)\n","\n","            # Combine once-only and repeated features\n","            batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","            # Reshape batch_sequences to match LSTM input shape\n","            batch_sequences = batch_sequences.reshape(1, days_sliding_window, -1)\n","\n","            # Extract targets\n","            batch_targets = df.iloc[end_ind:end_ind + NUM_ITEMS][target].to_numpy()\n","\n","            # Yield the batch\n","            yield batch_sequences, batch_targets"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Initialize the generator\n","repeated_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","once_only_features = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","num_features = len(once_only_features) + len(repeated_features) * NUM_ITEMS # Calculate the number of features\n","\n","train_generator = lstm_data_generator(df_train, num_features, target_col, days_sliding_window=DAYS_PER_SEQUENCE, batch_size=1, once_only_features=once_only_features, repeated_features=repeated_features)\n","val_generator = lstm_data_generator(df_val, num_features, target_col, days_sliding_window=DAYS_PER_SEQUENCE, batch_size=1, once_only_features=once_only_features, repeated_features=repeated_features)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["28\n"]}],"source":["length_days = len(df_val) // NUM_ITEMS  # 1941 days\n","print(length_days - 7)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# next iter(generator_train) to see what it looks like\n","# generator_train_out = next(generator_train)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# For testing purposes: check how large on batch is\n","# next train_generator\n","# x, y = next(train_generator)\n","# # size of memory in mb of x and y\n","# print(x.nbytes / 1e6)\n","# print(y.nbytes / 1e6)\n","\n","# print(x.shape)\n","# print(y.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# # list all columns in df_train\n","# df_train.columns\n","\n","# # call head of df_train displaying all columns without truncation\n","# pd.set_option('display.max_columns', None)\n","# df_train.head()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.389636Z","iopub.status.busy":"2024-01-22T13:03:18.389317Z","iopub.status.idle":"2024-01-22T13:03:18.416778Z","shell.execute_reply":"2024-01-22T13:03:18.415805Z","shell.execute_reply.started":"2024-01-22T13:03:18.389597Z"},"trusted":true},"outputs":[],"source":["# Model parameters\n","epochs = 2\n","batch_size = 1"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.428419Z","iopub.status.busy":"2024-01-22T13:03:18.428151Z","iopub.status.idle":"2024-01-22T13:03:18.437168Z","shell.execute_reply":"2024-01-22T13:03:18.436284Z","shell.execute_reply.started":"2024-01-22T13:03:18.428395Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.438535Z","iopub.status.busy":"2024-01-22T13:03:18.438246Z","iopub.status.idle":"2024-01-22T13:03:19.715259Z","shell.execute_reply":"2024-01-22T13:03:19.714216Z","shell.execute_reply.started":"2024-01-22T13:03:18.438511Z"},"trusted":true},"outputs":[],"source":["# This is a sequence-to-sequence model: errors can propagate through the sequence\n","# model = Sequential()\n","\n","# model.add(LSTM(units=30,\n","#                activation='tanh', #relu\n","#                return_sequences=False,\n","#                stateful=True))\n","\n","# model.add(RepeatVector(28))\n","\n","# model.add(LSTM(units=30, \n","#                activation='tanh', \n","#                return_sequences=True, \n","#                stateful=True))\n","\n","# model.add(Dense(units=1))\n","\n","# model.compile(optimizer='adam', loss='mean_squared_error')"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-02 20:36:11.306489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14606 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n"]}],"source":["# Neu: Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","model = Sequential()\n","\n","# First LSTM layer with more units and return sequences\n","model.add(LSTM(units=50, \n","               activation='tanh', \n","               return_sequences=True, \n","               stateful=True,\n","               input_shape=(DAYS_PER_SEQUENCE, num_features),\n","               batch_input_shape=(batch_size, DAYS_PER_SEQUENCE, num_features)))\n","model.add(Dropout(0.2))\n","\n","# Additional LSTM Layer\n","model.add(LSTM(units=30, \n","               activation='tanh', \n","               return_sequences=False, \n","               stateful=True))\n","model.add(Dropout(0.2))\n","\n","# Dense layer\n","model.add(Dense(units=80, \n","                activation='relu'))\n","\n","# Final Dense layer for output\n","model.add(Dense(units=NUM_ITEMS))\n","\n","# Reshape the output to be (number of items)\n","model.add(Reshape((NUM_ITEMS,))) # Eigentlich müsste das vorherige Layer bereits die richtige Form haben\n","\n","model.compile(optimizer='adam', \n","              loss='mse', \n","              metrics=[RootMeanSquaredError()])"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (1, 7, 50)                48796800  \n","                                                                 \n"," dropout (Dropout)           (1, 7, 50)                0         \n","                                                                 \n"," lstm_1 (LSTM)               (1, 30)                   9720      \n","                                                                 \n"," dropout_1 (Dropout)         (1, 30)                   0         \n","                                                                 \n"," dense (Dense)               (1, 80)                   2480      \n","                                                                 \n"," dense_1 (Dense)             (1, 30490)                2469690   \n","                                                                 \n"," reshape (Reshape)           (1, 30490)                0         \n","                                                                 \n","=================================================================\n","Total params: 51278690 (195.61 MB)\n","Trainable params: 51278690 (195.61 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# For tracking purposes: check the models parameters\n","model.summary()\n","\n","# Print input shape of the layers\n","# for layer in model.layers:\n","#     print(layer.input_shape)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:19.716971Z","iopub.status.busy":"2024-01-22T13:03:19.716580Z","iopub.status.idle":"2024-01-22T13:03:19.722552Z","shell.execute_reply":"2024-01-22T13:03:19.721412Z","shell.execute_reply.started":"2024-01-22T13:03:19.716930Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T21:41:47.956270Z","iopub.status.busy":"2024-01-22T21:41:47.955997Z","iopub.status.idle":"2024-01-22T21:41:48.300127Z","shell.execute_reply":"2024-01-22T21:41:48.298879Z","shell.execute_reply.started":"2024-01-22T21:41:47.956244Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["2024-02-02 20:36:16.104628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8903\n","2024-02-02 20:36:16.384988: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5af8017b10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2024-02-02 20:36:16.385032: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n","2024-02-02 20:36:16.392055: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2024-02-02 20:36:16.554537: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["   6/1878 [..............................] - ETA: 2:18 - loss: 8.1931e-05 - root_mean_squared_error: 0.0091WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0225s vs `on_train_batch_end` time: 0.0429s). Check your callbacks.\n","1878/1878 [==============================] - 146s 75ms/step - loss: 2.6232e-05 - root_mean_squared_error: 0.0051 - val_loss: 2.2208e-05 - val_root_mean_squared_error: 0.0047\n","Epoch 2/2\n","1878/1878 [==============================] - 140s 74ms/step - loss: 2.5772e-05 - root_mean_squared_error: 0.0051 - val_loss: 2.2220e-05 - val_root_mean_squared_error: 0.0047\n"]}],"source":["# Training the model\n","history = model.fit(x=train_generator,\n","          steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","          validation_data=val_generator,\n","          validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","          epochs=epochs,\n","          callbacks=[ResetStatesCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train and validation df not needed anymore\n","del df_train\n","del df_val"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:30:39.354772Z","iopub.status.busy":"2024-01-18T14:30:39.353690Z","iopub.status.idle":"2024-01-18T14:30:39.384792Z","shell.execute_reply":"2024-01-18T14:30:39.383711Z","shell.execute_reply.started":"2024-01-18T14:30:39.354715Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ubuntu/projects/Kaggle_M5PointPrediction/env/final_bsc/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["# Save the model to a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')\n","    \n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model.save('/kaggle/working/' + MODEL_NAME + '.h5')\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Start from here if you want to load the model\n","from keras.models import load_model\n","\n","# Load the model from a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model = load_model(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model = load_model('/kaggle/input/v1-model/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:00.674762Z","iopub.status.busy":"2024-01-18T14:34:00.673913Z","iopub.status.idle":"2024-01-18T14:34:00.949572Z","shell.execute_reply":"2024-01-18T14:34:00.948466Z","shell.execute_reply.started":"2024-01-18T14:34:00.674717Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def prepare_forecast_input(df, DAYS_PER_SEQUENCE, num_items):\n","#     #df_test starts at 1942-7 which we need take into account\n","#     # Prepare input data for forecasting\n","#     forecast_input = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx].drop('sales_amount', axis=1).to_numpy()\n","#         forecast_input.append(sequence)\n","#     return np.array(forecast_input)\n","\n","\n","# Custom function for input to prepare forecasts input for model\n","# def prepare_forecast_input(df, target, model, DAYS_PER_SEQUENCE, num_items):\n","#     forecast_output = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx, : ].drop(target, axis=1).to_numpy()\n","#         # forecast_output.append(model.predict(sequence))\n","#         forecast_output.append(model.predict(sequence.reshape(1, sequence.shape[0], sequence.shape[1])))\n","#     return np.array(forecast_output)#.reshape(-1, 1)\n","# forecast_output = prepare_forecast_input(df_test, target_col, model, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","#forecasts_original = scaler.inverse_transform(forecast_output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming df_all_data contains all data up to day 1941\n","# forecast_input = prepare_forecast_input(df_test, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","\n","# Generate forecasts\n","# forecasts = model.predict(forecast_input)\n","# forecasts_original = scaler.inverse_transform(forecasts)\n","\n","# forecasts_original now contains the predicted sales amounts for days 1942 to 1969\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:26.857391Z","iopub.status.busy":"2024-01-18T14:34:26.857016Z","iopub.status.idle":"2024-01-18T14:34:27.273805Z","shell.execute_reply":"2024-01-18T14:34:27.272980Z","shell.execute_reply.started":"2024-01-18T14:34:26.857362Z"},"trusted":true},"outputs":[],"source":["# Prepare input for forecasts\n","# I cannot use the custom lstm_data_generator\n","# Prepare 7 day slices each shifted by one day\n","def prepare_forecast_input(df, DAYS_PER_SEQUENCE, target_col):\n","    forecast_input = []\n","    for i in range(0, len(df)//NUM_ITEMS): #i=0; 1, 2, 3, ..., 35?\n","        if i + DAYS_PER_SEQUENCE < (len(df)-1)//NUM_ITEMS: #7, 8, 9, 10, ...\n","            start_idx = i*NUM_ITEMS\n","            end_idx   = start_idx + NUM_ITEMS * DAYS_PER_SEQUENCE\n","            sequence  = df.iloc[start_idx : end_idx, :].drop(target_col, axis=1).to_numpy()\n","            forecast_input.append(sequence)\n","    return np.array(forecast_input)\n","\n","predict_array = prepare_forecast_input(df=df_test, DAYS_PER_SEQUENCE=DAYS_PER_SEQUENCE, target_col=target_col)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:29.975040Z","iopub.status.busy":"2024-01-18T14:34:29.974179Z","iopub.status.idle":"2024-01-18T14:34:29.981976Z","shell.execute_reply":"2024-01-18T14:34:29.981017Z","shell.execute_reply.started":"2024-01-18T14:34:29.974993Z"},"trusted":true},"outputs":[],"source":["predict_array.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:35:58.035250Z","iopub.status.busy":"2024-01-18T14:35:58.034255Z","iopub.status.idle":"2024-01-18T14:36:03.405105Z","shell.execute_reply":"2024-01-18T14:36:03.404120Z","shell.execute_reply.started":"2024-01-18T14:35:58.035208Z"},"trusted":true},"outputs":[],"source":["forecast_normalized = model.predict(predict_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:36:45.205555Z","iopub.status.busy":"2024-01-18T14:36:45.205169Z","iopub.status.idle":"2024-01-18T14:36:45.210651Z","shell.execute_reply":"2024-01-18T14:36:45.209587Z","shell.execute_reply.started":"2024-01-18T14:36:45.205524Z"},"trusted":true},"outputs":[],"source":["forecasts_original = scaler.inverse_transform(forecast_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:37:28.194159Z","iopub.status.busy":"2024-01-18T14:37:28.193718Z","iopub.status.idle":"2024-01-18T14:37:28.200519Z","shell.execute_reply":"2024-01-18T14:37:28.199591Z","shell.execute_reply.started":"2024-01-18T14:37:28.194120Z"},"trusted":true},"outputs":[],"source":["forecasts_original.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, n):\n","    test_gen = lstm_data_generator(df_test, target_col, DAYS_PER_SEQUENCE, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","# evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, VAL_END)"]},{"cell_type":"markdown","metadata":{},"source":["- TPU nutzen und direkt aufrufen\n","- mutiprocessing\n","- use tensorflow dataset\n","- gpu nutzen (CUDA aufrufen)\n","- ConvLSTM1D layer: https://keras.io/api/layers/recurrent_layers/conv_lstm1d/\n","- https://www.kaggle.com/code/li325040229/eda-and-an-encoder-decoder-lstm-with-9-features/notebook#Build-a-LSTM-Model-\n","- Wie zum laufen bekommen?:\n","-   <b>Encoder-Decoder Model</b> --> https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243\n","-   https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243 --> 30490 als batch input nutzen, dann aber Problem, dass scheinbar nur Abhängigkeiten von einem auf den anderen Tag getrackt werden und keine Muster zwischen Zeitsequenzen gefunden werden können\n","- Herangehensweise:\n","    - Develop one model per site.\n","    -  Develop one model per group of sites.\n","    -  Develop one model for all sites.\n","\n","\n","<br>\n","\n","- Progress bars mit tqdm anzeigen\n","- Test, Validierung und Trainingzeitraum sollten sich nicht überlappen, ist aber ggf. der Fall?\n","- Ggf. zu float16 konvertieren checken, ob finaler df mit time slices dann deutlich kleiner und performance testen\n","- column 'd' in training df löschen?\n","- paralletl computing einstellen\n","- use_multiprocessing in keras auf true setzen (model.fit agument)\n","- Cross validation?\n","- Ensemble learning?\n","- brauche ich one-hot encoding für categorical features?\n","- Things to consider:\n","- dropout\n","- seed\n","- learning rate\n","- loss function\n","- optimizer\n","- metrics\n","- batch size\n","- epochs\n","- Add CNN layer\n","- model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(DAYS_PER_SEQUENCE, num_features)))\n","- model.add(MaxPooling1D(pool_size=2))\n","- model.add(Flatten())\n","- model.add(LSTM(50, activation='relu'))\n","- model.add(Dense(1)) / or more layers as needed\n","- model.compile()"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
