{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.714019Z","iopub.status.busy":"2024-02-04T10:28:45.713557Z","iopub.status.idle":"2024-02-04T10:28:45.724837Z","shell.execute_reply":"2024-02-04T10:28:45.723860Z","shell.execute_reply.started":"2024-02-04T10:28:45.713975Z"},"trusted":true},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V3_ohne_Cat_features_block_items'\n","CODE_ENV = 'local' #'kaggle', 'aws', 'local'\n","STATUS = 'training' #'production' "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.726757Z","iopub.status.busy":"2024-02-04T10:28:45.726461Z","iopub.status.idle":"2024-02-04T10:28:49.601403Z","shell.execute_reply":"2024-02-04T10:28:49.600379Z","shell.execute_reply.started":"2024-02-04T10:28:45.726733Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Masking, RepeatVector, Dropout, Reshape\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","from keras import backend as K\n","from keras.callbacks import Callback\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.603096Z","iopub.status.busy":"2024-02-04T10:28:49.602532Z","iopub.status.idle":"2024-02-04T10:28:49.707155Z","shell.execute_reply":"2024-02-04T10:28:49.706050Z","shell.execute_reply.started":"2024-02-04T10:28:49.603066Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n","False\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.710168Z","iopub.status.busy":"2024-02-04T10:28:49.709769Z","iopub.status.idle":"2024-02-04T10:28:49.717151Z","shell.execute_reply":"2024-02-04T10:28:49.716162Z","shell.execute_reply.started":"2024-02-04T10:28:49.710139Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v3/'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.718565Z","iopub.status.busy":"2024-02-04T10:28:49.718293Z","iopub.status.idle":"2024-02-04T10:28:49.727911Z","shell.execute_reply":"2024-02-04T10:28:49.727014Z","shell.execute_reply.started":"2024-02-04T10:28:49.718540Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","VALIDATION_DATA  = prc_dir +'df_1.pkl' # Validation data\n","BASE      = prc_dir +'df_2.pkl' # Base data\n","CALENDAR  = prc_dir +'df_3.pkl' # Calendar data\n","NUM_ITEMS = 30490 # Number of items per each day\n","DAYS_PER_SEQUENCE = 14  # Length of the sequence\n","TARGET_COL = 'sales_amount'\n","# REPEATED_FEATURES = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","REPEATED_FEATURES = ['sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","# ONCE_ONLY_FEATURES = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","ONCE_ONLY_FEATURES = ['snap_CA', 'snap_TX', 'snap_WI', 'mday_normalized', 'month_sin', 'month_cos', 'wday_sin', 'wday_cos', 'week_sin', 'week_cos', 'year_normalized'] # List to hold feature columns that are not repeated for each item"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Set test_end to 1969 in case of production\n","if STATUS=='production':\n","    TEST_END = 1969\n","elif STATUS=='training':\n","    TEST_END = 1941\n","\n","# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.729350Z","iopub.status.busy":"2024-02-04T10:28:49.728998Z","iopub.status.idle":"2024-02-04T10:28:53.163392Z","shell.execute_reply":"2024-02-04T10:28:53.162542Z","shell.execute_reply.started":"2024-02-04T10:28:49.729323Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","def get_whole_data():\n","    df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)\n","    return df_all_data"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Return a df with all unique combinations of store_id and dept_id\n","def get_combinations(df_all_data):\n","    # get all store_id and dept_id combinations\n","    df_combinations_store_dep = df_all_data[['store_id','dept_id']].drop_duplicates().reset_index(drop=True)\n","\n","    return df_combinations_store_dep"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Filter df down to only the current store_id and dept_id combination\n","def filter_df(df_combinations_store_dep, df_all_data, i):\n","    store_id = df_combinations_store_dep.loc[i, 'store_id']\n","    dept_id = df_combinations_store_dep.loc[i, 'dept_id']\n","    ids = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)]['id'].drop_duplicates().values\n","    filtered_df = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)].reset_index(drop=True)\n","    filtered_df.reset_index(drop=True, inplace=True)\n","\n","    # Get the number of block items\n","    num_block_items = len(ids)\n","\n","    # Get the number of features\n","    num_features = len(ONCE_ONLY_FEATURES) + len(REPEATED_FEATURES) * num_block_items # Calculate the number of features\n","\n","    # Get the input shape later on for the model\n","    input_shape = (DAYS_PER_SEQUENCE, num_features)\n","\n","    return filtered_df, ids, num_block_items, num_features, input_shape"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.293651Z","iopub.status.busy":"2024-02-04T10:29:02.293253Z","iopub.status.idle":"2024-02-04T10:29:02.323161Z","shell.execute_reply":"2024-02-04T10:29:02.322166Z","shell.execute_reply.started":"2024-02-04T10:29:02.293598Z"},"trusted":true},"outputs":[],"source":["# create a dataframe that stores only th 5 first items for each day\n","# indices = np.array([np.arange(start, start + num_block_items) for start in range(0, TEST_END * NUM_ITEMS, NUM_ITEMS)]).flatten()\n","# df_all_data = df_all_data.iloc[indices]\n","# df_all_data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Normalize numerical columns\n","def prepare_df(df_all_data):\n","    # Define categorical and numerical columns\n","    categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                        'd', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                        'snap_CA', 'snap_TX', 'snap_WI']\n","    numerical_cols = ['sell_price']\n","\n","    # Convert categorical columns to category dtype and encode with cat.codes\n","    for col in categorical_cols:\n","        df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","    # Normalize numerical columns\n","    scaler_numerical = MinMaxScaler()\n","    df_all_data[numerical_cols] = scaler_numerical.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","\n","    scaler_target = MinMaxScaler()\n","    df_all_data[TARGET_COL] = scaler_target.fit_transform(df_all_data[[TARGET_COL]].astype(np.float64))\n","\n","    return df_all_data, scaler_target"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.326509Z","iopub.status.busy":"2024-02-04T10:29:02.326211Z","iopub.status.idle":"2024-02-04T10:29:02.341825Z","shell.execute_reply":"2024-02-04T10:29:02.340817Z","shell.execute_reply.started":"2024-02-04T10:29:02.326485Z"},"trusted":true},"outputs":[],"source":["def train_test_split(df_all_data):\n","    # For training split up between train and validation dataset, else use all for training and create test dataset\n","    if STATUS=='training':\n","        df_train = df_all_data[df_all_data['d'] < TRAIN_END].reset_index(drop=True)\n","        df_val   = df_all_data[(df_all_data['d'] >= TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] < VAL_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_test  = None\n","    elif STATUS=='production':\n","        df_train = df_all_data[df_all_data['d'] < VAL_END].reset_index(drop=True)\n","        df_test  = df_all_data[(df_all_data['d'] >= VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] < TEST_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_val   = None\n","\n","    # Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","    del df_all_data\n","\n","    return df_train, df_val, df_test"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.343749Z","iopub.status.busy":"2024-02-04T10:29:02.343338Z","iopub.status.idle":"2024-02-04T10:29:02.353726Z","shell.execute_reply":"2024-02-04T10:29:02.352895Z","shell.execute_reply.started":"2024-02-04T10:29:02.343711Z"},"trusted":true},"outputs":[],"source":["### Use for batch generation input to model ###\n","def lstm_data_generator(df, num_block_items):\n","    length_days = len(df) // num_block_items  # 1941 days\n","    while True:\n","        for i in range(0, length_days - DAYS_PER_SEQUENCE):\n","            start_ind = i * num_block_items\n","            end_ind = start_ind + num_block_items * (DAYS_PER_SEQUENCE)  # predict the next day after the sequence\n","\n","            # Extract once-only features for all days in the sequence at once\n","            once_features = df.iloc[start_ind:end_ind:num_block_items][ONCE_ONLY_FEATURES].to_numpy() # 0,5,10,...295 --> len(once_features)=DAYS_PER_SEQUENCE (60); [3 cols]\n","            # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","            # Extract repeated features for all items and days at once\n","            repeated_features_stack = df.iloc[start_ind:end_ind][REPEATED_FEATURES].to_numpy() # 0:300 --> len(repeated_features_stack)=300 ;[3 cols]\n","\n","            # Reshape to a 3D array: 60 days, 5 items ,3 repeated features\n","            reshaped_3d = repeated_features_stack.reshape(DAYS_PER_SEQUENCE, num_block_items, len(REPEATED_FEATURES))\n","\n","            # Reshape to a 2D array: 60 days,  5 items * 3 features each (15)\n","            final_array = reshaped_3d.reshape(DAYS_PER_SEQUENCE, -1)\n","\n","            # Combine once-only and repeated features\n","            batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","            # Reshape batch_sequences to match LSTM input shape\n","            batch_sequences = batch_sequences.reshape(1, DAYS_PER_SEQUENCE, -1)\n","\n","            # Extract targets\n","            batch_targets = df.iloc[end_ind:end_ind + num_block_items][[TARGET_COL]].to_numpy().flatten()\n","\n","            # Yield the batch\n","            yield batch_sequences, batch_targets"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["### Create x and y in one go without the generator version autogeneration ###\n","def create_x_y(df, num_block_items):\n","    length_days = len(df) // num_block_items\n","    x = []\n","    y = []\n","    for i in range(0, length_days - DAYS_PER_SEQUENCE):\n","        start_ind = i * num_block_items\n","        end_ind = start_ind + num_block_items * (DAYS_PER_SEQUENCE)  # predict the next day after the sequence\n","\n","        # Extract once-only features for all days in the sequence at once\n","        once_features = df.iloc[start_ind:end_ind:num_block_items][ONCE_ONLY_FEATURES].to_numpy()\n","        # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","        # Extract repeated features for all items and days at once\n","        repeated_features_stack = df.iloc[start_ind:end_ind][REPEATED_FEATURES].to_numpy() # 210,000 items, 10 features\n","\n","        # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","        reshaped_3d = repeated_features_stack.reshape(DAYS_PER_SEQUENCE, num_block_items, len(REPEATED_FEATURES))\n","\n","        # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","        final_array = reshaped_3d.reshape(DAYS_PER_SEQUENCE, -1)\n","\n","        # Combine once-only and repeated features\n","        batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","        # Reshape batch_sequences to match LSTM input shape\n","        # batch_sequences = batch_sequences.reshape(1, DAYS_PER_SEQUENCE, -1)\n","\n","        # Extract targets\n","        batch_targets = df.iloc[end_ind:end_ind + num_block_items][[TARGET_COL]].to_numpy().flatten()\n","\n","        # Append to x and y\n","        x.append(batch_sequences)\n","        y.append(batch_targets)\n","\n","    return np.array(x), np.array(y)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.355635Z","iopub.status.busy":"2024-02-04T10:29:02.355210Z","iopub.status.idle":"2024-02-04T10:29:02.365871Z","shell.execute_reply":"2024-02-04T10:29:02.364973Z","shell.execute_reply.started":"2024-02-04T10:29:02.355590Z"},"trusted":true},"outputs":[],"source":["# Get the training data and labels array for the LSTM model\n","def get_x_and_y(df_train, df_val, df_test, num_block_items):\n","    # For generator use:\n","    # train_generator = lstm_data_generator(df_train)\n","    # val_generator = lstm_data_generator(df_val)\n","\n","    # For single batch input use:\n","    train_x, train_y = create_x_y(df_train, num_block_items)\n","\n","    if STATUS=='training':\n","        val_x, val_y = create_x_y(df_val, num_block_items)\n","        test_x, test_y = None, None\n","    elif STATUS=='production': \n","        test_x, test_y = create_x_y(df_test, num_block_items)\n","        val_x, val_y = None, None\n","\n","    # df_train not needed anymore\n","    del df_train\n","\n","    return train_x, train_y, val_x, val_y, test_x, test_y"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.386793Z","iopub.status.busy":"2024-02-04T10:29:02.386108Z","iopub.status.idle":"2024-02-04T10:29:02.392710Z","shell.execute_reply":"2024-02-04T10:29:02.391436Z","shell.execute_reply.started":"2024-02-04T10:29:02.386757Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:04.259644Z","iopub.status.busy":"2024-02-04T10:29:04.259265Z","iopub.status.idle":"2024-02-04T10:29:04.264825Z","shell.execute_reply":"2024-02-04T10:29:04.263758Z","shell.execute_reply.started":"2024-02-04T10:29:04.259591Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:04.266821Z","iopub.status.busy":"2024-02-04T10:29:04.266222Z","iopub.status.idle":"2024-02-04T10:30:29.341563Z","shell.execute_reply":"2024-02-04T10:30:29.340754Z","shell.execute_reply.started":"2024-02-04T10:29:04.266784Z"},"trusted":true},"outputs":[],"source":["def model_training(model, train_x, train_y, val_x, val_y, epochs):\n","    # Training the model in batches\n","    # history = model.fit(x=train_generator,\n","    #                      steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","    #                      validation_data=val_generator,\n","    #                      validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","    #                      epochs=epochs,\n","    #                      callbacks=[ResetStatesCallback()])\n","\n","    # Train in one go\n","    if STATUS=='training':\n","        history = model.fit(x=train_x,  # Entire training dataset\n","                        y=train_y,  # Corresponding training labels\n","                        validation_data=(val_x, val_y),  # Entire validation dataset and labels\n","                        epochs=epochs)\n","    elif STATUS=='production':\n","        history = model.fit(x=train_x,  # Entire training dataset\n","                        y=train_y,  # Corresponding training labels\n","                        epochs=epochs)\n","        \n","    return model, history"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def eval(val_x, val_y, model, num_features, scaler_target):\n","    df_eval = pd.DataFrame(columns=['day', 'prediction', 'actual'])\n","    for i in range(0, len(val_x)):\n","        # create new dataframe with the current day, the actual value and the prediction\n","        df_temp = pd.DataFrame({'day': i, 'prediction': model.predict(val_x[i].reshape(1, DAYS_PER_SEQUENCE, num_features), verbose=0).flatten(), 'actual': val_y[i]})\n","        df_eval = pd.concat([df_eval, df_temp], axis=0, ignore_index=True)\n","        # new column with the difference between actual and prediction\n","        df_eval['difference'] = df_eval['actual'] - df_eval['prediction']\n","        # new columns with inverse transformation of actual and prediction\n","        df_eval['actual_inv'] = scaler_target.inverse_transform(df_eval[['actual']]).astype(int)\n","        df_eval['prediction_inv'] = scaler_target.inverse_transform(df_eval[['prediction']]).round(0).astype(int)\n","        # new columns with the difference between actual and prediction\n","        df_eval['difference_inv'] = df_eval['actual_inv'] - df_eval['prediction_inv']\n","    return df_eval"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:52:42.174980Z","iopub.status.busy":"2024-02-04T10:52:42.174264Z","iopub.status.idle":"2024-02-04T10:52:42.181344Z","shell.execute_reply":"2024-02-04T10:52:42.180389Z","shell.execute_reply.started":"2024-02-04T10:52:42.174943Z"},"trusted":true},"outputs":[],"source":["# Evaluation for generator batches\n","def test_eval(val_generator, model, scaler_target):\n","    x, y = next(val_generator)\n","    \n","    prediction_original = model.predict(x)\n","\n","    true_array = scaler_target.inverse_transform(y).flatten()\n","    predicted_array = scaler_target.inverse_transform(prediction_original)[0]\n","    \n","    d = {\"true_array\": true_array, \"predicted_array\": predicted_array}\n","    df = pd.DataFrame(d)\n","    df['predicted_array_rounded'] = df['predicted_array'].round().astype(int)\n","    df['Difference'] = df['true_array'] - df['predicted_array']\n","\n","    print(df)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["################################### Function to forecast the next 28 days (This function for case all data in one batch) ###################################\n","def rolling_forecast(model, df_test, df_val, test_x, test_y, val_x, val_y, scaler_target, num_features, num_block_items):\n","    # Set the df_copy, x_copy and y_copy to the correct dataset\n","    if STATUS=='production':\n","        df_copy = df_test.copy()\n","        x_copy = test_x.copy()\n","        y_copy = test_y.copy()    \n","    \n","    elif STATUS=='training':\n","        df_copy = df_val.copy()\n","        x_copy = val_x.copy()\n","        y_copy = val_y.copy()\n","\n","    # Predict the next 28 days\n","    for i in range(TEST_DUR):\n","        prediction_normalized = model.predict(x_copy[i].reshape(1, DAYS_PER_SEQUENCE, num_features), verbose=0).flatten()\n","    \n","        # Impractical to adjust the prepared array, so we will update the df_test copy and use it to create a new array with the updated prediction values\n","        start_idx = DAYS_PER_SEQUENCE*num_block_items+(i*num_block_items)\n","        end_idx = start_idx + num_block_items - 1\n","        df_copy.loc[start_idx:end_idx, TARGET_COL] = prediction_normalized\n","\n","        # Create new df for x and y\n","        x_copy, _ = create_x_y(df_copy, num_block_items)\n","\n","        # Update the y array with the new prediction\n","        y_copy[i] = prediction_normalized\n","    \n","    # Inverse transform the predictions\n","    predictions_original = scaler_target.inverse_transform(y_copy).round(0).astype(int)\n","\n","    # Make sure no negative values are returned\n","    predictions_original[predictions_original < 0] = 0\n","        \n","    return predictions_original\n","#########################################################################################################"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Create a DataFrame for predictions\n","def prepare_fc_to_file(forecast_df, forecast_array, ids):\n","    # Transpose predictions to match the sample submission format\n","    forecast_array = forecast_array.T\n","\n","    # Create array to write to df\n","    forecast_array = np.concatenate((ids.reshape(len(ids),1), forecast_array), axis=1)\n","\n","    # Create a DataFrame for your predictions\n","    forecast_tmp_df = pd.DataFrame(forecast_array, columns=['id'] + [f'F{i+1}' for i in range(28)])\n","\n","    # concatenate forecast to forecast_df\n","    forecast_df = pd.concat([forecast_df, forecast_tmp_df], axis=0, ignore_index=True)\n","\n","    return forecast_df"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def write_to_csv(forecast_df, dir):\n","    # Get validation data\n","    val_df = pd.read_pickle(VALIDATION_DATA)\n","\n","    # Combine forecast with validation data\n","    forecast_df = pd.concat([val_df, forecast_df], axis=0, ignore_index=True)\n","\n","    # Save the forecast to a csv file\n","    forecast_df.to_csv(dir, index=False)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.401718Z","iopub.status.busy":"2024-02-04T10:29:02.401392Z","iopub.status.idle":"2024-02-04T10:29:02.411354Z","shell.execute_reply":"2024-02-04T10:29:02.410459Z","shell.execute_reply.started":"2024-02-04T10:29:02.401688Z"},"trusted":true},"outputs":[],"source":["# Model parameters\n","epochs = 4\n","batch_size = 1\n","lr = 0.001 #lr = 0.0001\n","clipvalue = 0.5\n","\n","# Model compile parameters\n","loss = rmse\n","optimizer = Adam(learning_rate=lr, clipvalue=clipvalue)\n","metrics = tf.keras.metrics.MeanAbsoluteError()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.412895Z","iopub.status.busy":"2024-02-04T10:29:02.412579Z","iopub.status.idle":"2024-02-04T10:29:04.224334Z","shell.execute_reply":"2024-02-04T10:29:04.223506Z","shell.execute_reply.started":"2024-02-04T10:29:02.412871Z"},"trusted":true},"outputs":[],"source":["# Neu: Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","def create_lstm_model(input_shape, num_block_items):\n","   model = Sequential([\n","      LSTM(units=100, activation='tanh', return_sequences=True, recurrent_dropout=0.1, input_shape=input_shape),\n","      # Dropout(0.2),\n","      LSTM(units=60,  activation='tanh', return_sequences=False, recurrent_dropout=0.1),\n","      # Dropout(0.2),\n","      Dense(units=num_block_items), # activation='relu', 'softmax; Final Dense layer for output\n","      Reshape((num_block_items,1))]) # Reshape the output to be (number of items)\n","\n","   model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","   # For tracking purposes: check the models parameters\n","   # model.summary()\n","\n","   return model"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["# for each store_id and dept_id call get whole data, filter for store_id and dept_id\n","def lstm_pipeline():\n","    df_all_data = get_whole_data()\n","\n","    # Get all store_id and dept_id combinations\n","    df_combinations_store_dep = get_combinations(df_all_data)\n","\n","    # Create empty dataframe to store the forecast\n","    forecast_df = pd.DataFrame(columns=['id'] + [f'F{i+1}' for i in range(28)])\n","\n","    # Loop over all store_id and dept_id combinations, create a model, train it, create the prediction and save it to a file\n","    for i in range(0, len(df_combinations_store_dep)):\n","        print(f'Processing {i+1} of {len(df_combinations_store_dep)}: store_id {df_combinations_store_dep.loc[i, \"store_id\"]} and dept_id {df_combinations_store_dep.loc[i, \"dept_id\"]}')\n","        # Filter df down to only the current store_id and dept_id combination\n","        filtered_df, ids, num_block_items, num_features, input_shape = filter_df(df_combinations_store_dep, df_all_data, i)\n","\n","        # Prepare the data for training\n","        filtered_df, scaler_target = prepare_df(filtered_df)\n","\n","        # Split the data into train, validation and test set\n","        df_train, df_val, df_test = train_test_split(filtered_df)\n","\n","        # Create training, validation and test data arrays from the dataframes\n","        train_x, train_y, val_x, val_y, test_x, test_y = get_x_and_y(df_train, df_val, df_test, num_block_items)\n","\n","        # Create the model\n","        model = create_lstm_model(input_shape, num_block_items)\n","\n","        # Train the model\n","        model_trained, history = model_training(model, train_x, train_y, val_x, val_y, epochs)\n","\n","        # Call eval function to get the evaluation dataframe and some feeling for the results\n","        # df_eval = eval(val_x, val_y, model, num_features, scaler_target)\n","\n","        # Test output for generator\n","        # test_data = test_eval(val_generator, model, scaler_target)\n","\n","        # Create the forecast\n","        predictions_original = rolling_forecast(model_trained, df_test, df_val, test_x, test_y, val_x, val_y, scaler_target, num_features, num_block_items)\n","\n","        # forecast_df = prepare_fc_to_file(forecast_df, predictions_original, ids)\n","        print(\"####################################################\\n\")\n","\n","    write_to_csv(forecast_df, sub_dir + 'sample_submission.csv')\n","\n","    return forecast_df"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing 1 of 70: store_id CA_1 and dept_id HOBBIES_1\n","Epoch 1/4\n","59/59 [==============================] - 5s 48ms/step - loss: 0.0163 - mean_absolute_error: 0.0105 - val_loss: 0.0091 - val_mean_absolute_error: 0.0043\n","Epoch 2/4\n","59/59 [==============================] - 3s 44ms/step - loss: 0.0093 - mean_absolute_error: 0.0039 - val_loss: 0.0088 - val_mean_absolute_error: 0.0041\n","Epoch 3/4\n","59/59 [==============================] - 3s 46ms/step - loss: 0.0092 - mean_absolute_error: 0.0038 - val_loss: 0.0089 - val_mean_absolute_error: 0.0041\n","Epoch 4/4\n","59/59 [==============================] - 3s 46ms/step - loss: 0.0091 - mean_absolute_error: 0.0038 - val_loss: 0.0087 - val_mean_absolute_error: 0.0040\n","####################################################\n","\n"]}],"source":["forecast_df = lstm_pipeline()"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>prediction</th>\n","      <th>actual</th>\n","      <th>difference</th>\n","      <th>actual_inv</th>\n","      <th>prediction_inv</th>\n","      <th>difference_inv</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.004543</td>\n","      <td>0.003401</td>\n","      <td>-0.001142</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>420</th>\n","      <td>1</td>\n","      <td>0.004377</td>\n","      <td>0.000000</td>\n","      <td>-0.004377</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>836</th>\n","      <td>2</td>\n","      <td>0.004371</td>\n","      <td>0.013605</td>\n","      <td>0.009234</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1252</th>\n","      <td>3</td>\n","      <td>0.004550</td>\n","      <td>0.013605</td>\n","      <td>0.009056</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1668</th>\n","      <td>4</td>\n","      <td>0.004813</td>\n","      <td>0.000000</td>\n","      <td>-0.004813</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2084</th>\n","      <td>5</td>\n","      <td>0.004880</td>\n","      <td>0.003401</td>\n","      <td>-0.001479</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2500</th>\n","      <td>6</td>\n","      <td>0.004699</td>\n","      <td>0.013605</td>\n","      <td>0.008906</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2916</th>\n","      <td>7</td>\n","      <td>0.004522</td>\n","      <td>0.000000</td>\n","      <td>-0.004522</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3332</th>\n","      <td>8</td>\n","      <td>0.004249</td>\n","      <td>0.003401</td>\n","      <td>-0.000848</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3748</th>\n","      <td>9</td>\n","      <td>0.004286</td>\n","      <td>0.000000</td>\n","      <td>-0.004286</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4164</th>\n","      <td>10</td>\n","      <td>0.004484</td>\n","      <td>0.003401</td>\n","      <td>-0.001082</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4580</th>\n","      <td>11</td>\n","      <td>0.004719</td>\n","      <td>0.000000</td>\n","      <td>-0.004719</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>12</td>\n","      <td>0.004781</td>\n","      <td>0.003401</td>\n","      <td>-0.001380</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5412</th>\n","      <td>13</td>\n","      <td>0.004738</td>\n","      <td>0.003401</td>\n","      <td>-0.001337</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5828</th>\n","      <td>14</td>\n","      <td>0.004454</td>\n","      <td>0.006803</td>\n","      <td>0.002349</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6244</th>\n","      <td>15</td>\n","      <td>0.004320</td>\n","      <td>0.000000</td>\n","      <td>-0.004320</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>6660</th>\n","      <td>16</td>\n","      <td>0.004275</td>\n","      <td>0.003401</td>\n","      <td>-0.000873</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7076</th>\n","      <td>17</td>\n","      <td>0.004444</td>\n","      <td>0.003401</td>\n","      <td>-0.001042</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7492</th>\n","      <td>18</td>\n","      <td>0.004619</td>\n","      <td>0.006803</td>\n","      <td>0.002184</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7908</th>\n","      <td>19</td>\n","      <td>0.004776</td>\n","      <td>0.003401</td>\n","      <td>-0.001375</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8324</th>\n","      <td>20</td>\n","      <td>0.004642</td>\n","      <td>0.003401</td>\n","      <td>-0.001240</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8740</th>\n","      <td>21</td>\n","      <td>0.004437</td>\n","      <td>0.000000</td>\n","      <td>-0.004437</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>9156</th>\n","      <td>22</td>\n","      <td>0.004223</td>\n","      <td>0.003401</td>\n","      <td>-0.000822</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9572</th>\n","      <td>23</td>\n","      <td>0.004192</td>\n","      <td>0.003401</td>\n","      <td>-0.000790</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9988</th>\n","      <td>24</td>\n","      <td>0.004361</td>\n","      <td>0.006803</td>\n","      <td>0.002442</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10404</th>\n","      <td>25</td>\n","      <td>0.004601</td>\n","      <td>0.006803</td>\n","      <td>0.002202</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10820</th>\n","      <td>26</td>\n","      <td>0.004739</td>\n","      <td>0.006803</td>\n","      <td>0.002064</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11236</th>\n","      <td>27</td>\n","      <td>0.004662</td>\n","      <td>0.013605</td>\n","      <td>0.008943</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      day  prediction    actual  difference  actual_inv  prediction_inv  \\\n","4       0    0.004543  0.003401   -0.001142           1               1   \n","420     1    0.004377  0.000000   -0.004377           0               1   \n","836     2    0.004371  0.013605    0.009234           4               1   \n","1252    3    0.004550  0.013605    0.009056           4               1   \n","1668    4    0.004813  0.000000   -0.004813           0               1   \n","2084    5    0.004880  0.003401   -0.001479           1               1   \n","2500    6    0.004699  0.013605    0.008906           4               1   \n","2916    7    0.004522  0.000000   -0.004522           0               1   \n","3332    8    0.004249  0.003401   -0.000848           1               1   \n","3748    9    0.004286  0.000000   -0.004286           0               1   \n","4164   10    0.004484  0.003401   -0.001082           1               1   \n","4580   11    0.004719  0.000000   -0.004719           0               1   \n","4996   12    0.004781  0.003401   -0.001380           1               1   \n","5412   13    0.004738  0.003401   -0.001337           1               1   \n","5828   14    0.004454  0.006803    0.002349           2               1   \n","6244   15    0.004320  0.000000   -0.004320           0               1   \n","6660   16    0.004275  0.003401   -0.000873           1               1   \n","7076   17    0.004444  0.003401   -0.001042           1               1   \n","7492   18    0.004619  0.006803    0.002184           2               1   \n","7908   19    0.004776  0.003401   -0.001375           1               1   \n","8324   20    0.004642  0.003401   -0.001240           1               1   \n","8740   21    0.004437  0.000000   -0.004437           0               1   \n","9156   22    0.004223  0.003401   -0.000822           1               1   \n","9572   23    0.004192  0.003401   -0.000790           1               1   \n","9988   24    0.004361  0.006803    0.002442           2               1   \n","10404  25    0.004601  0.006803    0.002202           2               1   \n","10820  26    0.004739  0.006803    0.002064           2               1   \n","11236  27    0.004662  0.013605    0.008943           4               1   \n","\n","       difference_inv  \n","4                   0  \n","420                -1  \n","836                 3  \n","1252                3  \n","1668               -1  \n","2084                0  \n","2500                3  \n","2916               -1  \n","3332                0  \n","3748               -1  \n","4164                0  \n","4580               -1  \n","4996                0  \n","5412                0  \n","5828                1  \n","6244               -1  \n","6660                0  \n","7076                0  \n","7492                1  \n","7908                0  \n","8324                0  \n","8740               -1  \n","9156                0  \n","9572                0  \n","9988                1  \n","10404               1  \n","10820               1  \n","11236               3  "]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["# Test output\n","# forecast_df.head(30)\n","# every 5h row\n","forecast_df.iloc[4::416,]\n","#how many rows with day = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # For testing purposes: check how large on batch is\n","# # next train_generator\n","# x, y = next(train_generator)\n","\n","# # size of memory in mb of x and y\n","# # print(train_x.nbytes / 1e6)\n","# # print(train_y.nbytes / 1e6)\n","\n","# print(train_x.shape)\n","# print(train_y.shape)\n","# print(x.shape)\n","# print(y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[],"source":["# # Save the model to a specified directory\n","# if CODE_ENV=='local':\n","#     ###local###\n","#     model.save(src_dir + 'models/' + MODEL_NAME + '.h5')\n","    \n","# if CODE_ENV=='kaggle':\n","#     ###On Kaggle###\n","#     model.save('/kaggle/working/' + MODEL_NAME + '.h5')\n","\n","# if CODE_ENV=='aws':\n","#     ###aws###\n","#     model.save(src_dir + 'models/' + MODEL_NAME + '.h5')"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[],"source":["# Start from here if you want to load the model\n","# from keras.models import load_model\n","\n","# # Load the model from a specified directory\n","# if CODE_ENV=='local':\n","#     ###local###\n","#     model = load_model(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","# if CODE_ENV=='kaggle':\n","#     ###On Kaggle###\n","#     model = load_model('/kaggle/input/v1-model/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","# if CODE_ENV=='aws':\n","#     ###aws###\n","#     model.save(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No history to plot\n"]}],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":["# def prepare_forecast_input(df, DAYS_PER_SEQUENCE, num_items):\n","#     #df_test starts at 1942-7 which we need take into account\n","#     # Prepare input data for forecasting\n","#     forecast_input = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx].drop('sales_amount', axis=1).to_numpy()\n","#         forecast_input.append(sequence)\n","#     return np.array(forecast_input)\n","\n","\n","# Custom function for input to prepare forecasts input for model\n","# def prepare_forecast_input(df, target, model, DAYS_PER_SEQUENCE, num_items):\n","#     forecast_output = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx, : ].drop(target, axis=1).to_numpy()\n","#         # forecast_output.append(model.predict(sequence))\n","#         forecast_output.append(model.predict(sequence.reshape(1, sequence.shape[0], sequence.shape[1])))\n","#     return np.array(forecast_output)#.reshape(-1, 1)\n","# forecast_output = prepare_forecast_input(df_test, TARGET_COL, model, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","#forecasts_original = scaler.inverse_transform(forecast_output)\n","\n"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming df_all_data contains all data up to day 1941\n","# forecast_input = prepare_forecast_input(df_test, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","\n","# Generate forecasts\n","# forecasts = model.predict(forecast_input)\n","# forecasts_original = scaler.inverse_transform(forecasts)\n","\n","# forecasts_original now contains the predicted sales amounts for days 1942 to 1969\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[],"source":["# Prepare input for forecasts\n","# I cannot use the custom lstm_data_generator\n","# Prepare 7 day slices each shifted by one day\n","def prepare_forecast_input(df, DAYS_PER_SEQUENCE, target_col):\n","    forecast_input = []\n","    for i in range(0, len(df)//NUM_ITEMS): #i=0; 1, 2, 3, ..., 35?\n","        if i + DAYS_PER_SEQUENCE < (len(df)-1)//NUM_ITEMS: #7, 8, 9, 10, ...\n","            start_idx = i*NUM_ITEMS\n","            end_idx   = start_idx + NUM_ITEMS * DAYS_PER_SEQUENCE\n","            sequence  = df.iloc[start_idx : end_idx, :].drop(target_col, axis=1).to_numpy()\n","            forecast_input.append(sequence)\n","    return np.array(forecast_input)\n","\n","# predict_array = prepare_forecast_input(df=df_test, DAYS_PER_SEQUENCE=DAYS_PER_SEQUENCE, target_col=TARGET_COL)"]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, n):\n","    test_gen = lstm_data_generator(df_test, target_col, DAYS_PER_SEQUENCE, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":38,"metadata":{"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","# evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, VAL_END)"]},{"cell_type":"markdown","metadata":{},"source":["- Cross validation\n","- lambda irgendwo nutzen\n","- TPU nutzen und direkt aufrufen\n","- mutiprocessing\n","- use tensorflow dataset\n","- gpu nutzen (CUDA aufrufen)\n","- ConvLSTM1D layer: https://keras.io/api/layers/recurrent_layers/conv_lstm1d/\n","- https://www.kaggle.com/code/li325040229/eda-and-an-encoder-decoder-lstm-with-9-features/notebook#Build-a-LSTM-Model-\n","- Wie zum laufen bekommen?:\n","-   <b>Encoder-Decoder Model</b> --> https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243\n","-   https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243 --> 30490 als batch input nutzen, dann aber Problem, dass scheinbar nur Abhängigkeiten von einem auf den anderen Tag getrackt werden und keine Muster zwischen Zeitsequenzen gefunden werden können\n","- Herangehensweise:\n","    - Develop one model per site.\n","    -  Develop one model per group of sites.\n","    -  Develop one model for all sites.\n","\n","\n","<br>\n","\n","- Progress bars mit tqdm anzeigen\n","- Test, Validierung und Trainingzeitraum sollten sich nicht überlappen, ist aber ggf. der Fall?\n","- Ggf. zu float16 konvertieren checken, ob finaler df mit time slices dann deutlich kleiner und performance testen\n","- column 'd' in training df löschen?\n","- paralletl computing einstellen\n","- use_multiprocessing in keras auf true setzen (model.fit agument)\n","- Cross validation?\n","- Ensemble learning?\n","- brauche ich one-hot encoding für categorical features?\n","- Things to consider:\n","- dropout\n","- seed\n","- learning rate\n","- loss function\n","- optimizer\n","- metrics\n","- batch size\n","- epochs\n","- Add CNN layer\n","- model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(DAYS_PER_SEQUENCE, num_features)))\n","- model.add(MaxPooling1D(pool_size=2))\n","- model.add(Flatten())\n","- model.add(LSTM(50, activation='relu'))\n","- model.add(Dense(1)) / or more layers as needed\n","- model.compile()"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
