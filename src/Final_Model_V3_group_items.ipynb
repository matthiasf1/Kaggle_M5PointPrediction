{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V3_ohne_Cat_features_block_items'\n","CODE_ENV = 'aws' #'kaggle', 'aws', 'local'\n","TEST_END  = 1941 #1969 "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.393140Z","iopub.status.busy":"2024-01-22T13:02:34.392833Z","iopub.status.idle":"2024-01-22T13:02:47.056364Z","shell.execute_reply":"2024-01-22T13:02:47.055528Z","shell.execute_reply.started":"2024-01-22T13:02:34.393114Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_5031/3405039513.py:2: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd\n","2024-02-03 19:52:51.391314: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-02-03 19:52:52.317390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Masking, RepeatVector, Dropout, Reshape\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","from keras import backend as K\n","from keras.callbacks import Callback\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n","True\n"]},{"name":"stderr","output_type":"stream","text":["2024-02-03 19:52:53.262379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-02-03 19:52:53.299351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-02-03 19:52:53.299624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.373809Z","iopub.status.busy":"2024-01-22T13:02:34.373363Z","iopub.status.idle":"2024-01-22T13:02:34.379504Z","shell.execute_reply":"2024-01-22T13:02:34.378489Z","shell.execute_reply.started":"2024-01-22T13:02:34.373780Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v3/'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.381096Z","iopub.status.busy":"2024-01-22T13:02:34.380802Z","iopub.status.idle":"2024-01-22T13:02:34.390571Z","shell.execute_reply":"2024-01-22T13:02:34.389570Z","shell.execute_reply.started":"2024-01-22T13:02:34.381070Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","BASE      = prc_dir +'df_1.pkl'\n","CALENDAR  = prc_dir +'df_2.pkl'\n","NUM_ITEMS = 30490 # Number of items per each day\n","# Set time_steps for defining test, train and validation sets\n","DAYS_PER_SEQUENCE = 28  # Length of the sequence\n","TARGET_COL = 'sales_amount'\n","NUM_BLOCK_ITEMS = 10 # Reduce the number of items to be used for training to avoid curse of dimensionality"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:47.058053Z","iopub.status.busy":"2024-01-22T13:02:47.057502Z","iopub.status.idle":"2024-01-22T13:03:01.455370Z","shell.execute_reply":"2024-01-22T13:03:01.454363Z","shell.execute_reply.started":"2024-01-22T13:02:47.058028Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# create a dataframe that stores only th 5 first items for each day\n","indices = np.array([np.arange(start, start + 5) for start in range(0, TEST_END * NUM_ITEMS, NUM_ITEMS)]).flatten()\n","\n","extracted_df = df_all_data.iloc[indices]"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>dept_id</th>\n","      <th>cat_id</th>\n","      <th>store_id</th>\n","      <th>state_id</th>\n","      <th>sales_amount</th>\n","      <th>sell_price</th>\n","      <th>is_available</th>\n","      <th>d</th>\n","      <th>wday</th>\n","      <th>...</th>\n","      <th>event_type_1</th>\n","      <th>event_name_2</th>\n","      <th>event_type_2</th>\n","      <th>snap_CA</th>\n","      <th>snap_TX</th>\n","      <th>snap_WI</th>\n","      <th>mday</th>\n","      <th>week</th>\n","      <th>month</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>59028644</th>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>2.88</td>\n","      <td>1</td>\n","      <td>1937</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>18</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59059130</th>\n","      <td>HOBBIES_1_001</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>3.0</td>\n","      <td>8.38</td>\n","      <td>1</td>\n","      <td>1938</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59059131</th>\n","      <td>HOBBIES_1_002</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>3.97</td>\n","      <td>1</td>\n","      <td>1938</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59059132</th>\n","      <td>HOBBIES_1_003</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>2.0</td>\n","      <td>2.97</td>\n","      <td>1</td>\n","      <td>1938</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59059133</th>\n","      <td>HOBBIES_1_004</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>3.0</td>\n","      <td>4.64</td>\n","      <td>1</td>\n","      <td>1938</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59059134</th>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>2.88</td>\n","      <td>1</td>\n","      <td>1938</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59089620</th>\n","      <td>HOBBIES_1_001</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>3.0</td>\n","      <td>8.38</td>\n","      <td>1</td>\n","      <td>1939</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59089621</th>\n","      <td>HOBBIES_1_002</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>3.97</td>\n","      <td>1</td>\n","      <td>1939</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59089622</th>\n","      <td>HOBBIES_1_003</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>3.0</td>\n","      <td>2.97</td>\n","      <td>1</td>\n","      <td>1939</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59089623</th>\n","      <td>HOBBIES_1_004</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>4.64</td>\n","      <td>1</td>\n","      <td>1939</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59089624</th>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>2.0</td>\n","      <td>2.88</td>\n","      <td>1</td>\n","      <td>1939</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59120110</th>\n","      <td>HOBBIES_1_001</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>8.38</td>\n","      <td>1</td>\n","      <td>1940</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59120111</th>\n","      <td>HOBBIES_1_002</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>3.97</td>\n","      <td>1</td>\n","      <td>1940</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59120112</th>\n","      <td>HOBBIES_1_003</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>2.97</td>\n","      <td>1</td>\n","      <td>1940</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59120113</th>\n","      <td>HOBBIES_1_004</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>2.0</td>\n","      <td>4.64</td>\n","      <td>1</td>\n","      <td>1940</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59120114</th>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>1.0</td>\n","      <td>2.88</td>\n","      <td>1</td>\n","      <td>1940</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59150600</th>\n","      <td>HOBBIES_1_001</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>1.0</td>\n","      <td>8.38</td>\n","      <td>1</td>\n","      <td>1941</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59150601</th>\n","      <td>HOBBIES_1_002</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>3.97</td>\n","      <td>1</td>\n","      <td>1941</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59150602</th>\n","      <td>HOBBIES_1_003</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>1.0</td>\n","      <td>2.97</td>\n","      <td>1</td>\n","      <td>1941</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59150603</th>\n","      <td>HOBBIES_1_004</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>6.0</td>\n","      <td>4.64</td>\n","      <td>1</td>\n","      <td>1941</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>59150604</th>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>2.88</td>\n","      <td>1</td>\n","      <td>1941</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>2016</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>21 rows × 21 columns</p>\n","</div>"],"text/plain":["                item_id    dept_id   cat_id store_id state_id  sales_amount  \\\n","59028644  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59059130  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA           3.0   \n","59059131  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59059132  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1       CA           2.0   \n","59059133  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1       CA           3.0   \n","59059134  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59089620  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA           3.0   \n","59089621  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59089622  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1       CA           3.0   \n","59089623  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59089624  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1       CA           2.0   \n","59120110  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59120111  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59120112  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59120113  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1       CA           2.0   \n","59120114  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1       CA           1.0   \n","59150600  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA           1.0   \n","59150601  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","59150602  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1       CA           1.0   \n","59150603  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1       CA           6.0   \n","59150604  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","\n","          sell_price  is_available     d  wday  ... event_type_1 event_name_2  \\\n","59028644        2.88             1  1937     5  ...          NaN          NaN   \n","59059130        8.38             1  1938     6  ...          NaN          NaN   \n","59059131        3.97             1  1938     6  ...          NaN          NaN   \n","59059132        2.97             1  1938     6  ...          NaN          NaN   \n","59059133        4.64             1  1938     6  ...          NaN          NaN   \n","59059134        2.88             1  1938     6  ...          NaN          NaN   \n","59089620        8.38             1  1939     7  ...          NaN          NaN   \n","59089621        3.97             1  1939     7  ...          NaN          NaN   \n","59089622        2.97             1  1939     7  ...          NaN          NaN   \n","59089623        4.64             1  1939     7  ...          NaN          NaN   \n","59089624        2.88             1  1939     7  ...          NaN          NaN   \n","59120110        8.38             1  1940     1  ...          NaN          NaN   \n","59120111        3.97             1  1940     1  ...          NaN          NaN   \n","59120112        2.97             1  1940     1  ...          NaN          NaN   \n","59120113        4.64             1  1940     1  ...          NaN          NaN   \n","59120114        2.88             1  1940     1  ...          NaN          NaN   \n","59150600        8.38             1  1941     2  ...          NaN          NaN   \n","59150601        3.97             1  1941     2  ...          NaN          NaN   \n","59150602        2.97             1  1941     2  ...          NaN          NaN   \n","59150603        4.64             1  1941     2  ...          NaN          NaN   \n","59150604        2.88             1  1941     2  ...          NaN          NaN   \n","\n","         event_type_2 snap_CA  snap_TX  snap_WI  mday  week  month  year  \n","59028644          NaN       0        0        0    18    20      5  2016  \n","59059130          NaN       0        0        0    19    20      5  2016  \n","59059131          NaN       0        0        0    19    20      5  2016  \n","59059132          NaN       0        0        0    19    20      5  2016  \n","59059133          NaN       0        0        0    19    20      5  2016  \n","59059134          NaN       0        0        0    19    20      5  2016  \n","59089620          NaN       0        0        0    20    20      5  2016  \n","59089621          NaN       0        0        0    20    20      5  2016  \n","59089622          NaN       0        0        0    20    20      5  2016  \n","59089623          NaN       0        0        0    20    20      5  2016  \n","59089624          NaN       0        0        0    20    20      5  2016  \n","59120110          NaN       0        0        0    21    20      5  2016  \n","59120111          NaN       0        0        0    21    20      5  2016  \n","59120112          NaN       0        0        0    21    20      5  2016  \n","59120113          NaN       0        0        0    21    20      5  2016  \n","59120114          NaN       0        0        0    21    20      5  2016  \n","59150600          NaN       0        0        0    22    20      5  2016  \n","59150601          NaN       0        0        0    22    20      5  2016  \n","59150602          NaN       0        0        0    22    20      5  2016  \n","59150603          NaN       0        0        0    22    20      5  2016  \n","59150604          NaN       0        0        0    22    20      5  2016  \n","\n","[21 rows x 21 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["extracted_df.tail(21)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:01.457014Z","iopub.status.busy":"2024-01-22T13:03:01.456690Z","iopub.status.idle":"2024-01-22T13:03:09.962427Z","shell.execute_reply":"2024-01-22T13:03:09.961481Z","shell.execute_reply.started":"2024-01-22T13:03:01.456986Z"},"trusted":true},"outputs":[],"source":["# Define categorical and numerical columns\n","categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n","numerical_cols = ['sell_price']\n","\n","# Convert categorical columns to category dtype and encode with cat.codes\n","for col in categorical_cols:\n","    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","# Normalize numerical columns\n","scaler_numerical = MinMaxScaler()\n","df_all_data[numerical_cols] = scaler_numerical.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","\n","scaler_target = MinMaxScaler()\n","df_all_data[TARGET_COL] = scaler_target.fit_transform(df_all_data[[TARGET_COL]].astype(np.float64))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:09.970478Z","iopub.status.busy":"2024-01-22T13:03:09.970194Z","iopub.status.idle":"2024-01-22T13:03:18.370792Z","shell.execute_reply":"2024-01-22T13:03:18.369916Z","shell.execute_reply.started":"2024-01-22T13:03:09.970453Z"},"trusted":true},"outputs":[],"source":["# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation \n","\n","df_train = df_all_data[df_all_data['d'] < TRAIN_END].reset_index(drop=True)\n","df_val   = df_all_data[(df_all_data['d'] >= TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] < VAL_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","df_test  = df_all_data[(df_all_data['d'] >= VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] < TEST_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","\n","# Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","del df_all_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.372402Z","iopub.status.busy":"2024-01-22T13:03:18.372106Z","iopub.status.idle":"2024-01-22T13:03:18.386039Z","shell.execute_reply":"2024-01-22T13:03:18.385156Z","shell.execute_reply.started":"2024-01-22T13:03:18.372378Z"},"trusted":true},"outputs":[],"source":["def lstm_data_generator(df, num_features, target, days_sliding_window, batch_size, once_only_features, repeated_features):\n","    length_days = len(df) // NUM_ITEMS  # 1941 days\n","    while True:\n","        for i in range(0, length_days - days_sliding_window):\n","            start_ind = i * NUM_ITEMS\n","            end_ind = start_ind + NUM_ITEMS * (days_sliding_window)  # predict the next day after the sequence\n","\n","            # Extract once-only features for all days in the sequence at once\n","            once_features = df.iloc[start_ind:end_ind:NUM_ITEMS][once_only_features].to_numpy()\n","            # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","            # Extract repeated features for all items and days at once\n","            repeated_features_stack = df.iloc[start_ind:end_ind][repeated_features].to_numpy() # 210,000 items, 10 features\n","\n","            # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","            reshaped_3d = repeated_features_stack.reshape(days_sliding_window, NUM_ITEMS, len(repeated_features))\n","\n","            # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","            final_array = reshaped_3d.reshape(days_sliding_window, -1)\n","\n","            # Combine once-only and repeated features\n","            batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","            # Reshape batch_sequences to match LSTM input shape\n","            batch_sequences = batch_sequences.reshape(1, days_sliding_window, -1)\n","\n","            # Extract targets\n","            batch_targets = df.iloc[end_ind:end_ind + NUM_ITEMS][target].to_numpy()\n","\n","            # Yield the batch\n","            yield batch_sequences, batch_targets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the generator\n","# repeated_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","repeated_features = ['sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","# once_only_features = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","once_only_features = ['snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","num_features = len(once_only_features) + len(repeated_features) * NUM_ITEMS # Calculate the number of features\n","\n","train_generator = lstm_data_generator(df_train, num_features, TARGET_COL, days_sliding_window=DAYS_PER_SEQUENCE, batch_size=1, once_only_features=once_only_features, repeated_features=repeated_features)\n","val_generator = lstm_data_generator(df_val, num_features, TARGET_COL, days_sliding_window=DAYS_PER_SEQUENCE, batch_size=1, once_only_features=once_only_features, repeated_features=repeated_features)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For testing purposes: check how large on batch is\n","# next train_generator\n","# x, y = next(train_generator)\n","# # size of memory in mb of x and y\n","# print(x.nbytes / 1e6)\n","# print(y.nbytes / 1e6)\n","\n","# print(x.shape)\n","# print(y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # list all columns in df_train\n","# df_train.columns\n","\n","# # call head of df_train displaying all columns without truncation\n","# pd.set_option('display.max_columns', None)\n","# df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.428419Z","iopub.status.busy":"2024-01-22T13:03:18.428151Z","iopub.status.idle":"2024-01-22T13:03:18.437168Z","shell.execute_reply":"2024-01-22T13:03:18.436284Z","shell.execute_reply.started":"2024-01-22T13:03:18.428395Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.438535Z","iopub.status.busy":"2024-01-22T13:03:18.438246Z","iopub.status.idle":"2024-01-22T13:03:19.715259Z","shell.execute_reply":"2024-01-22T13:03:19.714216Z","shell.execute_reply.started":"2024-01-22T13:03:18.438511Z"},"trusted":true},"outputs":[],"source":["# This is a sequence-to-sequence model: errors can propagate through the sequence\n","# model = Sequential()\n","\n","# model.add(LSTM(units=30,\n","#                activation='tanh', #relu\n","#                return_sequences=False,\n","#                stateful=True))\n","\n","# model.add(RepeatVector(28))\n","\n","# model.add(LSTM(units=30, \n","#                activation='tanh', \n","#                return_sequences=True, \n","#                stateful=True))\n","\n","# model.add(Dense(units=1))\n","\n","# model.compile(optimizer='adam', loss='mean_squared_error')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.389636Z","iopub.status.busy":"2024-01-22T13:03:18.389317Z","iopub.status.idle":"2024-01-22T13:03:18.416778Z","shell.execute_reply":"2024-01-22T13:03:18.415805Z","shell.execute_reply.started":"2024-01-22T13:03:18.389597Z"},"trusted":true},"outputs":[],"source":["# Model parameters\n","epochs = 5\n","batch_size = 1\n","lr = 0.01"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Neu: Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","Units_LSTM_1 = 500\n","Units_LSTM_2 = 300\n","Units_LSTM_3 = 200\n","Units_LSTM_4 = 200\n","Units_Dense_1 = 200\n","\n","model = Sequential()\n","\n","# First LSTM layer with more units and return sequences\n","model.add(LSTM(units=Units_LSTM_1, \n","               activation='tanh', \n","               return_sequences=True, \n","               stateful=True,\n","               input_shape=(DAYS_PER_SEQUENCE, num_features),\n","               batch_input_shape=(batch_size, DAYS_PER_SEQUENCE, num_features)))\n","model.add(Dropout(0.3))\n","\n","# Second LSTM layer with less units and return sequences\n","model.add(LSTM(units=Units_LSTM_2, \n","               activation='tanh', \n","               return_sequences=True, \n","               stateful=True))\n","model.add(Dropout(0.3))\n","\n","# Third LSTM layer with less units and return sequences\n","model.add(LSTM(units=Units_LSTM_3, \n","               activation='tanh', \n","               return_sequences=True, \n","               stateful=True))\n","model.add(Dropout(0.3))\n","\n","# Additional LSTM Layer\n","model.add(LSTM(units=Units_LSTM_4, \n","               activation='tanh', \n","               return_sequences=False, \n","               stateful=True))\n","model.add(Dropout(0.3))\n","\n","# Dense layer\n","model.add(Dense(units=Units_Dense_1, \n","                activation='relu'))\n","\n","# Final Dense layer for output\n","model.add(Dense(units=NUM_ITEMS))\n","\n","# Reshape the output to be (number of items)\n","#model.add(Reshape((NUM_ITEMS,))) # Eigentlich müsste das vorherige Layer bereits die richtige Form haben\n","\n","model.compile(optimizer=Adam(learning_rate=lr), \n","              loss=rmse, \n","              metrics=[RootMeanSquaredError()])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For tracking purposes: check the models parameters\n","model.summary()\n","\n","# Print input shape of the layers\n","# for layer in model.layers:\n","#     print(layer.input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:19.716971Z","iopub.status.busy":"2024-01-22T13:03:19.716580Z","iopub.status.idle":"2024-01-22T13:03:19.722552Z","shell.execute_reply":"2024-01-22T13:03:19.721412Z","shell.execute_reply.started":"2024-01-22T13:03:19.716930Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T21:41:47.956270Z","iopub.status.busy":"2024-01-22T21:41:47.955997Z","iopub.status.idle":"2024-01-22T21:41:48.300127Z","shell.execute_reply":"2024-01-22T21:41:48.298879Z","shell.execute_reply.started":"2024-01-22T21:41:47.956244Z"},"trusted":true},"outputs":[],"source":["# Training the model\n","history = model.fit(x=train_generator,\n","          steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","          validation_data=val_generator,\n","          validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","          epochs=epochs,\n","          callbacks=[ResetStatesCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train and validation df not needed anymore\n","del df_train\n","del df_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:30:39.354772Z","iopub.status.busy":"2024-01-18T14:30:39.353690Z","iopub.status.idle":"2024-01-18T14:30:39.384792Z","shell.execute_reply":"2024-01-18T14:30:39.383711Z","shell.execute_reply.started":"2024-01-18T14:30:39.354715Z"},"trusted":true},"outputs":[],"source":["# Save the model to a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')\n","    \n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model.save('/kaggle/working/' + MODEL_NAME + '.h5')\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Start from here if you want to load the model\n","from keras.models import load_model\n","\n","# Load the model from a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model = load_model(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model = load_model('/kaggle/input/v1-model/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:00.674762Z","iopub.status.busy":"2024-01-18T14:34:00.673913Z","iopub.status.idle":"2024-01-18T14:34:00.949572Z","shell.execute_reply":"2024-01-18T14:34:00.948466Z","shell.execute_reply.started":"2024-01-18T14:34:00.674717Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x, y = next(val_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction_original = model.predict(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction_original"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaler_target.inverse_transform([y])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaler_target.inverse_transform(prediction_original)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction_original"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict_next_day(model, last_window_data, num_features):\n","    # Predict the next day\n","    next_day_prediction = model.predict(last_window_data.reshape(1, 7, num_features))\n","    return next_day_prediction\n","\n","# Assuming you have a function to update your data with new predictions\n","def update_data_with_prediction(data, new_prediction):\n","    # Logic to update your dataset with the new_prediction\n","    # This could involve shifting the window and inserting the new prediction\n","    pass\n","\n","# Starting with actual historical data\n","last_window_data = get_last_window_of_actual_data()  # Shape: (7, num_features)\n","\n","for i in range(num_future_days):\n","    # Predict the next day\n","    next_day_prediction = predict_next_day(model, last_window_data, num_features)\n","\n","    # Update your data with this new prediction\n","    last_window_data = update_data_with_prediction(last_window_data, next_day_prediction)\n","\n","    # Now last_window_data contains the most recent prediction, which will be used in the next iteration\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def prepare_forecast_input(df, DAYS_PER_SEQUENCE, num_items):\n","#     #df_test starts at 1942-7 which we need take into account\n","#     # Prepare input data for forecasting\n","#     forecast_input = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx].drop('sales_amount', axis=1).to_numpy()\n","#         forecast_input.append(sequence)\n","#     return np.array(forecast_input)\n","\n","\n","# Custom function for input to prepare forecasts input for model\n","# def prepare_forecast_input(df, target, model, DAYS_PER_SEQUENCE, num_items):\n","#     forecast_output = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx, : ].drop(target, axis=1).to_numpy()\n","#         # forecast_output.append(model.predict(sequence))\n","#         forecast_output.append(model.predict(sequence.reshape(1, sequence.shape[0], sequence.shape[1])))\n","#     return np.array(forecast_output)#.reshape(-1, 1)\n","# forecast_output = prepare_forecast_input(df_test, TARGET_COL, model, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","#forecasts_original = scaler.inverse_transform(forecast_output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming df_all_data contains all data up to day 1941\n","# forecast_input = prepare_forecast_input(df_test, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","\n","# Generate forecasts\n","# forecasts = model.predict(forecast_input)\n","# forecasts_original = scaler.inverse_transform(forecasts)\n","\n","# forecasts_original now contains the predicted sales amounts for days 1942 to 1969\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:26.857391Z","iopub.status.busy":"2024-01-18T14:34:26.857016Z","iopub.status.idle":"2024-01-18T14:34:27.273805Z","shell.execute_reply":"2024-01-18T14:34:27.272980Z","shell.execute_reply.started":"2024-01-18T14:34:26.857362Z"},"trusted":true},"outputs":[],"source":["# Prepare input for forecasts\n","# I cannot use the custom lstm_data_generator\n","# Prepare 7 day slices each shifted by one day\n","def prepare_forecast_input(df, DAYS_PER_SEQUENCE, target_col):\n","    forecast_input = []\n","    for i in range(0, len(df)//NUM_ITEMS): #i=0; 1, 2, 3, ..., 35?\n","        if i + DAYS_PER_SEQUENCE < (len(df)-1)//NUM_ITEMS: #7, 8, 9, 10, ...\n","            start_idx = i*NUM_ITEMS\n","            end_idx   = start_idx + NUM_ITEMS * DAYS_PER_SEQUENCE\n","            sequence  = df.iloc[start_idx : end_idx, :].drop(target_col, axis=1).to_numpy()\n","            forecast_input.append(sequence)\n","    return np.array(forecast_input)\n","\n","predict_array = prepare_forecast_input(df=df_test, DAYS_PER_SEQUENCE=DAYS_PER_SEQUENCE, target_col=TARGET_COL)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, n):\n","    test_gen = lstm_data_generator(df_test, target_col, DAYS_PER_SEQUENCE, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","# evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, VAL_END)"]},{"cell_type":"markdown","metadata":{},"source":["- TPU nutzen und direkt aufrufen\n","- mutiprocessing\n","- use tensorflow dataset\n","- gpu nutzen (CUDA aufrufen)\n","- ConvLSTM1D layer: https://keras.io/api/layers/recurrent_layers/conv_lstm1d/\n","- https://www.kaggle.com/code/li325040229/eda-and-an-encoder-decoder-lstm-with-9-features/notebook#Build-a-LSTM-Model-\n","- Wie zum laufen bekommen?:\n","-   <b>Encoder-Decoder Model</b> --> https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243\n","-   https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243 --> 30490 als batch input nutzen, dann aber Problem, dass scheinbar nur Abhängigkeiten von einem auf den anderen Tag getrackt werden und keine Muster zwischen Zeitsequenzen gefunden werden können\n","- Herangehensweise:\n","    - Develop one model per site.\n","    -  Develop one model per group of sites.\n","    -  Develop one model for all sites.\n","\n","\n","<br>\n","\n","- Progress bars mit tqdm anzeigen\n","- Test, Validierung und Trainingzeitraum sollten sich nicht überlappen, ist aber ggf. der Fall?\n","- Ggf. zu float16 konvertieren checken, ob finaler df mit time slices dann deutlich kleiner und performance testen\n","- column 'd' in training df löschen?\n","- paralletl computing einstellen\n","- use_multiprocessing in keras auf true setzen (model.fit agument)\n","- Cross validation?\n","- Ensemble learning?\n","- brauche ich one-hot encoding für categorical features?\n","- Things to consider:\n","- dropout\n","- seed\n","- learning rate\n","- loss function\n","- optimizer\n","- metrics\n","- batch size\n","- epochs\n","- Add CNN layer\n","- model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(DAYS_PER_SEQUENCE, num_features)))\n","- model.add(MaxPooling1D(pool_size=2))\n","- model.add(Flatten())\n","- model.add(LSTM(50, activation='relu'))\n","- model.add(Dense(1)) / or more layers as needed\n","- model.compile()"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
