{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.714019Z","iopub.status.busy":"2024-02-04T10:28:45.713557Z","iopub.status.idle":"2024-02-04T10:28:45.724837Z","shell.execute_reply":"2024-02-04T10:28:45.723860Z","shell.execute_reply.started":"2024-02-04T10:28:45.713975Z"},"trusted":true},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V3_ohne_Cat_features_block_items'\n","CODE_ENV = 'local' #'kaggle', 'aws', 'local'\n","STATUS = 'training' #'training', 'production'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.726757Z","iopub.status.busy":"2024-02-04T10:28:45.726461Z","iopub.status.idle":"2024-02-04T10:28:49.601403Z","shell.execute_reply":"2024-02-04T10:28:49.600379Z","shell.execute_reply.started":"2024-02-04T10:28:45.726733Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import (Input, LSTM, Dense, Embedding, Dropout, Reshape, \n","                                     concatenate, Flatten, Bidirectional, GlobalAveragePooling1D)\n","from tensorflow.keras.optimizers.legacy import Adam\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.initializers import GlorotNormal\n","from tensorflow.keras.callbacks import Callback\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from keras_self_attention import SeqSelfAttention"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.603096Z","iopub.status.busy":"2024-02-04T10:28:49.602532Z","iopub.status.idle":"2024-02-04T10:28:49.707155Z","shell.execute_reply":"2024-02-04T10:28:49.706050Z","shell.execute_reply.started":"2024-02-04T10:28:49.603066Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n","False\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.710168Z","iopub.status.busy":"2024-02-04T10:28:49.709769Z","iopub.status.idle":"2024-02-04T10:28:49.717151Z","shell.execute_reply":"2024-02-04T10:28:49.716162Z","shell.execute_reply.started":"2024-02-04T10:28:49.710139Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data/'\n","    src_dir = '/kaggle/working/'\n","    sub_dir = src_dir + 'submissions/'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.718565Z","iopub.status.busy":"2024-02-04T10:28:49.718293Z","iopub.status.idle":"2024-02-04T10:28:49.727911Z","shell.execute_reply":"2024-02-04T10:28:49.727014Z","shell.execute_reply.started":"2024-02-04T10:28:49.718540Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","VALIDATION_DATA  = prc_dir +'df_1.pkl' # Validation data\n","BASE      = prc_dir +'df_2.pkl' # Base data\n","CALENDAR  = prc_dir +'df_3.pkl' # Calendar data\n","# NUM_ITEMS = 30490 # Number of items per each day\n","\n","DAYS_PER_SEQUENCE = 28  # Length of the sequence\n","MAX_BATCH_SIZE = 900 # Maximum number of ids to be used in each batch to avoid memory issues and curse of dimensionality\n","\n","\n","TARGET_COL = 'sales_amount'\n","# REPEATED_FEATURES = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","REPEATED_FEATURES = ['sales_amount', 'sell_price', 'is_available',\n","                     'sales_amount_moving_avg_7', 'sales_amount_moving_avg_28', 'sales_amount_lag_1',\n","                     'zero_sales_available', 'consecutive_zero_sales'] # List to hold all feature columns that are used for each item\n","SALES_AMOUNT_COLS = ['sales_amount', 'sales_amount_moving_avg_7', 'sales_amount_moving_avg_28', 'sales_amount_lag_1']\n","# ONCE_ONLY_FEATURES = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","ONCE_ONLY_FEATURES = ['snap_CA', 'snap_TX', 'snap_WI', 'mday_normalized', 'day_continuous_normalized',\n","                      'month_sin', 'month_cos', 'wday_sin', 'wday_cos', 'week_sin', 'week_cos', \n","                      'year_normalized'] # List to hold feature columns that are not repeated for each item\n","EVENT_COLS = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n","EVENT_LEN = len(EVENT_COLS)\n","NOT_NEEDED_COLS = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Set test_end to 1969 in case of production\n","if STATUS=='production':\n","    TEST_END = 1969\n","elif STATUS=='training':\n","    TEST_END = 1969 #1941\n","\n","# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.729350Z","iopub.status.busy":"2024-02-04T10:28:49.728998Z","iopub.status.idle":"2024-02-04T10:28:53.163392Z","shell.execute_reply":"2024-02-04T10:28:53.162542Z","shell.execute_reply.started":"2024-02-04T10:28:49.729323Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","def get_whole_data():\n","    df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)\n","    return df_all_data"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["# df_all_data = get_whole_data()"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# get all ['store_id','dept_id'] combinations from df_all_data and count the occurences\n","# df_combinations = df_all_data[(df_all_data['d']==1)].groupby(['store_id', 'dept_id']).size().reset_index(name='count')  \n"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>store_id</th>\n","      <th>dept_id</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>32</th>\n","      <td>TX_1</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>TX_1</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>TX_1</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>TX_1</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>TX_1</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>TX_1</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>TX_1</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   store_id      dept_id  count\n","32     TX_1    HOBBIES_2    149\n","28     TX_1      FOODS_1    216\n","29     TX_1      FOODS_2    398\n","31     TX_1    HOBBIES_1    416\n","34     TX_1  HOUSEHOLD_2    515\n","33     TX_1  HOUSEHOLD_1    532\n","30     TX_1      FOODS_3    823"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["#pd.set_option('display.max_rows', None)\n","# df_combinations[df_combinations['store_id'] == 'TX_1'].sort_values('count').tail(50)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# Return a df with all unique combinations of store_id and dept_id\n","def get_combinations(df_all_data):\n","    # get all store_id and dept_id combinations\n","    df_combinations_store_dep = df_all_data[['store_id','dept_id']].drop_duplicates().reset_index(drop=True)\n","\n","    return df_combinations_store_dep"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Filter df down to only the current store_id and dept_id combination\n","def filter_df(df_combinations_store_dep, df_all_data, i):\n","    store_id = df_combinations_store_dep.loc[i, 'store_id']\n","    dept_id = df_combinations_store_dep.loc[i, 'dept_id']\n","    ids = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)]['id'].drop_duplicates().values\n","    filtered_df = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)].reset_index(drop=True)\n","    filtered_df.reset_index(drop=True, inplace=True) ##################################################????\n","\n","    # Remove all unused columns\n","    filtered_df.drop(NOT_NEEDED_COLS, axis=1, inplace=True)\n","\n","    # Calculate number of batches\n","    num_batches = int(np.ceil(len(ids)/MAX_BATCH_SIZE))\n","\n","    return filtered_df, ids, num_batches"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def calc_vocab_size(filtered_df, embedding_dims_max=50):\n","    vocab_size=[]\n","    embedding_dims=[]\n","    # count the unique entries of event_name_1 event_type_1 event_name_2 event_type_2\n","    # append the number of unique entries to the list vocab_size\n","    vocab_size.append(len(filtered_df['event_name_1'].unique()))\n","    vocab_size.append(len(filtered_df['event_type_1'].unique()))\n","    vocab_size.append(len(filtered_df['event_name_2'].unique()))\n","    vocab_size.append(len(filtered_df['event_type_2'].unique()))\n","    \n","    # loop over all other indices and calculate the embedding dimensions\n","    for i in range(0, len(vocab_size)):\n","        embedding_dims.append(int(embedding_dims_max * (vocab_size[i]/max(vocab_size))))\n","\n","    return vocab_size, embedding_dims"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def filtered_df_batches(filtered_df, ids, num_batches, counter):\n","    # get ids for the current batch\n","    start_idx = counter * MAX_BATCH_SIZE\n","    if counter < num_batches - 1:\n","        end_idx = (counter + 1) * MAX_BATCH_SIZE\n","        ids_batch = ids[start_idx:end_idx]\n","    else:\n","        ids_batch = ids[start_idx:]\n","\n","    # filter the df for the current batch\n","    filtered_df_batch = filtered_df[filtered_df['id'].isin(ids_batch)].reset_index(drop=True)\n","\n","    # Get the number of block items\n","    num_block_items = len(ids_batch)\n","\n","    # Get the number of features\n","    num_features = len(ONCE_ONLY_FEATURES) + len(REPEATED_FEATURES) * num_block_items # Calculate the number of features\n","\n","    # Get the input shape later on for the model\n","    input_shape = (DAYS_PER_SEQUENCE, num_features)\n","\n","    return filtered_df_batch, num_block_items, num_features, input_shape, ids_batch"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.293651Z","iopub.status.busy":"2024-02-04T10:29:02.293253Z","iopub.status.idle":"2024-02-04T10:29:02.323161Z","shell.execute_reply":"2024-02-04T10:29:02.322166Z","shell.execute_reply.started":"2024-02-04T10:29:02.293598Z"},"trusted":true},"outputs":[],"source":["# create a dataframe that stores only th 5 first items for each day\n","# indices = np.array([np.arange(start, start + num_block_items) for start in range(0, TEST_END * NUM_ITEMS, NUM_ITEMS)]).flatten()\n","# df_all_data = df_all_data.iloc[indices]\n","# df_all_data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# all_df = get_whole_data()\n","# show dataframe with all columns from all_df"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# pd.set_option('display.max_columns', None)\n","# all_df[all_df['d']== 1530].head()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Normalize numerical columns\n","def prepare_df(df_all_data):\n","    # Define categorical and numerical columns\n","    categorical_cols = ['id'] #'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'snap_CA', 'snap_TX', 'snap_WI', 'is_available'\n","    \n","    numerical_cols = ['sell_price']\n","\n","    # Convert categorical columns to category dtype and encode with cat.codes\n","    for col in categorical_cols:\n","        df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","    # Adjust the event cols\n","    # 1. Create an encoder instance for each column\n","    encoders = {col: LabelEncoder() for col in EVENT_COLS}\n","\n","    # Apply encoding to each column\n","    for col, encoder in encoders.items():\n","        df_all_data[col] = encoder.fit_transform(df_all_data[col].astype(str)).astype('int8')\n","\n","    # Normalize numerical columns\n","    scaler_numerical = MinMaxScaler()\n","    df_all_data[numerical_cols] = scaler_numerical.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","\n","    # scaler_target = MinMaxScaler() #not used any more\n","    # df_all_data[SALES_AMOUNT_COLS] = scaler_target.fit_transform(df_all_data[SALES_AMOUNT_COLS].astype(np.float64))\n","    df_all_data[SALES_AMOUNT_COLS] = df_all_data[SALES_AMOUNT_COLS].apply(np.log1p)\n","\n","    return df_all_data#, scaler_target"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.326509Z","iopub.status.busy":"2024-02-04T10:29:02.326211Z","iopub.status.idle":"2024-02-04T10:29:02.341825Z","shell.execute_reply":"2024-02-04T10:29:02.340817Z","shell.execute_reply.started":"2024-02-04T10:29:02.326485Z"},"trusted":true},"outputs":[],"source":["def train_test_split(df_all_data):\n","    # For training split up between train and validation dataset, else use all for training and create test dataset\n","    if STATUS=='training':\n","        df_train = df_all_data[df_all_data['d'] <= TRAIN_END].reset_index(drop=True)\n","        df_val   = df_all_data[(df_all_data['d'] > TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] <= VAL_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_test  = None\n","        \n","    elif STATUS=='production':\n","        df_train = df_all_data[df_all_data['d'] <= VAL_END].reset_index(drop=True)\n","        df_test  = df_all_data[(df_all_data['d'] > VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] <= TEST_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_val   = None\n","\n","    # Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","    del df_all_data\n","\n","    return df_train, df_val, df_test"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["### Create x and y in one go without the generator version autogeneration ###\n","def create_x_y(df, num_block_items):\n","    length_days = len(df) // num_block_items\n","    x = []\n","    y = []\n","    events = []\n","\n","    for i in range(0, length_days - DAYS_PER_SEQUENCE):\n","        start_ind = i * num_block_items\n","        end_ind = start_ind + num_block_items * (DAYS_PER_SEQUENCE)  # predict the next day after the sequence\n","\n","        # Extract once-only features for all days in the sequence at once\n","        once_features = df.iloc[start_ind:end_ind:num_block_items][ONCE_ONLY_FEATURES].to_numpy()\n","\n","        # Get event columns\n","        event_features = df.iloc[start_ind:end_ind:num_block_items][EVENT_COLS].to_numpy()\n","\n","        # Extract repeated features for all items and days at once\n","        repeated_features_stack = df.iloc[start_ind:end_ind][REPEATED_FEATURES].to_numpy() # 210,000 items, 10 features\n","\n","        # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","        reshaped_3d = repeated_features_stack.reshape(DAYS_PER_SEQUENCE, num_block_items, len(REPEATED_FEATURES))\n","\n","        # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","        final_array = reshaped_3d.reshape(DAYS_PER_SEQUENCE, -1)\n","\n","        # Combine once-only and repeated features\n","        batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","        # Extract targets\n","        batch_targets = df.iloc[end_ind:end_ind + num_block_items][[TARGET_COL]].to_numpy().flatten()\n","\n","        # Append to x, y and events\n","        x.append(batch_sequences)\n","        events.append(event_features)\n","        y.append(batch_targets)\n","\n","\n","    return np.array(x), np.array(events), np.array(y)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.343749Z","iopub.status.busy":"2024-02-04T10:29:02.343338Z","iopub.status.idle":"2024-02-04T10:29:02.353726Z","shell.execute_reply":"2024-02-04T10:29:02.352895Z","shell.execute_reply.started":"2024-02-04T10:29:02.343711Z"},"trusted":true},"outputs":[],"source":["### Use for batch generation input to model ###\n","def lstm_data_generator(df, num_block_items):\n","    length_days = len(df) // num_block_items  # 1941 days\n","    while True:\n","        for i in range(0, length_days - DAYS_PER_SEQUENCE):\n","            start_ind = i * num_block_items\n","            end_ind = start_ind + num_block_items * (DAYS_PER_SEQUENCE)  # predict the next day after the sequence\n","\n","            # Extract once-only features for all days in the sequence at once\n","            once_features = df.iloc[start_ind:end_ind:num_block_items][ONCE_ONLY_FEATURES].to_numpy() # 0,5,10,...295 --> len(once_features)=DAYS_PER_SEQUENCE (60); [3 cols]\n","            # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","            # Extract repeated features for all items and days at once\n","            repeated_features_stack = df.iloc[start_ind:end_ind][REPEATED_FEATURES].to_numpy() # 0:300 --> len(repeated_features_stack)=300 ;[3 cols]\n","\n","            # Reshape to a 3D array: 60 days, 5 items ,3 repeated features\n","            reshaped_3d = repeated_features_stack.reshape(DAYS_PER_SEQUENCE, num_block_items, len(REPEATED_FEATURES))\n","\n","            # Reshape to a 2D array: 60 days,  5 items * 3 features each (15)\n","            final_array = reshaped_3d.reshape(DAYS_PER_SEQUENCE, -1)\n","\n","            # Combine once-only and repeated features\n","            batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","            # Reshape batch_sequences to match LSTM input shape\n","            batch_sequences = batch_sequences.reshape(1, DAYS_PER_SEQUENCE, -1)\n","\n","            # Extract targets\n","            batch_targets = df.iloc[end_ind:end_ind + num_block_items][[TARGET_COL]].to_numpy().flatten()\n","\n","            # Yield the batch\n","            yield batch_sequences, batch_targets"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.355635Z","iopub.status.busy":"2024-02-04T10:29:02.355210Z","iopub.status.idle":"2024-02-04T10:29:02.365871Z","shell.execute_reply":"2024-02-04T10:29:02.364973Z","shell.execute_reply.started":"2024-02-04T10:29:02.355590Z"},"trusted":true},"outputs":[],"source":["# Get the training data and labels array for the LSTM model\n","def get_x_and_y(df_train, df_val, df_test, num_block_items):\n","    # For generator use:\n","    # train_generator = lstm_data_generator(df_train)\n","    # val_generator = lstm_data_generator(df_val)\n","\n","    # For single batch input use:\n","    train_x, train_event_x, train_y = create_x_y(df_train, num_block_items)\n","\n","    if STATUS=='training':\n","        val_x, val_event_x, val_y = create_x_y(df_val, num_block_items)\n","        test_x, test_event_x, test_y = None, None, None\n","    elif STATUS=='production': \n","        test_x, test_event_x, test_y = create_x_y(df_test, num_block_items)\n","        val_x, val_event_x, val_y = None, None, None\n","\n","    # df_train not needed anymore\n","    del df_train\n","\n","    return train_x, train_event_x, train_y, val_x, val_event_x, val_y, test_x, test_event_x, test_y"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.386793Z","iopub.status.busy":"2024-02-04T10:29:02.386108Z","iopub.status.idle":"2024-02-04T10:29:02.392710Z","shell.execute_reply":"2024-02-04T10:29:02.391436Z","shell.execute_reply.started":"2024-02-04T10:29:02.386757Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:04.259644Z","iopub.status.busy":"2024-02-04T10:29:04.259265Z","iopub.status.idle":"2024-02-04T10:29:04.264825Z","shell.execute_reply":"2024-02-04T10:29:04.263758Z","shell.execute_reply.started":"2024-02-04T10:29:04.259591Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# When transfer learning is used, the model should be recompiled with a new, lower learning rate which this function does\n","def prepare_model_tl(model, new_learning_rate, frozen_layers):\n","    # Instantiate a new optimizer with the desired learning rate\n","    new_optimizer = Adam(learning_rate=new_learning_rate)\n","\n","    if frozen_layers:\n","        # Freeze the layers\n","        for layer in frozen_layers:\n","            model.layers[layer].trainable = False\n","\n","    # Recompile the model with the new optimizer\n","    model.compile(optimizer=new_optimizer, loss=loss, metrics=metrics)\n","\n","    return model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:04.266821Z","iopub.status.busy":"2024-02-04T10:29:04.266222Z","iopub.status.idle":"2024-02-04T10:30:29.341563Z","shell.execute_reply":"2024-02-04T10:30:29.340754Z","shell.execute_reply.started":"2024-02-04T10:29:04.266784Z"},"trusted":true},"outputs":[],"source":["def model_training(model, _train_x, train_event_x, train_y, _val_x, val_event_x, val_y, epochs, use_embeddings_events):\n","    # Training the model in batches\n","    # history = model.fit(x=train_generator,\n","    #                      steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","    #                      validation_data=val_generator,\n","    #                      validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","    #                      epochs=epochs,\n","    #                      callbacks=[ResetStatesCallback()])\n","\n","    # Train in one go\n","    if STATUS=='training':\n","        if use_embeddings_events:\n","            train_x = [_train_x, train_event_x[:,:,0], train_event_x[:,:,1], train_event_x[:,:,2], train_event_x[:,:,3]]\n","            val_x   = [_val_x, val_event_x[:,:,0], val_event_x[:,:,1], val_event_x[:,:,2], val_event_x[:,:,3]]\n","        else:\n","            train_x = _train_x\n","            val_x = _val_x\n","\n","        history = model.fit(x=train_x,  # Entire training dataset\n","                        y=train_y,  # Corresponding training labels\n","                        validation_data=(val_x, val_y),  # Entire validation dataset and labels\n","                        epochs=epochs,\n","                        batch_size=batch_size)\n","        \n","    elif STATUS=='production':\n","        if use_embeddings_events:\n","            train_x = [_train_x, train_event_x[:,:,0], train_event_x[:,:,1], train_event_x[:,:,2], train_event_x[:,:,3]]\n","        else:\n","            train_x = _train_x\n","\n","        history = model.fit(x=train_x,  # Entire training dataset\n","                        y=train_y,  # Corresponding training labels\n","                        epochs=epochs,\n","                        batch_size=batch_size)\n","        \n","    return model, history"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:52:42.174980Z","iopub.status.busy":"2024-02-04T10:52:42.174264Z","iopub.status.idle":"2024-02-04T10:52:42.181344Z","shell.execute_reply":"2024-02-04T10:52:42.180389Z","shell.execute_reply.started":"2024-02-04T10:52:42.174943Z"},"trusted":true},"outputs":[],"source":["# Evaluation for generator batches\n","def test_eval(val_generator, model, scaler_target):\n","    x, y = next(val_generator)\n","    \n","    prediction_original = model.predict(x)\n","\n","    true_array = scaler_target.inverse_transform(y).flatten()\n","    predicted_array = scaler_target.inverse_transform(prediction_original)[0]\n","    \n","    d = {\"true_array\": true_array, \"predicted_array\": predicted_array}\n","    df = pd.DataFrame(d)\n","    df['predicted_array_rounded'] = df['predicted_array'].round().astype(int)\n","    df['Difference'] = df['true_array'] - df['predicted_array']\n","\n","    print(df)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def eval(predictions_original, predictions_normalized, val_y, ids_batch):\n","    # df_eval = pd.DataFrame(columns=['day', 'normalized', 'prediction', 'actual'])\n","    df_eval = pd.DataFrame()\n","\n","    # fill the dataframe with prediction values\n","    # fill df_eval['id'] with 28 times all ids_batch values\n","    df_eval['id']  = ids_batch.tolist()*TEST_DUR\n","    df_eval['day'] = [i for i in range(1, TEST_DUR+1) for _ in range(len(predictions_normalized[1]))]\n","    \n","    df_eval['actual_normalized'] = val_y.flatten()\n","    df_eval['pred_normalized'] = predictions_normalized.flatten()\n","    df_eval['difference_norm'] = df_eval['actual_normalized'] - df_eval['pred_normalized']\n","\n","    df_eval['actual_inv'] = np.expm1(val_y).round(0).astype(int).flatten()\n","    df_eval['pred_inv'] = predictions_original.flatten()\n","    df_eval['difference_inv'] = df_eval['actual_inv'] - df_eval['pred_inv']\n","\n","    return df_eval"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Perform feature engineering\n","\"\"\"\n","#####- these columns have to be update in the next notebook based on the predictions made by the model #####\n","- 1 days lag #float 64\n","- moving average for 7 and 28 days #float 64\n","- is there a price reduction?\n","- is there a price increase?\n","- adjust for inflation?\n","- consumer sentiment\n","- holiday\n","- weather\n","- \n","\"\"\"\n","def feature_engineering(df, num_block_items): \n","    ################## lag 1 day sales amount ##############################################################################\n","    # After shifting the first days values are NAN but not important as we skip them because we start with the second day\n","    df['sales_amount_lag_1'] = df['sales_amount'].shift(num_block_items)\n","    ########################################################################################################################\n","\n","    ################## moving average 7 and 28 days #########################\n","    df['sales_amount_moving_avg_7'] = df.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=7).mean())\n","    df['sales_amount_moving_avg_7'] = df['sales_amount_moving_avg_7'].fillna(method='bfill')\n","\n","    df['sales_amount_moving_avg_28'] = df.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=28).mean())\n","    df['sales_amount_moving_avg_28'] = df['sales_amount_moving_avg_28'].fillna(method='bfill')\n","    #########################################################################\n","\n","    ################# days consecutive zero sales and if an entry means that this is a zero sale  #########################\n","    # Step 1: Mark zero sales days where item is available\n","    df['zero_sales_available'] = np.where((df['sales_amount'] == 0) & (df['is_available'] == 1), 1, 0).astype(np.int8)\n","\n","    # Function to apply to each group\n","    def calculate_consecutive_zeros(group):\n","        # Step 2: Identify change points to reset the count for consecutive zeros\n","        group['block'] = (group['zero_sales_available'] == 0).cumsum().astype(np.int16)\n","        \n","        # Step 3: Count consecutive zeros within each block\n","        group['consecutive_zero_sales'] = group.groupby('block').cumcount()\n","        \n","        # Reset count where 'zero_sales_available' is 0, as these are not zero sales days or the item is not available\n","        group['consecutive_zero_sales'] = np.where(group['zero_sales_available'] == 1, group['consecutive_zero_sales'], 0).astype(np.int16)\n","        \n","        return group\n","\n","    # Apply the function to each item group\n","    df = df.groupby('id', group_keys=False).apply(calculate_consecutive_zeros)\n","\n","    # Drop the 'block' column because no longer needed\n","    del df['block']\n","\n","    return df\n","########################################################################################################################"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["################################### Function to forecast the next 28 days (This function for case all data in one batch) ###################################\n","def rolling_forecast(model, df_test, df_val, test_x, test_event_x, test_y , val_x, val_y, val_event_x, num_features, num_block_items): #scaler_target\n","    # Set the df_copy, x_copy and y_copy to the correct dataset\n","    if STATUS=='production':\n","        df_copy = df_test.copy()\n","        x_copy = test_x.copy()\n","        y_copy = test_y.copy()\n","        events_copy = test_event_x.copy()    \n","    \n","    elif STATUS=='training':\n","        df_copy = df_val.copy()\n","        x_copy = val_x.copy()\n","        y_copy = val_y.copy()\n","        events_copy = val_event_x.copy()\n","\n","    # return x_copy, events_copy --> (28,28,1259); (28, 28, 4)\n","\n","    # Predict the next 28 days\n","    for i in range(TEST_DUR):\n","        prediction_normalized = model.predict([x_copy[i].reshape(1, DAYS_PER_SEQUENCE, num_features)] + \n","                                              [events_copy[i][:,j].reshape(1, DAYS_PER_SEQUENCE) for j in range(EVENT_LEN)], verbose=0).flatten() \n","\n","        # Impractical to adjust the prepared array, so we will update the df_test copy and use it to create a new array with the updated prediction values\n","        start_idx = DAYS_PER_SEQUENCE*num_block_items+(i*num_block_items)\n","        end_idx = start_idx + num_block_items - 1\n","\n","        # 1. Rolling forecast: Update the df_test copy with the new prediction\n","        df_copy.loc[start_idx:end_idx, TARGET_COL] = prediction_normalized\n","\n","        # 2. Update sales_amount_lag_1, sales_amount_moving_avg_7, sales_amount_moving_avg_28, zero_sales_available, consecutive_zero_sales\n","        df_copy = feature_engineering(df_copy, num_block_items)\n","        # return df_copy\n","\n","\n","\n","\n","\n","\n","\n","        # Create new df for x and y\n","        x_copy, events_copy, _ = create_x_y(df_copy, num_block_items)\n","\n","        # Update the y array with the new prediction\n","        y_copy[i] = prediction_normalized\n","    \n","    # Inverse transform the predictions\n","    predictions_normalized = y_copy\n","    # predictions_original = scaler_target.inverse_transform(y_copy).round(0).astype(int)\n","    predictions_original_raw = np.expm1(y_copy)\n","\n","    predictions_original_rounded = predictions_original_raw.round(0).astype(int)\n","\n","    # Make sure no negative values are returned\n","    predictions_normalized[predictions_normalized < 0] = 0\n","    predictions_original_raw[predictions_original_raw < 0] = 0\n","    predictions_original_rounded[predictions_original_rounded < 0] = 0\n","        \n","    return predictions_original_raw, predictions_original_rounded, predictions_normalized\n","#########################################################################################################"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Create a DataFrame for predictions\n","def prepare_fc_to_file(forecast_df, forecast_array, ids):\n","    # Transpose predictions to match the sample submission format\n","    forecast_array = forecast_array.T\n","\n","    # Create array to write to df\n","    forecast_array = np.concatenate((ids.reshape(len(ids),1), forecast_array), axis=1)\n","\n","    # Create a DataFrame for your predictions\n","    forecast_tmp_df = pd.DataFrame(forecast_array, columns=['id'] + [f'F{i+1}' for i in range(28)])\n","\n","    # concatenate forecast to forecast_df\n","    forecast_df = pd.concat([forecast_df, forecast_tmp_df], axis=0, ignore_index=True)\n","\n","    return forecast_df"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def write_to_csv(forecast_df, dir):\n","    # Get validation data\n","    val_df = pd.read_pickle(VALIDATION_DATA)\n","\n","    # Combine forecast with validation data\n","    forecast_df = pd.concat([val_df, forecast_df], axis=0, ignore_index=True)\n","\n","    # Save the forecast to a csv file\n","    forecast_df.to_csv(dir, index=False)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def tweedie_loss_func(p):\n","    def loss(y_true, y_pred):\n","        # Ensure predictions are strictly positive\n","        epsilon = 1e-10\n","        y_pred = tf.maximum(y_pred, epsilon)\n","\n","        # Tweedie loss calculation\n","        loss = -y_true * tf.pow(y_pred, 1-p) / (1-p) + \\\n","               tf.pow(y_pred, 2-p) / (2-p)\n","        return tf.reduce_mean(loss)\n","    return loss"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.412895Z","iopub.status.busy":"2024-02-04T10:29:02.412579Z","iopub.status.idle":"2024-02-04T10:29:04.224334Z","shell.execute_reply":"2024-02-04T10:29:04.223506Z","shell.execute_reply.started":"2024-02-04T10:29:02.412871Z"},"trusted":true},"outputs":[],"source":["# Neu: Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","def create_lstm_model(input_shape, num_block_items):\n","   model = Sequential([\n","      LSTM(units=80, activation='relu', return_sequences=True, recurrent_dropout=0.1, input_shape=input_shape),\n","      Dropout(0.3),\n","      LSTM(units=40, activation='relu', return_sequences=False, recurrent_dropout=0.1),\n","      Dropout(0.3),\n","      # LSTM(units=40, activation='tanh', return_sequences=False, recurrent_dropout=0.1),\n","      # Dropout(0.1),\n","      Dense(units=num_block_items, activation='relu'), # activation='relu', 'softmax; Final Dense layer for output\n","      Reshape((num_block_items, 1))]) # Reshape the output to be (number of items)\n","\n","   model.compile(optimizer=initial_optimizer, loss=loss, metrics=metrics)\n","\n","   # For tracking purposes: check the models parameters\n","   # model.summary()\n","\n","   return model"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# for each store_id and dept_id call get whole data, filter for store_id and dept_id\n","def lstm_pipeline(verbose, use_embeddings_events):\n","    df_all_data = get_whole_data()\n","\n","    # Get all store_id and dept_id combinations\n","    df_combinations_store_dep = get_combinations(df_all_data)\n","\n","    # Create empty dataframe to store the forecast\n","    forecast_df = pd.DataFrame(columns=['id'] + [f'F{i+1}' for i in range(TEST_DUR)])\n","\n","    # define the number of loops\n","    num_loop = 1 if verbose == 1 else len(df_combinations_store_dep)\n","\n","    # Loop over all store_id and dept_id combinations, create a model, train it, create the prediction and save it to a file\n","    for i in range(0, num_loop):\n","        print(f'Processing {i+1} of {len(df_combinations_store_dep)}: store_id {df_combinations_store_dep.loc[i, \"store_id\"]} and dept_id {df_combinations_store_dep.loc[i, \"dept_id\"]}')\n","        # Filter df down to only the current store_id and dept_id combination\n","        filtered_df, ids, num_batches = filter_df(df_combinations_store_dep, df_all_data, i)\n","\n","        # Calculate the vocab size for the embedding layers later when model is defined\n","        vocab_sizes, embedding_dims = calc_vocab_size(filtered_df) # Funktioniert nur, wenn num_batches 1 ist, sonst muss komplexere Berechnung innerhalb des loops erfolgen\n","\n","        # Loop over all batches\n","        for counter in range(1):#num_batches):\n","            print(f'Processing batch {counter+1} of {num_batches}')\n","                \n","            # Create batches for the current store_id and dept_id combination to avoid memory issues and curse of dimensionality\n","            filtered_df_batch, num_block_items, num_features, input_shape, ids_batch = filtered_df_batches(filtered_df, ids, num_batches, counter)\n","\n","            print(f'Number of ids in this batch: {len(ids_batch)}')\n","                \n","            # Prepare the data for training\n","            filtered_df_batch = prepare_df(filtered_df_batch) #filtered_df_batch, scaler_target\n","\n","            # Split the data into train, validation and test set\n","            df_train, df_val, df_test = train_test_split(filtered_df_batch)\n","\n","            # return df_train\n","\n","            # Create training, validation and test data arrays from the dataframes\n","            train_x, train_event_x, train_y, val_x, val_event_x, val_y, test_x, test_event_x, test_y = get_x_and_y(df_train, df_val, df_test, num_block_items)\n","            # --> train event and val event return (1857, 28, 4) vs (28, 28, 4)         \n","\n","            # If this is the first batch create the model, for subsequent batches retrain the current model with smaller learning rate\n","            if counter == 0 or counter == num_batches - 1:\n","                # Create the model\n","                model = create_lstm_model_embedding(input_shape, num_block_items, vocab_sizes, embedding_dims)\n","\n","                # model = create_lstm_model(input_shape, num_block_items)\n","                epochs = initial_epochs\n","            else:\n","                model = prepare_model_tl(model, subsequent_lr, [])\n","                epochs = subsequent_epochs\n","\n","            # Training the model\n","            model_trained, history = model_training(model, train_x, train_event_x, train_y, \n","                                                    val_x, val_event_x, val_y, \n","                                                    epochs, use_embeddings_events)\n","\n","            # Create the forecast\n","            # df_copy = rolling_forecast(model_trained, df_test, df_val, test_x, test_event_x, test_y, val_x, val_y, val_event_x, num_features, num_block_items)\n","\n","            # return df_copy\n","        \n","\n","        \n","        \n","            # x, events = rolling_forecast(model_trained, df_test, df_val, test_x, test_event_x, test_y, val_x, val_y, val_event_x, scaler_target, num_features, num_block_items)\n","            predictions_original_raw, predictions_original_rounded, predictions_normalized = rolling_forecast(model_trained, df_test, df_val, test_x, test_event_x, test_y, val_x, val_y, val_event_x, num_features, num_block_items)\n","\n","            # Testing the model\n","            if verbose == 1:\n","                # Call eval function to get the evaluation dataframe and some feeling for the results\n","                # df_eval = eval(val_x, val_event_x, val_y, model_trained, num_features)#, scaler_target)\n","                df_eval = eval(predictions_original_rounded, predictions_normalized, val_y, ids_batch)#, scaler_target)\n","            \n","                # Test output for generator\n","                # test_data = test_eval(val_generator, model_trained, scaler_target)\n","\n","            forecast_df = prepare_fc_to_file(forecast_df, predictions_original_raw, ids_batch)\n","            print(\"####################################################\\n\")\n","\n","    if verbose == 0:\n","        write_to_csv(forecast_df, sub_dir + 'sample_submission.csv')\n","        return forecast_df\n","    \n","    if verbose == 1:\n","        return df_eval"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.401718Z","iopub.status.busy":"2024-02-04T10:29:02.401392Z","iopub.status.idle":"2024-02-04T10:29:02.411354Z","shell.execute_reply":"2024-02-04T10:29:02.410459Z","shell.execute_reply.started":"2024-02-04T10:29:02.401688Z"},"trusted":true},"outputs":[],"source":["# Model parameters\n","initial_epochs = 10\n","subsequent_epochs = 6\n","batch_size = 100\n","# Learning rate schedule\n","initial_lr = 0.01\n","decay_steps = 1000\n","alpha = 0.001  # Final learning rate\n","lr_schedule = tf.keras.experimental.CosineDecay(\n","    initial_learning_rate=initial_lr,\n","    decay_steps=decay_steps,\n","    alpha=alpha  # Minimum learning rate value as a fraction of initial_learning_rate.\n",")\n","subsequent_lr = 0.005 # Reduce learning rate by factor of 10 for transfer learning\n","\n","clipvalue = 0.5\n","initializer = GlorotNormal(seed=42)\n","\n","# Model compile parameters\n","loss = tweedie_loss_func(p=1.2) #tf.keras.losses.MeanAbsoluteError() #rmse\n","initial_optimizer = Adam(learning_rate=lr_schedule, clipvalue=clipvalue)\n","metrics = tf.keras.metrics.MeanAbsoluteError()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["# Use functional API to create a model\n","def create_lstm_model_embedding(numerical_input_shape, num_block_items, vocab_sizes, embedding_dims): \n","    numerical_input = Input(shape=numerical_input_shape, name='numerical_input')\n","    event_input = [Input(shape=(DAYS_PER_SEQUENCE,), name=f'event_input_{i}') for i in range(1, 5)]\n","\n","    cat_embeddings = [Embedding(input_dim=vocab_sizes[i], output_dim=embedding_dims[i], input_length=DAYS_PER_SEQUENCE, embeddings_initializer=initializer)(event_input[i]) for i in range(0, 4)]\n","\n","    # Combine numerical input and embeddings\n","    combined_input = concatenate([numerical_input] + cat_embeddings)\n","\n","    # LSTM layer\n","    lstm_out = Bidirectional(LSTM(units=16, activation='tanh', return_sequences=True, recurrent_dropout=0.2, kernel_regularizer=l2(0.1), kernel_initializer=initializer))(combined_input)\n","    dropout = Dropout(0.2)(lstm_out)\n","    lstm_out = LSTM(units=12, activation='tanh', return_sequences=True, recurrent_dropout=0.1, kernel_regularizer=l2(0.1))(dropout)\n","    dropout = Dropout(0.2)(lstm_out)\n","    attention_out = SeqSelfAttention(attention_activation='sigmoid', kernel_initializer=initializer)(dropout)\n","\n","    # Aggregate sequence information\n","    pooled_output = GlobalAveragePooling1D()(attention_out)\n","\n","    # Output layer\n","    output = Dense(num_block_items, kernel_regularizer=l2(0.1), kernel_initializer=initializer)(pooled_output)\n","\n","    # Create and compile the model\n","    model = Model(inputs=[numerical_input] + event_input, outputs=output)\n","\n","    model.compile(optimizer=initial_optimizer, loss=loss, metrics=metrics)\n","\n","    return model"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing 1 of 70: store_id CA_1 and dept_id HOBBIES_1\n","Processing batch 1 of 11\n","Number of ids in this batch: 40\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["2024-02-27 11:38:35.655671: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"]},{"name":"stdout","output_type":"stream","text":["19/19 [==============================] - 4s 62ms/step - loss: 50.1506 - mean_absolute_error: 0.4862 - val_loss: 17.2549 - val_mean_absolute_error: 0.5182\n","Epoch 2/10\n","19/19 [==============================] - 1s 40ms/step - loss: 8.4962 - mean_absolute_error: 0.4628 - val_loss: 7.1477 - val_mean_absolute_error: 0.5127\n","Epoch 3/10\n","19/19 [==============================] - 1s 40ms/step - loss: 4.3983 - mean_absolute_error: 0.4611 - val_loss: 5.8646 - val_mean_absolute_error: 0.5018\n","Epoch 4/10\n","19/19 [==============================] - 1s 42ms/step - loss: 3.5986 - mean_absolute_error: 0.4555 - val_loss: 5.4311 - val_mean_absolute_error: 0.4919\n","Epoch 5/10\n","19/19 [==============================] - 1s 43ms/step - loss: 3.3510 - mean_absolute_error: 0.4520 - val_loss: 6.1969 - val_mean_absolute_error: 0.4863\n","Epoch 6/10\n","19/19 [==============================] - 1s 55ms/step - loss: 3.2703 - mean_absolute_error: 0.4524 - val_loss: 5.2376 - val_mean_absolute_error: 0.4867\n","Epoch 7/10\n","19/19 [==============================] - 1s 71ms/step - loss: 3.2360 - mean_absolute_error: 0.4498 - val_loss: 5.2056 - val_mean_absolute_error: 0.4812\n","Epoch 8/10\n","19/19 [==============================] - 1s 57ms/step - loss: 3.2036 - mean_absolute_error: 0.4489 - val_loss: 5.1936 - val_mean_absolute_error: 0.4766\n","Epoch 9/10\n","19/19 [==============================] - 1s 60ms/step - loss: 3.1910 - mean_absolute_error: 0.4467 - val_loss: 5.1727 - val_mean_absolute_error: 0.4733\n","Epoch 10/10\n","19/19 [==============================] - 1s 59ms/step - loss: 3.1831 - mean_absolute_error: 0.4461 - val_loss: 5.1683 - val_mean_absolute_error: 0.4705\n","predict day:  1\n","predict day:  2\n","predict day:  3\n","predict day:  4\n","predict day:  5\n","predict day:  6\n","predict day:  7\n","predict day:  8\n","predict day:  9\n","predict day:  10\n","predict day:  11\n","predict day:  12\n","predict day:  13\n","predict day:  14\n","predict day:  15\n","predict day:  16\n","predict day:  17\n","predict day:  18\n","predict day:  19\n","predict day:  20\n","predict day:  21\n","predict day:  22\n","predict day:  23\n","predict day:  24\n","predict day:  25\n","predict day:  26\n","predict day:  27\n","predict day:  28\n","####################################################\n","\n"]}],"source":["forecast_df = lstm_pipeline(verbose=1, use_embeddings_events=True)\n","# forecast_df = lstm_pipeline(verbose=1, use_embeddings_events=True)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>day</th>\n","      <th>actual_normalized</th>\n","      <th>pred_normalized</th>\n","      <th>difference_norm</th>\n","      <th>actual_inv</th>\n","      <th>pred_inv</th>\n","      <th>difference_inv</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0.173935</td>\n","      <td>-0.173935</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>2</td>\n","      <td>0.693147</td>\n","      <td>0.172634</td>\n","      <td>0.520514</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>81</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>3</td>\n","      <td>0.000000</td>\n","      <td>0.172662</td>\n","      <td>-0.172662</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>121</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>4</td>\n","      <td>0.000000</td>\n","      <td>0.172679</td>\n","      <td>-0.172679</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>161</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>5</td>\n","      <td>0.000000</td>\n","      <td>0.172688</td>\n","      <td>-0.172688</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>201</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>6</td>\n","      <td>0.000000</td>\n","      <td>0.172697</td>\n","      <td>-0.172697</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>241</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>7</td>\n","      <td>0.000000</td>\n","      <td>0.172699</td>\n","      <td>-0.172699</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>281</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>8</td>\n","      <td>0.000000</td>\n","      <td>0.172692</td>\n","      <td>-0.172692</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>321</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>9</td>\n","      <td>0.000000</td>\n","      <td>0.172679</td>\n","      <td>-0.172679</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>361</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>10</td>\n","      <td>0.693147</td>\n","      <td>0.172654</td>\n","      <td>0.520493</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>401</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>11</td>\n","      <td>0.000000</td>\n","      <td>0.172624</td>\n","      <td>-0.172624</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>441</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>12</td>\n","      <td>0.000000</td>\n","      <td>0.172594</td>\n","      <td>-0.172594</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>481</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>13</td>\n","      <td>0.000000</td>\n","      <td>0.172557</td>\n","      <td>-0.172557</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>521</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>14</td>\n","      <td>0.000000</td>\n","      <td>0.172526</td>\n","      <td>-0.172526</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>561</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>15</td>\n","      <td>0.000000</td>\n","      <td>0.172487</td>\n","      <td>-0.172487</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>601</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>16</td>\n","      <td>0.000000</td>\n","      <td>0.172443</td>\n","      <td>-0.172443</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>641</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>17</td>\n","      <td>0.000000</td>\n","      <td>0.172392</td>\n","      <td>-0.172392</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>681</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>18</td>\n","      <td>0.000000</td>\n","      <td>0.172331</td>\n","      <td>-0.172331</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>721</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>19</td>\n","      <td>0.000000</td>\n","      <td>0.172270</td>\n","      <td>-0.172270</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>761</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>20</td>\n","      <td>0.693147</td>\n","      <td>0.172193</td>\n","      <td>0.520955</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>801</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>21</td>\n","      <td>1.098612</td>\n","      <td>0.172113</td>\n","      <td>0.926500</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>841</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>22</td>\n","      <td>0.693147</td>\n","      <td>0.172055</td>\n","      <td>0.521092</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>881</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>23</td>\n","      <td>0.693147</td>\n","      <td>0.171992</td>\n","      <td>0.521155</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>921</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>24</td>\n","      <td>0.000000</td>\n","      <td>0.171917</td>\n","      <td>-0.171917</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>961</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>25</td>\n","      <td>0.000000</td>\n","      <td>0.171844</td>\n","      <td>-0.171844</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1001</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>26</td>\n","      <td>0.000000</td>\n","      <td>0.171761</td>\n","      <td>-0.171761</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1041</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>27</td>\n","      <td>0.000000</td>\n","      <td>0.171652</td>\n","      <td>-0.171652</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1081</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>28</td>\n","      <td>0.000000</td>\n","      <td>0.171561</td>\n","      <td>-0.171561</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 id  day  actual_normalized  pred_normalized  \\\n","1     HOBBIES_1_002_CA_1_evaluation    1           0.000000         0.173935   \n","41    HOBBIES_1_002_CA_1_evaluation    2           0.693147         0.172634   \n","81    HOBBIES_1_002_CA_1_evaluation    3           0.000000         0.172662   \n","121   HOBBIES_1_002_CA_1_evaluation    4           0.000000         0.172679   \n","161   HOBBIES_1_002_CA_1_evaluation    5           0.000000         0.172688   \n","201   HOBBIES_1_002_CA_1_evaluation    6           0.000000         0.172697   \n","241   HOBBIES_1_002_CA_1_evaluation    7           0.000000         0.172699   \n","281   HOBBIES_1_002_CA_1_evaluation    8           0.000000         0.172692   \n","321   HOBBIES_1_002_CA_1_evaluation    9           0.000000         0.172679   \n","361   HOBBIES_1_002_CA_1_evaluation   10           0.693147         0.172654   \n","401   HOBBIES_1_002_CA_1_evaluation   11           0.000000         0.172624   \n","441   HOBBIES_1_002_CA_1_evaluation   12           0.000000         0.172594   \n","481   HOBBIES_1_002_CA_1_evaluation   13           0.000000         0.172557   \n","521   HOBBIES_1_002_CA_1_evaluation   14           0.000000         0.172526   \n","561   HOBBIES_1_002_CA_1_evaluation   15           0.000000         0.172487   \n","601   HOBBIES_1_002_CA_1_evaluation   16           0.000000         0.172443   \n","641   HOBBIES_1_002_CA_1_evaluation   17           0.000000         0.172392   \n","681   HOBBIES_1_002_CA_1_evaluation   18           0.000000         0.172331   \n","721   HOBBIES_1_002_CA_1_evaluation   19           0.000000         0.172270   \n","761   HOBBIES_1_002_CA_1_evaluation   20           0.693147         0.172193   \n","801   HOBBIES_1_002_CA_1_evaluation   21           1.098612         0.172113   \n","841   HOBBIES_1_002_CA_1_evaluation   22           0.693147         0.172055   \n","881   HOBBIES_1_002_CA_1_evaluation   23           0.693147         0.171992   \n","921   HOBBIES_1_002_CA_1_evaluation   24           0.000000         0.171917   \n","961   HOBBIES_1_002_CA_1_evaluation   25           0.000000         0.171844   \n","1001  HOBBIES_1_002_CA_1_evaluation   26           0.000000         0.171761   \n","1041  HOBBIES_1_002_CA_1_evaluation   27           0.000000         0.171652   \n","1081  HOBBIES_1_002_CA_1_evaluation   28           0.000000         0.171561   \n","\n","      difference_norm  actual_inv  pred_inv  difference_inv  \n","1           -0.173935           0         0               0  \n","41           0.520514           1         0               1  \n","81          -0.172662           0         0               0  \n","121         -0.172679           0         0               0  \n","161         -0.172688           0         0               0  \n","201         -0.172697           0         0               0  \n","241         -0.172699           0         0               0  \n","281         -0.172692           0         0               0  \n","321         -0.172679           0         0               0  \n","361          0.520493           1         0               1  \n","401         -0.172624           0         0               0  \n","441         -0.172594           0         0               0  \n","481         -0.172557           0         0               0  \n","521         -0.172526           0         0               0  \n","561         -0.172487           0         0               0  \n","601         -0.172443           0         0               0  \n","641         -0.172392           0         0               0  \n","681         -0.172331           0         0               0  \n","721         -0.172270           0         0               0  \n","761          0.520955           1         0               1  \n","801          0.926500           2         0               2  \n","841          0.521092           1         0               1  \n","881          0.521155           1         0               1  \n","921         -0.171917           0         0               0  \n","961         -0.171844           0         0               0  \n","1001        -0.171761           0         0               0  \n","1041        -0.171652           0         0               0  \n","1081        -0.171561           0         0               0  "]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# forecast_df\n","forecast_df[forecast_df['id']=='HOBBIES_1_002_CA_1_evaluation'].head(30)\n","# forecast_df.iloc[4::MAX_BATCH_SIZE,]"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
