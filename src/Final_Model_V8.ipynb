{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.714019Z","iopub.status.busy":"2024-02-04T10:28:45.713557Z","iopub.status.idle":"2024-02-04T10:28:45.724837Z","shell.execute_reply":"2024-02-04T10:28:45.723860Z","shell.execute_reply.started":"2024-02-04T10:28:45.713975Z"},"trusted":true},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V3_ohne_Cat_features_block_items'\n","CODE_ENV = 'local' #'kaggle', 'aws', 'local'\n","STATUS = 'training'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.726757Z","iopub.status.busy":"2024-02-04T10:28:45.726461Z","iopub.status.idle":"2024-02-04T10:28:49.601403Z","shell.execute_reply":"2024-02-04T10:28:49.600379Z","shell.execute_reply.started":"2024-02-04T10:28:45.726733Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import os\n","import shutil\n","from pathlib import Path\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import (Input, LSTM, Dense, Embedding, Dropout, Reshape, \n","                                     concatenate, Flatten, Bidirectional, GlobalAveragePooling1D)\n","from tensorflow.keras.optimizers.legacy import Adam\n","from tensorflow.keras.losses import Huber, MeanAbsoluteError\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.initializers import GlorotNormal\n","from tensorflow.keras.callbacks import Callback, EarlyStopping\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from keras_self_attention import SeqSelfAttention\n","from keras_tuner import HyperModel\n","from keras_tuner.tuners import RandomSearch"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.603096Z","iopub.status.busy":"2024-02-04T10:28:49.602532Z","iopub.status.idle":"2024-02-04T10:28:49.707155Z","shell.execute_reply":"2024-02-04T10:28:49.706050Z","shell.execute_reply.started":"2024-02-04T10:28:49.603066Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n","False\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.710168Z","iopub.status.busy":"2024-02-04T10:28:49.709769Z","iopub.status.idle":"2024-02-04T10:28:49.717151Z","shell.execute_reply":"2024-02-04T10:28:49.716162Z","shell.execute_reply.started":"2024-02-04T10:28:49.710139Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files\n","    LOGGING_DIR = src_dir + 'models/hyperparameter_tuning'\n","    CSV_PATH = LOGGING_DIR + '/hyperparameter_search_results.csv'\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data/'\n","    src_dir = '/kaggle/working/'\n","    sub_dir = src_dir + 'submissions/'\n","    LOGGING_DIR = src_dir + 'models/hyperparameter_tuning'\n","    CSV_PATH = LOGGING_DIR + '/hyperparameter_search_results.csv'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.718565Z","iopub.status.busy":"2024-02-04T10:28:49.718293Z","iopub.status.idle":"2024-02-04T10:28:49.727911Z","shell.execute_reply":"2024-02-04T10:28:49.727014Z","shell.execute_reply.started":"2024-02-04T10:28:49.718540Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","VALIDATION_DATA  = prc_dir +'df_1.pkl' # Validation data\n","BASE      = prc_dir +'df_2.pkl' # Base data\n","CALENDAR  = prc_dir +'df_3.pkl' # Calendar data\n","# NUM_ITEMS = 30490 # Number of items per each day\n","\n","DAYS_PER_SEQUENCE = 28  # Length of the sequence\n","MAX_BATCH_SIZE = 900 # Maximum number of ids to be used in each batch to avoid memory issues and curse of dimensionality\n","\n","\n","TARGET_COL = 'sales_amount'\n","# REPEATED_FEATURES = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","REPEATED_FEATURES = ['sales_amount', 'sell_price', 'is_available',\n","                     'sales_amount_moving_avg_7', 'sales_amount_moving_avg_28', 'sales_amount_lag_1',\n","                     'zero_sales_available', 'consecutive_zero_sales'] # List to hold all feature columns that are used for each item\n","SALES_AMOUNT_COLS = ['sales_amount', 'sales_amount_moving_avg_7', 'sales_amount_moving_avg_28', 'sales_amount_lag_1']\n","# ONCE_ONLY_FEATURES = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","ONCE_ONLY_FEATURES = ['snap_CA', 'snap_TX', 'snap_WI', 'mday_normalized', 'day_continuous_normalized',\n","                      'month_sin', 'month_cos', 'wday_sin', 'wday_cos', 'week_sin', 'week_cos', \n","                      'year_normalized'] # List to hold feature columns that are not repeated for each item\n","EVENT_COLS = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n","EVENT_LEN = len(EVENT_COLS)\n","NOT_NEEDED_COLS = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Set test_end to 1969 in case of production\n","if STATUS=='production':\n","    TEST_END = 1969\n","elif STATUS=='training':\n","    TEST_END = 1969 #1941\n","\n","# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.729350Z","iopub.status.busy":"2024-02-04T10:28:49.728998Z","iopub.status.idle":"2024-02-04T10:28:53.163392Z","shell.execute_reply":"2024-02-04T10:28:53.162542Z","shell.execute_reply.started":"2024-02-04T10:28:49.729323Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","def get_whole_data():\n","    df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)\n","    return df_all_data"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# df_all_data = get_whole_data()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# get all ['store_id','dept_id'] combinations from df_all_data and count the occurences\n","# df_combinations = df_all_data[(df_all_data['d']==1)].groupby(['store_id', 'dept_id']).size().reset_index(name='count')  \n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#pd.set_option('display.max_rows', None)\n","# df_combinations[df_combinations['store_id'] == 'TX_1'].sort_values('count').tail(50)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Return a df with all unique combinations of store_id and dept_id\n","def get_combinations(df_all_data):\n","    # get all store_id and dept_id combinations\n","    df_combinations_store_dep = df_all_data[['store_id','dept_id']].drop_duplicates().reset_index(drop=True)\n","    # get the length of the df_combinations_store_dep\n","    df_length = len(df_combinations_store_dep)\n","\n","    return df_combinations_store_dep, df_length"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Filter df down to only the current store_id and dept_id combination\n","def filter_df(df_combinations_store_dep, df_all_data, i):\n","    store_id = df_combinations_store_dep.loc[i, 'store_id']\n","    dept_id = df_combinations_store_dep.loc[i, 'dept_id']\n","    ids = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)]['id'].drop_duplicates().values\n","    filtered_df = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)].reset_index(drop=True)\n","    filtered_df.reset_index(drop=True, inplace=True) ##################################################????\n","\n","    # Remove all unused columns\n","    filtered_df.drop(NOT_NEEDED_COLS, axis=1, inplace=True)\n","\n","    # Get the number of block items\n","    num_block_items = len(ids)\n","\n","    # Get the number of features\n","    num_features = len(ONCE_ONLY_FEATURES) + len(REPEATED_FEATURES) * num_block_items # Calculate the number of features\n","\n","    # Get the input shape later on for the model\n","    input_shape = (DAYS_PER_SEQUENCE, num_features)\n","\n","    return filtered_df, num_block_items, num_features, input_shape"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def calc_vocab_size(filtered_df, embedding_dims_max=50):\n","    vocab_size=[]\n","    embedding_dims=[]\n","    # count the unique entries of event_name_1 event_type_1 event_name_2 event_type_2\n","    # append the number of unique entries to the list vocab_size\n","    vocab_size.append(len(filtered_df['event_name_1'].unique()))\n","    vocab_size.append(len(filtered_df['event_type_1'].unique()))\n","    vocab_size.append(len(filtered_df['event_name_2'].unique()))\n","    vocab_size.append(len(filtered_df['event_type_2'].unique()))\n","    \n","    # loop over all other indices and calculate the embedding dimensions\n","    for i in range(0, len(vocab_size)):\n","        embedding_dims.append(int(embedding_dims_max * (vocab_size[i]/max(vocab_size))))\n","\n","    return vocab_size, embedding_dims"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Normalize numerical columns\n","def prepare_df(df_all_data):\n","    # Define categorical and numerical columns\n","    categorical_cols = ['id'] #'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'snap_CA', 'snap_TX', 'snap_WI', 'is_available'\n","    \n","    numerical_cols = ['sell_price']\n","\n","    # Convert categorical columns to category dtype and encode with cat.codes\n","    for col in categorical_cols:\n","        df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","    # Adjust the event cols\n","    # 1. Create an encoder instance for each column\n","    encoders = {col: LabelEncoder() for col in EVENT_COLS}\n","\n","    # Apply encoding to each column\n","    for col, encoder in encoders.items():\n","        df_all_data[col] = encoder.fit_transform(df_all_data[col].astype(str)).astype('int8')\n","\n","    # Normalize numerical columns\n","    scaler_numerical = MinMaxScaler()\n","    df_all_data[numerical_cols] = scaler_numerical.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","\n","    # scaler_target = MinMaxScaler() #not used any more\n","    # df_all_data[SALES_AMOUNT_COLS] = scaler_target.fit_transform(df_all_data[SALES_AMOUNT_COLS].astype(np.float64))\n","    df_all_data[SALES_AMOUNT_COLS] = df_all_data[SALES_AMOUNT_COLS].apply(np.log1p)\n","\n","    return df_all_data#, scaler_target"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.326509Z","iopub.status.busy":"2024-02-04T10:29:02.326211Z","iopub.status.idle":"2024-02-04T10:29:02.341825Z","shell.execute_reply":"2024-02-04T10:29:02.340817Z","shell.execute_reply.started":"2024-02-04T10:29:02.326485Z"},"trusted":true},"outputs":[],"source":["def train_test_split(df_all_data):\n","    # For training split up between train and validation dataset, else use all for training and create test dataset\n","    if STATUS=='training':\n","        df_train = df_all_data[df_all_data['d'] <= TRAIN_END].reset_index(drop=True)\n","        df_val   = df_all_data[(df_all_data['d'] > TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] <= VAL_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_test  = None\n","        \n","    elif STATUS=='production':\n","        df_train = df_all_data[df_all_data['d'] <= VAL_END].reset_index(drop=True)\n","        df_test  = df_all_data[(df_all_data['d'] > VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] <= TEST_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_val   = None\n","\n","    # Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","    del df_all_data\n","\n","    return df_train, df_val, df_test"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["### Create x and y in one go without the generator version autogeneration ###\n","def create_x_y(df, num_block_items):\n","    length_days = len(df) // num_block_items\n","    x = []\n","    y = []\n","    events = []\n","\n","    for i in range(0, length_days - DAYS_PER_SEQUENCE):\n","        start_ind = i * num_block_items\n","        end_ind = start_ind + num_block_items * (DAYS_PER_SEQUENCE)  # predict the next day after the sequence\n","\n","        # Extract once-only features for all days in the sequence at once\n","        once_features = df.iloc[start_ind:end_ind:num_block_items][ONCE_ONLY_FEATURES].to_numpy()\n","\n","        # Get event columns\n","        event_features = df.iloc[start_ind:end_ind:num_block_items][EVENT_COLS].to_numpy()\n","\n","        # Extract repeated features for all items and days at once\n","        repeated_features_stack = df.iloc[start_ind:end_ind][REPEATED_FEATURES].to_numpy() # 210,000 items, 10 features\n","\n","        # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","        reshaped_3d = repeated_features_stack.reshape(DAYS_PER_SEQUENCE, num_block_items, len(REPEATED_FEATURES))\n","\n","        # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","        final_array = reshaped_3d.reshape(DAYS_PER_SEQUENCE, -1)\n","\n","        # Combine once-only and repeated features\n","        batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","        # Extract targets\n","        batch_targets = df.iloc[end_ind:end_ind + num_block_items][[TARGET_COL]].to_numpy().flatten()\n","\n","        # Append to x, y and events\n","        x.append(batch_sequences)\n","        events.append(event_features)\n","        y.append(batch_targets)\n","\n","    train_x = np.array(x)\n","    train_event_x = np.array(events)\n","    train_y = np.array(y)\n","\n","    train_x = [train_x, train_event_x[:,:,0], train_event_x[:,:,1], train_event_x[:,:,2], train_event_x[:,:,3]]\n","\n","    return train_x, train_y\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.355635Z","iopub.status.busy":"2024-02-04T10:29:02.355210Z","iopub.status.idle":"2024-02-04T10:29:02.365871Z","shell.execute_reply":"2024-02-04T10:29:02.364973Z","shell.execute_reply.started":"2024-02-04T10:29:02.355590Z"},"trusted":true},"outputs":[],"source":["# Get the training data and labels array for the LSTM model\n","def get_x_and_y(df_train, df_val, df_test, num_block_items):\n","    train_x, train_y = create_x_y(df_train, num_block_items)\n","\n","    if STATUS=='training':\n","        val_x, val_y = create_x_y(df_val, num_block_items)\n","    elif STATUS=='production': \n","        val_x, val_y = None, None\n","\n","    # df_train not needed anymore\n","    del df_train\n","\n","    return train_x, train_y, val_x, val_y"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.386793Z","iopub.status.busy":"2024-02-04T10:29:02.386108Z","iopub.status.idle":"2024-02-04T10:29:02.392710Z","shell.execute_reply":"2024-02-04T10:29:02.391436Z","shell.execute_reply.started":"2024-02-04T10:29:02.386757Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:04.259644Z","iopub.status.busy":"2024-02-04T10:29:04.259265Z","iopub.status.idle":"2024-02-04T10:29:04.264825Z","shell.execute_reply":"2024-02-04T10:29:04.263758Z","shell.execute_reply.started":"2024-02-04T10:29:04.259591Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Perform feature engineering\n","\"\"\"\n","#####- these columns have to be update in the next notebook based on the predictions made by the model #####\n","- 1 days lag #float 64\n","- moving average for 7 and 28 days #float 64\n","- is there a price reduction?\n","- is there a price increase?\n","- adjust for inflation?\n","- consumer sentiment\n","- holiday\n","- weather\n","- \n","\"\"\"\n","def feature_engineering(df, num_block_items): \n","    ################## lag 1 day sales amount ##############################################################################\n","    # After shifting the first days values are NAN but not important as we skip them because we start with the second day\n","    df['sales_amount_lag_1'] = df['sales_amount'].shift(num_block_items)\n","    ########################################################################################################################\n","\n","    ################## moving average 7 and 28 days #########################\n","    df['sales_amount_moving_avg_7'] = df.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=7).mean())\n","    df['sales_amount_moving_avg_7'] = df['sales_amount_moving_avg_7'].fillna(method='bfill')\n","\n","    df['sales_amount_moving_avg_28'] = df.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=28).mean())\n","    df['sales_amount_moving_avg_28'] = df['sales_amount_moving_avg_28'].fillna(method='bfill')\n","    #########################################################################\n","\n","    ################# days consecutive zero sales and if an entry means that this is a zero sale  #########################\n","    # Step 1: Mark zero sales days where item is available\n","    df['zero_sales_available'] = np.where((df['sales_amount'] == 0) & (df['is_available'] == 1), 1, 0).astype(np.int8)\n","\n","    # Function to apply to each group\n","    def calculate_consecutive_zeros(group):\n","        # Step 2: Identify change points to reset the count for consecutive zeros\n","        group['block'] = (group['zero_sales_available'] == 0).cumsum().astype(np.int16)\n","        \n","        # Step 3: Count consecutive zeros within each block\n","        group['consecutive_zero_sales'] = group.groupby('block').cumcount()\n","        \n","        # Reset count where 'zero_sales_available' is 0, as these are not zero sales days or the item is not available\n","        group['consecutive_zero_sales'] = np.where(group['zero_sales_available'] == 1, group['consecutive_zero_sales'], 0).astype(np.int16)\n","        \n","        return group\n","\n","    # Apply the function to each item group\n","    df = df.groupby('id', group_keys=False).apply(calculate_consecutive_zeros)\n","\n","    # Drop the 'block' column because no longer needed\n","    del df['block']\n","\n","    return df\n","########################################################################################################################"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# for each store_id and dept_id call get whole data, filter for store_id and dept_id\n","def lstm_pipeline(verbose):\n","    #Get all data\n","    df_all_data = get_whole_data()\n","\n","    # Get all store_id and dept_id combinations\n","    df_combinations_store_dep, num_combinations = get_combinations(df_all_data)\n","\n","    # define the number of loops\n","    num_loop = 1 if verbose == 1 else num_combinations\n","\n","    # Loop over all store_id and dept_id combinations, create a model, train it, create the prediction and save it to a file\n","    for counter in range(0, num_loop):\n","        ############## For debugging purposes ##############\n","        store_id = df_combinations_store_dep.loc[counter, \"store_id\"]\n","        dept_id = df_combinations_store_dep.loc[counter, \"dept_id\"]\n","\n","        print(f'Processing {counter+1} of {num_combinations}: store_id {store_id} and dept_id {dept_id}')\n","        ####################################################\n","\n","        # Filter df down to only the current store_id and dept_id combination\n","        filtered_df, num_block_items, num_features, input_shape = filter_df(df_combinations_store_dep, df_all_data, counter)\n","\n","        ############## For debugging purposes ##############\n","        print(f'Number of ids in this batch: {num_block_items}')\n","        ####################################################\n","\n","        # Calculate the vocab size for the embedding layers later when model is defined\n","        vocab_sizes, embedding_dims = calc_vocab_size(filtered_df) # Funktioniert nur, wenn num_batches 1 ist, sonst muss komplexere Berechnung innerhalb des loops erfolgen\n","\n","        # Prepare the data for training\n","        filtered_df = prepare_df(filtered_df)\n","\n","        # Split the data into train, validation and test set\n","        df_train, df_val, df_test = train_test_split(filtered_df)\n","\n","        # Create training, validation and test data arrays from the dataframes\n","        train_x, train_y, val_x, val_y = get_x_and_y(df_train, df_val, df_test, num_block_items)\n","\n","        # Delete directory for a clean new run and loggin\n","        remove_directory(LOGGING_DIR)\n","\n","        # Do grid search and log to file\n","        start_search(train_x, train_y,\n","                     val_x, val_y,\n","                     input_shape, num_block_items, vocab_sizes,\n","                     embedding_dims, counter, store_id, dept_id,\n","                     csv_path=CSV_PATH)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Remove the logging directory such that not old states are used before new grid search runs begin\n","def remove_directory(dir_path):\n","    shutil.rmtree(Path(dir_path))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# def tweedie_loss_func(p):\n","#     def loss(y_true, y_pred):\n","#         # Ensure predictions are strictly positive\n","#         epsilon = 1e-10\n","#         y_pred = tf.maximum(y_pred, epsilon)\n","\n","#         # Tweedie loss calculation\n","#         loss = -y_true * tf.pow(y_pred, 1-p) / (1-p) + \\\n","#                tf.pow(y_pred, 2-p) / (2-p)\n","#         return tf.reduce_mean(loss)\n","#     return loss\n","\n","\n","\n","class TweedieLoss(tf.keras.losses.Loss):\n","    def __init__(self, p, name=\"TweedieLoss\"):\n","        super().__init__(name=name)\n","        self.p = p\n","\n","    def call(self, y_true, y_pred):\n","        # Ensure predictions are strictly positive\n","        epsilon = 1e-8\n","        y_pred = tf.maximum(y_pred, epsilon)\n","\n","        # Tweedie loss calculation\n","        loss = -y_true * tf.pow(y_pred, 1 - self.p) / (1 - self.p) + \\\n","               tf.pow(y_pred, 2 - self.p) / (2 - self.p)\n","        return tf.reduce_mean(loss)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def quantile_loss(q, y_true, y_pred):\n","    e = y_true - y_pred\n","    return tf.reduce_mean(tf.maximum(q * e, (q - 1) * e), axis=-1)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def custom_loss_wrapper(p_value, q_value, loss_choice):\n","    def custom_loss(y_true, y_pred):\n","        if loss_choice == 'tweedie':\n","            return TweedieLoss(p_value)(y_true, y_pred)\n","        elif loss_choice == 'mean_absolute_error':\n","            return MeanAbsoluteError()(y_true, y_pred)\n","        elif loss_choice == 'huber':\n","            return Huber()(y_true, y_pred)\n","        elif loss_choice == 'quantile_loss':\n","            return quantile_loss(q_value, y_true, y_pred)\n","    return custom_loss"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Prepare model and model params for hyperparameter tuning with grid search\n","class LSTMTuningModel(HyperModel):\n","    def __init__(self, input_shape, num_block_items, vocab_sizes, embedding_dims, metrics):\n","        self.input_shape = input_shape\n","        self.num_block_items = num_block_items\n","        self.vocab_sizes = vocab_sizes\n","        self.embedding_dims = embedding_dims\n","        self.metrics = metrics\n","    \n","    def build(self, hp):\n","        ################################ Hyperparameters ################################\n","        # Embeddings\n","        # embedding_dim_options = [16, 32, 64]\n","\n","        # Give global min max values for layers\n","        num_lstm_layers_min = 0\n","        num_lstm_layers_max = 3\n","\n","        lstm_units_min      = 16\n","        lstm_units_max      = 128\n","        lstm_step_size      = 16\n","\n","        activation_choice   = ['relu', 'tanh', 'sigmoid']\n","        self_att_activation = ['sigmoid', 'softmax', 'tanh', 'linear']\n","\n","        recurr_dropout_min  = 0.1\n","        recurr_dropout_max  = 0.5\n","        recurr_dropout_step = 0.1\n","\n","        l2_reg_min          = 1e-4\n","        l2_reg_max          = 1e-2\n","\n","        dropout_rate_min    = 0.1\n","        dropout_rate_max    = 0.5\n","        dropout_rate_step   = 0.1\n","\n","        # Learning rate\n","        initial_learning_rate = hp.Float('initial_learning_rate', 1e-4, 1e-2, sampling='log')\n","\n","        # Loss\n","        p_value               = hp.Float('p_value', 1.0, 1.5, step=0.1)\n","        q_value               = hp.Float('quantile', 0.8, 0.95, step=0.05)\n","        loss_choice           = hp.Choice('loss_function', values=['tweedie']) #'quantile_loss', 'mean_absolute_error', 'huber', \n","        ####################################################################################\n","\n","        ############################# Model architecture #####################################\n","        # Inputs\n","        numerical_input = Input(shape=self.input_shape, name='numerical_input')\n","        event_input = [Input(shape=(DAYS_PER_SEQUENCE,), name=f'event_input_{i}') for i in range(1, 5)]\n","        \n","        initializer = GlorotNormal(seed=42)\n","\n","        # Embeddings\n","        cat_embeddings = [Embedding(input_dim=self.vocab_sizes[i], \n","                                    output_dim=self.embedding_dims[i], \n","                                    input_length=DAYS_PER_SEQUENCE, \n","                                    embeddings_initializer=initializer)(event_input[i]) for i in range(4)]\n","\n","        combined_input = concatenate([numerical_input] + cat_embeddings)\n","\n","        # Define the layers, at least 1 Bidirectional layer\n","        lstm_out = Bidirectional(\n","                    LSTM(units=hp.Int('lstm_units_bidirectional', lstm_units_min, lstm_units_max, lstm_step_size),\n","                        activation=hp.Choice('activation', values=activation_choice),\n","                        return_sequences=True,\n","                        recurrent_dropout=hp.Float('recurrent_dropout_bidirectional', recurr_dropout_min, recurr_dropout_max, recurr_dropout_step), \n","                        kernel_regularizer=l2(hp.Float('l2_reg_bidirectional', l2_reg_min, l2_reg_max, sampling='log')), \n","                        kernel_initializer=initializer))(combined_input)\n","        lstm_out = Dropout(hp.Float('dropout_1', dropout_rate_min, dropout_rate_max, dropout_rate_step))(lstm_out)\n","\n","        # Varying numbers of following LSTM layers possible\n","        for i in range(0, hp.Int('num_lstm_layers', num_lstm_layers_min, num_lstm_layers_max)):\n","            lstm_out = LSTM(\n","                units=hp.Int(f'lstm_units_{i}', lstm_units_min, lstm_units_max, lstm_step_size),\n","                activation=hp.Choice(f'activation_{i}', values=activation_choice),  \n","                return_sequences=True,\n","                recurrent_dropout=hp.Float(f'lstm_dropout_{i}', recurr_dropout_min, recurr_dropout_max, recurr_dropout_step),\n","                kernel_regularizer=l2(hp.Float(f'l2_reg_lstm_{i}', l2_reg_min, l2_reg_max, sampling='log')),\n","                kernel_initializer=initializer)(lstm_out)\n","            lstm_out = Dropout(hp.Float(f'dropout_{i}', dropout_rate_min, dropout_rate_max, dropout_rate_step))(lstm_out)\n","\n","        # Attention layer\n","        attention_out = SeqSelfAttention(\n","                            attention_activation=hp.Choice('activation_self_att', \n","                            values=self_att_activation), \n","                            kernel_initializer=initializer)(lstm_out)\n","        pooled_output = GlobalAveragePooling1D()(attention_out)\n","        output = Dense(\n","                    self.num_block_items, \n","                    kernel_regularizer=l2(hp.Float('l2_reg_dense', l2_reg_min, l2_reg_max, sampling='log')),\n","                    kernel_initializer=initializer)(pooled_output)\n","\n","        model = Model(inputs=[numerical_input] + event_input, outputs=output)\n","        \n","        ####################################################################################\n","\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=initial_learning_rate,\n","            decay_steps=1000,\n","            decay_rate=0.9)\n","\n","        model.compile(\n","            optimizer=Adam(learning_rate=lr_schedule, clipvalue=0.5),\n","            loss=custom_loss_wrapper(p_value, q_value, loss_choice),\n","            metrics=[self.metrics])\n","\n","        return model#, batch_size"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def create_tuner(input_shape, num_block_items, vocab_sizes, embedding_dims, counter, store_id, dept_id):\n","    hypermodel = LSTMTuningModel(\n","        input_shape=input_shape, \n","        num_block_items=num_block_items, \n","        vocab_sizes=vocab_sizes, \n","        embedding_dims=embedding_dims, \n","        metrics=MeanAbsoluteError())\n","\n","    # Setting up the tuner, for example, using RandomSearch\n","    tuner = RandomSearch(\n","        hypermodel,\n","        objective='val_loss',\n","        max_trials=10, #The maximum number of hyperparameter combinations to test. This sets the upper limit on how many model configurations will be evaluated.\n","        executions_per_trial=2,\n","        directory=LOGGING_DIR,\n","        project_name='tuning_batch_' + str(counter) + '_storeId_' + str(store_id) + '_deptId_' + str(dept_id))\n","\n","    return tuner"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def start_search(train_x, train_y, val_x, val_y, input_shape, num_block_items, vocab_sizes, embedding_dims, counter, store_id, dept_id, csv_path):\n","    batch_sizes = [32, 64, 128, 256]\n","    search_results = []\n","    \n","    early_stopping = EarlyStopping(\n","        monitor='val_loss', \n","        patience=3, \n","        restore_best_weights=True)\n","\n","    for batch_size in batch_sizes:\n","        # Create and configure your hypermodel and tuner for the current combination\n","        tuner = create_tuner(input_shape, num_block_items, vocab_sizes, embedding_dims, counter, store_id, dept_id)\n","        # Start the search\n","        tuner.search(\n","            train_x, \n","            train_y,\n","            epochs=30,\n","            batch_size=batch_size,\n","            validation_data=(val_x, val_y),\n","            callbacks=[early_stopping],\n","            verbose=2)\n","\n","        best_trial = tuner.oracle.get_best_trials(1)[0]\n","        best_hyperparams = best_trial.hyperparameters.values\n","        \n","        # Create a dictionary to store the results along with store and dept ids\n","        result = {\n","            'store_id': store_id,\n","            'dept_id': dept_id,\n","            'batch_size': batch_size,\n","            'best_val_loss': best_trial.score\n","        }\n","        \n","        # Add each hyperparameter to the result dictionary\n","        for param, value in best_hyperparams.items():\n","            result[param] = value\n","\n","        search_results.append(result)\n","\n","    # Write results to a csv file\n","    append_search_results_to_csv(search_results, csv_path)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def append_search_results_to_csv(search_results, csv_path):\n","    # Convert search results to a DataFrame\n","    search_results_df = pd.DataFrame(search_results)\n","    \n","    # Check if the CSV file already exists\n","    if os.path.exists(csv_path):\n","        # Append without writing the header\n","        search_results_df.to_csv(csv_path, mode='a', header=False, index=False)\n","    else:\n","        # Create a new file with header\n","        search_results_df.to_csv(csv_path, mode='w', header=True, index=False)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 2 Complete [00h 00m 50s]\n","val_loss: nan\n","\n","Best val_loss So Far: 8.771679162979126\n","Total elapsed time: 00h 09m 07s\n","\n","Search: Running Trial #3\n","\n","Value             |Best Value So Far |Hyperparameter\n","0.0011735         |0.00080391        |initial_learning_rate\n","1.1               |1.1               |p_value\n","0.9               |0.8               |quantile\n","tweedie           |tweedie           |loss_function\n","80                |112               |lstm_units_bidirectional\n","sigmoid           |sigmoid           |activation\n","0.5               |0.2               |recurrent_dropout_bidirectional\n","0.007355          |0.00025615        |l2_reg_bidirectional\n","0.2               |0.3               |dropout_1\n","3                 |1                 |num_lstm_layers\n","softmax           |tanh              |activation_self_att\n","0.00023279        |0.0054616         |l2_reg_dense\n","112               |16                |lstm_units_0\n","tanh              |relu              |activation_0\n","0.4               |0.1               |lstm_dropout_0\n","0.0024015         |0.0001            |l2_reg_lstm_0\n","0.5               |0.1               |dropout_0\n","\n","Epoch 1/30\n","59/59 - 16s - loss: 15.6693 - mean_absolute_error: 0.4973 - val_loss: 12.5592 - val_mean_absolute_error: 0.5062 - 16s/epoch - 268ms/step\n","Epoch 2/30\n","59/59 - 12s - loss: 9.0477 - mean_absolute_error: 0.4468 - val_loss: 10.5888 - val_mean_absolute_error: 0.4808 - 12s/epoch - 203ms/step\n","Epoch 3/30\n","59/59 - 11s - loss: 8.0048 - mean_absolute_error: 0.4211 - val_loss: 9.9073 - val_mean_absolute_error: 0.4683 - 11s/epoch - 186ms/step\n","Epoch 4/30\n","59/59 - 12s - loss: 7.6704 - mean_absolute_error: 0.4088 - val_loss: 9.8117 - val_mean_absolute_error: 0.4702 - 12s/epoch - 198ms/step\n","Epoch 5/30\n","59/59 - 12s - loss: 7.6064 - mean_absolute_error: 0.4168 - val_loss: 9.7145 - val_mean_absolute_error: 0.4614 - 12s/epoch - 204ms/step\n","Epoch 6/30\n","59/59 - 11s - loss: 7.4455 - mean_absolute_error: 0.4015 - val_loss: 9.5313 - val_mean_absolute_error: 0.4570 - 11s/epoch - 184ms/step\n","Epoch 7/30\n","59/59 - 11s - loss: 7.3280 - mean_absolute_error: 0.3941 - val_loss: 9.4567 - val_mean_absolute_error: 0.4589 - 11s/epoch - 193ms/step\n","Epoch 8/30\n","59/59 - 11s - loss: 7.1827 - mean_absolute_error: 0.3907 - val_loss: 9.0566 - val_mean_absolute_error: 0.4526 - 11s/epoch - 190ms/step\n","Epoch 9/30\n","59/59 - 11s - loss: 6.6324 - mean_absolute_error: 0.3906 - val_loss: 8.6547 - val_mean_absolute_error: 0.4505 - 11s/epoch - 188ms/step\n","Epoch 10/30\n","59/59 - 11s - loss: 6.3942 - mean_absolute_error: 0.3904 - val_loss: 8.3600 - val_mean_absolute_error: 0.4497 - 11s/epoch - 188ms/step\n","Epoch 11/30\n","59/59 - 11s - loss: 6.2484 - mean_absolute_error: 0.3851 - val_loss: 8.1773 - val_mean_absolute_error: 0.4466 - 11s/epoch - 194ms/step\n","Epoch 12/30\n","59/59 - 11s - loss: 6.1663 - mean_absolute_error: 0.3832 - val_loss: 8.1581 - val_mean_absolute_error: 0.4392 - 11s/epoch - 188ms/step\n","Epoch 13/30\n","59/59 - 11s - loss: 6.1313 - mean_absolute_error: 0.3808 - val_loss: 8.1906 - val_mean_absolute_error: 0.4382 - 11s/epoch - 187ms/step\n","Epoch 14/30\n","59/59 - 11s - loss: 6.1114 - mean_absolute_error: 0.3811 - val_loss: 8.1691 - val_mean_absolute_error: 0.4370 - 11s/epoch - 186ms/step\n","Epoch 15/30\n","59/59 - 11s - loss: 6.0621 - mean_absolute_error: 0.3802 - val_loss: 8.1202 - val_mean_absolute_error: 0.4345 - 11s/epoch - 192ms/step\n","Epoch 16/30\n","59/59 - 11s - loss: 6.0264 - mean_absolute_error: 0.3828 - val_loss: 7.8751 - val_mean_absolute_error: 0.4385 - 11s/epoch - 187ms/step\n","Epoch 17/30\n","59/59 - 10s - loss: 5.8871 - mean_absolute_error: 0.3837 - val_loss: 7.9521 - val_mean_absolute_error: 0.4299 - 10s/epoch - 178ms/step\n","Epoch 18/30\n","59/59 - 11s - loss: 5.8756 - mean_absolute_error: 0.3827 - val_loss: 7.9440 - val_mean_absolute_error: 0.4291 - 11s/epoch - 184ms/step\n","Epoch 19/30\n","59/59 - 11s - loss: 5.8640 - mean_absolute_error: 0.3839 - val_loss: 7.9854 - val_mean_absolute_error: 0.4261 - 11s/epoch - 182ms/step\n","Epoch 1/30\n","59/59 - 15s - loss: 15.7779 - mean_absolute_error: 0.4177 - val_loss: 12.5978 - val_mean_absolute_error: 0.5017 - 15s/epoch - 256ms/step\n","Epoch 2/30\n","59/59 - 10s - loss: 8.1153 - mean_absolute_error: 0.4511 - val_loss: 10.0408 - val_mean_absolute_error: 0.4684 - 10s/epoch - 168ms/step\n","Epoch 3/30\n","59/59 - 12s - loss: 7.5536 - mean_absolute_error: 0.4131 - val_loss: 9.2960 - val_mean_absolute_error: 0.4578 - 12s/epoch - 209ms/step\n","Epoch 4/30\n","59/59 - 11s - loss: 7.1844 - mean_absolute_error: 0.4009 - val_loss: 8.8655 - val_mean_absolute_error: 0.4485 - 11s/epoch - 190ms/step\n","Epoch 5/30\n","59/59 - 11s - loss: 6.8879 - mean_absolute_error: 0.3919 - val_loss: 8.8090 - val_mean_absolute_error: 0.4452 - 11s/epoch - 184ms/step\n","Epoch 6/30\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m forecast_df \u001b[38;5;241m=\u001b[39m lstm_pipeline(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[0;32mIn[21], line 44\u001b[0m, in \u001b[0;36mlstm_pipeline\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m     41\u001b[0m remove_directory(LOGGING_DIR)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Do grid search and log to file\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m start_search(train_x, train_y,\n\u001b[1;32m     45\u001b[0m              val_x, val_y,\n\u001b[1;32m     46\u001b[0m              input_shape, num_block_items, vocab_sizes,\n\u001b[1;32m     47\u001b[0m              embedding_dims, counter, store_id, dept_id,\n\u001b[1;32m     48\u001b[0m              csv_path\u001b[38;5;241m=\u001b[39mCSV_PATH)\n","Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mstart_search\u001b[0;34m(train_x, train_y, val_x, val_y, input_shape, num_block_items, vocab_sizes, embedding_dims, counter, store_id, dept_id, csv_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m tuner \u001b[38;5;241m=\u001b[39m create_tuner(input_shape, num_block_items, vocab_sizes, embedding_dims, counter, store_id, dept_id)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Start the search\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m     15\u001b[0m     train_x, \n\u001b[1;32m     16\u001b[0m     train_y,\n\u001b[1;32m     17\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     18\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     19\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(val_x, val_y),\n\u001b[1;32m     20\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],\n\u001b[1;32m     21\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39mhyperparameters\u001b[38;5;241m.\u001b[39mvalues\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:233\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:273\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m    274\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:238\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 238\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    241\u001b[0m     ):\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    254\u001b[0m         )\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["forecast_df = lstm_pipeline(verbose=1)\n","# train_x, train_y, val_x, val_y = lstm_pipeline(verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def write_to_csv(forecast_df, dir):\n","    # Get validation data\n","    val_df = pd.read_pickle(VALIDATION_DATA)\n","\n","    # Combine forecast with validation data\n","    forecast_df = pd.concat([val_df, forecast_df], axis=0, ignore_index=True)\n","\n","    # Save the forecast to a csv file\n","    forecast_df.to_csv(dir, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model parameters\n","initial_epochs = 10\n","subsequent_epochs = 6\n","batch_size = 100\n","# Learning rate schedule\n","initial_lr = 0.01\n","decay_steps = 1000\n","alpha = 0.001  # Final learning rate\n","lr_schedule = tf.keras.experimental.CosineDecay(\n","    initial_learning_rate=initial_lr,\n","    decay_steps=decay_steps,\n","    alpha=alpha  # Minimum learning rate value as a fraction of initial_learning_rate.\n",")\n","subsequent_lr = 0.005 # Reduce learning rate by factor of 10 for transfer learning\n","\n","clipvalue = 0.5\n","initializer = GlorotNormal(seed=42)\n","\n","# Model compile parameters\n","loss = tweedie_loss_func(p=1.2) #MeanAbsoluteError() #rmse\n","initial_optimizer = Adam(learning_rate=lr_schedule, clipvalue=clipvalue)\n","metrics = MeanAbsoluteError()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a DataFrame for predictions\n","def prepare_fc_to_file(forecast_df, forecast_array, ids):\n","    # Transpose predictions to match the sample submission format\n","    forecast_array = forecast_array.T\n","\n","    # Create array to write to df\n","    forecast_array = np.concatenate((ids.reshape(len(ids),1), forecast_array), axis=1)\n","\n","    # Create a DataFrame for your predictions\n","    forecast_tmp_df = pd.DataFrame(forecast_array, columns=['id'] + [f'F{i+1}' for i in range(28)])\n","\n","    # concatenate forecast to forecast_df\n","    forecast_df = pd.concat([forecast_df, forecast_tmp_df], axis=0, ignore_index=True)\n","\n","    return forecast_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def eval(predictions_original, predictions_normalized, val_y, ids_batch):\n","    # df_eval = pd.DataFrame(columns=['day', 'normalized', 'prediction', 'actual'])\n","    df_eval = pd.DataFrame()\n","\n","    # fill the dataframe with prediction values\n","    # fill df_eval['id'] with 28 times all ids_batch values\n","    df_eval['id']  = ids_batch.tolist()*TEST_DUR\n","    df_eval['day'] = [i for i in range(1, TEST_DUR+1) for _ in range(len(predictions_normalized[1]))]\n","    \n","    df_eval['actual_normalized'] = val_y.flatten()\n","    df_eval['pred_normalized'] = predictions_normalized.flatten()\n","    df_eval['difference_norm'] = df_eval['actual_normalized'] - df_eval['pred_normalized']\n","\n","    df_eval['actual_inv'] = np.expm1(val_y).round(0).astype(int).flatten()\n","    df_eval['pred_inv'] = predictions_original.flatten()\n","    df_eval['difference_inv'] = df_eval['actual_inv'] - df_eval['pred_inv']\n","\n","    return df_eval"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def model_training(model, _train_x, train_event_x, train_y, _val_x, val_event_x, val_y, epochs, use_embeddings_events):\n","    # Train in one go\n","    if STATUS=='training':\n","        if use_embeddings_events:\n","            train_x = [_train_x, train_event_x[:,:,0], train_event_x[:,:,1], train_event_x[:,:,2], train_event_x[:,:,3]]\n","            val_x   = [_val_x, val_event_x[:,:,0], val_event_x[:,:,1], val_event_x[:,:,2], val_event_x[:,:,3]]\n","        else:\n","            train_x = _train_x\n","            val_x = _val_x\n","\n","        history = model.fit(x=train_x,  # Entire training dataset\n","                        y=train_y,  # Corresponding training labels\n","                        validation_data=(val_x, val_y),  # Entire validation dataset and labels\n","                        epochs=epochs,\n","                        batch_size=batch_size)\n","        \n","    elif STATUS=='production':\n","        if use_embeddings_events:\n","            train_x = [_train_x, train_event_x[:,:,0], train_event_x[:,:,1], train_event_x[:,:,2], train_event_x[:,:,3]]\n","        else:\n","            train_x = _train_x\n","\n","        history = model.fit(x=train_x,  # Entire training dataset\n","                        y=train_y,  # Corresponding training labels\n","                        epochs=epochs,\n","                        batch_size=batch_size)\n","        \n","    return model, history"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["################################### Function to forecast the next 28 days (This function for case all data in one batch) ###################################\n","def rolling_forecast(model, df_test, df_val, test_x, test_event_x, test_y , val_x, val_y, val_event_x, num_features, num_block_items): #scaler_target\n","    # Set the df_copy, x_copy and y_copy to the correct dataset\n","    if STATUS=='production':\n","        df_copy = df_test.copy()\n","        x_copy = test_x.copy()\n","        y_copy = test_y.copy()\n","        events_copy = test_event_x.copy()    \n","    \n","    elif STATUS=='training':\n","        df_copy = df_val.copy()\n","        x_copy = val_x.copy()\n","        y_copy = val_y.copy()\n","        events_copy = val_event_x.copy()\n","\n","    # return x_copy, events_copy --> (28,28,1259); (28, 28, 4)\n","\n","    # Predict the next 28 days\n","    for i in range(TEST_DUR):\n","        prediction_normalized = model.predict([x_copy[i].reshape(1, DAYS_PER_SEQUENCE, num_features)] + \n","                                              [events_copy[i][:,j].reshape(1, DAYS_PER_SEQUENCE) for j in range(EVENT_LEN)], verbose=0).flatten() \n","\n","        # Impractical to adjust the prepared array, so we will update the df_test copy and use it to create a new array with the updated prediction values\n","        start_idx = DAYS_PER_SEQUENCE*num_block_items+(i*num_block_items)\n","        end_idx = start_idx + num_block_items - 1\n","\n","        # 1. Rolling forecast: Update the df_test copy with the new prediction\n","        df_copy.loc[start_idx:end_idx, TARGET_COL] = prediction_normalized\n","\n","        # 2. Update sales_amount_lag_1, sales_amount_moving_avg_7, sales_amount_moving_avg_28, zero_sales_available, consecutive_zero_sales\n","        df_copy = feature_engineering(df_copy, num_block_items)\n","        # return df_copy\n","\n","\n","\n","\n","\n","\n","\n","        # Create new df for x and y\n","        x_copy, events_copy, _ = create_x_y(df_copy, num_block_items)\n","\n","        # Update the y array with the new prediction\n","        y_copy[i] = prediction_normalized\n","    \n","    # Inverse transform the predictions\n","    predictions_normalized = y_copy\n","    # predictions_original = scaler_target.inverse_transform(y_copy).round(0).astype(int)\n","    predictions_original_raw = np.expm1(y_copy)\n","\n","    predictions_original_rounded = predictions_original_raw.round(0).astype(int)\n","\n","    # Make sure no negative values are returned\n","    predictions_normalized[predictions_normalized < 0] = 0\n","    predictions_original_raw[predictions_original_raw < 0] = 0\n","    predictions_original_rounded[predictions_original_rounded < 0] = 0\n","        \n","    return predictions_original_raw, predictions_original_rounded, predictions_normalized\n","#########################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# When transfer learning is used, the model should be recompiled with a new, lower learning rate which this function does\n","def prepare_model_tl(model, new_learning_rate, frozen_layers):\n","    # Instantiate a new optimizer with the desired learning rate\n","    new_optimizer = Adam(learning_rate=new_learning_rate)\n","\n","    if frozen_layers:\n","        # Freeze the layers\n","        for layer in frozen_layers:\n","            model.layers[layer].trainable = False\n","\n","    # Recompile the model with the new optimizer\n","    model.compile(optimizer=new_optimizer, loss=loss, metrics=metrics)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Neu: Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","def create_lstm_model(input_shape, num_block_items):\n","   model = Sequential([\n","      LSTM(units=80, activation='relu', return_sequences=True, recurrent_dropout=0.1, input_shape=input_shape),\n","      Dropout(0.3),\n","      LSTM(units=40, activation='relu', return_sequences=False, recurrent_dropout=0.1),\n","      Dropout(0.3),\n","      # LSTM(units=40, activation='tanh', return_sequences=False, recurrent_dropout=0.1),\n","      # Dropout(0.1),\n","      Dense(units=num_block_items, activation='relu'), # activation='relu', 'softmax; Final Dense layer for output\n","      Reshape((num_block_items, 1))]) # Reshape the output to be (number of items)\n","\n","   model.compile(optimizer=initial_optimizer, loss=loss, metrics=metrics)\n","\n","   # For tracking purposes: check the models parameters\n","   # model.summary()\n","\n","   return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use functional API to create a model\n","def create_lstm_model_embedding(numerical_input_shape, num_block_items, vocab_sizes, embedding_dims): \n","    numerical_input = Input(shape=numerical_input_shape, name='numerical_input')\n","    event_input = [Input(shape=(DAYS_PER_SEQUENCE,), name=f'event_input_{i}') for i in range(1, 5)]\n","\n","    cat_embeddings = [Embedding(input_dim=vocab_sizes[i], output_dim=embedding_dims[i], input_length=DAYS_PER_SEQUENCE, embeddings_initializer=initializer)(event_input[i]) for i in range(0, 4)]\n","\n","    # Combine numerical input and embeddings\n","    combined_input = concatenate([numerical_input] + cat_embeddings)\n","\n","    # LSTM layer\n","    lstm_out = Bidirectional(LSTM(units=16, activation='tanh', return_sequences=True, recurrent_dropout=0.2, kernel_regularizer=l2(0.1), kernel_initializer=initializer))(combined_input)\n","    dropout = Dropout(0.2)(lstm_out)\n","    lstm_out = LSTM(units=12, activation='tanh', return_sequences=True, recurrent_dropout=0.1, kernel_regularizer=l2(0.1))(dropout)\n","    dropout = Dropout(0.2)(lstm_out)\n","    attention_out = SeqSelfAttention(attention_activation='sigmoid', kernel_initializer=initializer)(dropout)\n","\n","    # Aggregate sequence information\n","    pooled_output = GlobalAveragePooling1D()(attention_out)\n","\n","    # Output layer\n","    output = Dense(num_block_items, kernel_regularizer=l2(0.1), kernel_initializer=initializer)(pooled_output)\n","\n","    # Create and compile the model\n","    model = Model(inputs=[numerical_input] + event_input, outputs=output)\n","\n","    model.compile(optimizer=initial_optimizer, loss=loss, metrics=metrics)\n","\n","    return model"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
