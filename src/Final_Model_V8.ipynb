{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import requests\n","\n","def detect_environment():\n","    # Check for Kaggle\n","    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n","        print(\"Environment: Kaggle\")\n","        return 'kaggle'\n","    # Check for AWS\n","    try:\n","        response = requests.get('http://169.254.169.254/latest/meta-data/', timeout=2)\n","        if response.status_code == 200:\n","            print(\"Environment: AWS\")\n","            return 'aws'\n","    except requests.exceptions.RequestException:\n","        pass\n","    # Default to local\n","    print(\"Environment: Local\")\n","    return 'local'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.714019Z","iopub.status.busy":"2024-02-04T10:28:45.713557Z","iopub.status.idle":"2024-02-04T10:28:45.724837Z","shell.execute_reply":"2024-02-04T10:28:45.723860Z","shell.execute_reply.started":"2024-02-04T10:28:45.713975Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Environment: Local\n"]}],"source":["# Setting to adjust before each run:\n","CODE_ENV = detect_environment()\n","STATUS = 'training'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["if CODE_ENV == 'kaggle':\n","    !pip install keras_self_attention"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.726757Z","iopub.status.busy":"2024-02-04T10:28:45.726461Z","iopub.status.idle":"2024-02-04T10:28:49.601403Z","shell.execute_reply":"2024-02-04T10:28:49.600379Z","shell.execute_reply.started":"2024-02-04T10:28:45.726733Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import shutil\n","import glob\n","import math\n","from pathlib import Path\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import (Input, LSTM, Dense, Embedding, Dropout, Reshape, \n","                                     concatenate, Flatten, Bidirectional, GlobalAveragePooling1D)\n","from tensorflow.keras.optimizers.legacy import Adam\n","from tensorflow.keras.losses import Huber, MeanAbsoluteError\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.initializers import GlorotNormal\n","from tensorflow.keras.callbacks import Callback, EarlyStopping\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from keras_self_attention import SeqSelfAttention\n","from keras_tuner import HyperModel\n","from keras_tuner.tuners import RandomSearch, BayesianOptimization"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.603096Z","iopub.status.busy":"2024-02-04T10:28:49.602532Z","iopub.status.idle":"2024-02-04T10:28:49.707155Z","shell.execute_reply":"2024-02-04T10:28:49.706050Z","shell.execute_reply.started":"2024-02-04T10:28:49.603066Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n","False\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.710168Z","iopub.status.busy":"2024-02-04T10:28:49.709769Z","iopub.status.idle":"2024-02-04T10:28:49.717151Z","shell.execute_reply":"2024-02-04T10:28:49.716162Z","shell.execute_reply.started":"2024-02-04T10:28:49.710139Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files\n","    LOGGING_DIR = src_dir + 'models/hyperparameter_tuning'\n","    CSV_PATH = LOGGING_DIR + '/hyperparameter_search_results.csv'\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data/'\n","    src_dir = '/kaggle/working/'\n","    sub_dir = src_dir + 'submissions/'\n","    LOGGING_DIR = src_dir + 'hyperparameter_tuning'\n","    CSV_PATH = LOGGING_DIR + '/hyperparameter_search_results.csv'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files\n","    LOGGING_DIR = src_dir + 'hyperparameter_tuning'\n","    CSV_PATH = LOGGING_DIR + '/hyperparameter_search_results.csv'"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.718565Z","iopub.status.busy":"2024-02-04T10:28:49.718293Z","iopub.status.idle":"2024-02-04T10:28:49.727911Z","shell.execute_reply":"2024-02-04T10:28:49.727014Z","shell.execute_reply.started":"2024-02-04T10:28:49.718540Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","VALIDATION_DATA  = prc_dir +'df_1.pkl' # Validation data\n","BASE      = prc_dir +'df_2.pkl' # Base data\n","CALENDAR  = prc_dir +'df_3.pkl' # Calendar data\n","# NUM_ITEMS = 30490 # Number of items per each day\n","\n","DAYS_PER_SEQUENCE = 28  # Length of the sequence\n","MAX_BATCH_SIZE = 900 # Maximum number of ids to be used in each batch to avoid memory issues and curse of dimensionality\n","\n","\n","TARGET_COL = 'sales_amount'\n","# REPEATED_FEATURES = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","REPEATED_FEATURES = ['sales_amount', 'sell_price', 'is_available',\n","                     'sales_amount_moving_avg_7', 'sales_amount_moving_avg_28', 'sales_amount_lag_1',\n","                     'zero_sales_available', 'consecutive_zero_sales'] # List to hold all feature columns that are used for each item\n","SALES_AMOUNT_COLS = ['sales_amount', 'sales_amount_moving_avg_7', 'sales_amount_moving_avg_28', 'sales_amount_lag_1']\n","# ONCE_ONLY_FEATURES = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","ONCE_ONLY_FEATURES = ['snap_CA', 'snap_TX', 'snap_WI', 'mday_normalized', 'day_continuous_normalized',\n","                      'month_sin', 'month_cos', 'wday_sin', 'wday_cos', 'week_sin', 'week_cos', \n","                      'year_normalized'] # List to hold feature columns that are not repeated for each item\n","EVENT_COLS = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n","EVENT_LEN = len(EVENT_COLS)\n","NOT_NEEDED_COLS = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Set test_end to 1969 in case of production\n","if STATUS=='production':\n","    TEST_END = 1969\n","elif STATUS=='training':\n","    TEST_END = 1969 #1941\n","\n","# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.729350Z","iopub.status.busy":"2024-02-04T10:28:49.728998Z","iopub.status.idle":"2024-02-04T10:28:53.163392Z","shell.execute_reply":"2024-02-04T10:28:53.162542Z","shell.execute_reply.started":"2024-02-04T10:28:49.729323Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","def get_whole_data():\n","    df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)\n","    return df_all_data"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>dept_id</th>\n","      <th>cat_id</th>\n","      <th>store_id</th>\n","      <th>state_id</th>\n","      <th>sales_amount</th>\n","      <th>sell_price</th>\n","      <th>is_available</th>\n","      <th>sales_amount_lag_1</th>\n","      <th>sales_amount_moving_avg_7</th>\n","      <th>...</th>\n","      <th>snap_WI</th>\n","      <th>month_sin</th>\n","      <th>month_cos</th>\n","      <th>wday_sin</th>\n","      <th>wday_cos</th>\n","      <th>week_sin</th>\n","      <th>week_cos</th>\n","      <th>mday_normalized</th>\n","      <th>day_continuous_normalized</th>\n","      <th>year_normalized</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HOBBIES_1_001</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0.5</td>\n","      <td>0.866211</td>\n","      <td>0.781738</td>\n","      <td>0.623535</td>\n","      <td>0.456629</td>\n","      <td>0.889657</td>\n","      <td>0.933333</td>\n","      <td>0.000508</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HOBBIES_1_002</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0.5</td>\n","      <td>0.866211</td>\n","      <td>0.781738</td>\n","      <td>0.623535</td>\n","      <td>0.456629</td>\n","      <td>0.889657</td>\n","      <td>0.933333</td>\n","      <td>0.000508</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HOBBIES_1_003</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0.5</td>\n","      <td>0.866211</td>\n","      <td>0.781738</td>\n","      <td>0.623535</td>\n","      <td>0.456629</td>\n","      <td>0.889657</td>\n","      <td>0.933333</td>\n","      <td>0.000508</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HOBBIES_1_004</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0.5</td>\n","      <td>0.866211</td>\n","      <td>0.781738</td>\n","      <td>0.623535</td>\n","      <td>0.456629</td>\n","      <td>0.889657</td>\n","      <td>0.933333</td>\n","      <td>0.000508</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0.5</td>\n","      <td>0.866211</td>\n","      <td>0.781738</td>\n","      <td>0.623535</td>\n","      <td>0.456629</td>\n","      <td>0.889657</td>\n","      <td>0.933333</td>\n","      <td>0.000508</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>"],"text/plain":["         item_id    dept_id   cat_id store_id state_id  sales_amount  \\\n","0  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","1  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","2  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","3  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","4  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1       CA           0.0   \n","\n","   sell_price  is_available  sales_amount_lag_1  sales_amount_moving_avg_7  \\\n","0         0.0             0                 0.0                        0.0   \n","1         0.0             0                 0.0                        0.0   \n","2         0.0             0                 0.0                        0.0   \n","3         0.0             0                 0.0                        0.0   \n","4         0.0             0                 0.0                        0.0   \n","\n","   ...  snap_WI  month_sin  month_cos  wday_sin  wday_cos  week_sin  week_cos  \\\n","0  ...        0        0.5   0.866211  0.781738  0.623535  0.456629  0.889657   \n","1  ...        0        0.5   0.866211  0.781738  0.623535  0.456629  0.889657   \n","2  ...        0        0.5   0.866211  0.781738  0.623535  0.456629  0.889657   \n","3  ...        0        0.5   0.866211  0.781738  0.623535  0.456629  0.889657   \n","4  ...        0        0.5   0.866211  0.781738  0.623535  0.456629  0.889657   \n","\n","  mday_normalized day_continuous_normalized  year_normalized  \n","0        0.933333                  0.000508              0.0  \n","1        0.933333                  0.000508              0.0  \n","2        0.933333                  0.000508              0.0  \n","3        0.933333                  0.000508              0.0  \n","4        0.933333                  0.000508              0.0  \n","\n","[5 rows x 31 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# pd.set_option('display.max_rows', None)\n","# df_all_data = get_whole_data()\n","# df_all_data.head()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# get all ['store_id','dept_id'] combinations from df_all_data and count the occurences\n","# df_combinations = df_all_data[(df_all_data['d']==1)].groupby(['store_id', 'dept_id']).size().reset_index(name='count')  \n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>store_id</th>\n","      <th>dept_id</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CA_1</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CA_1</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CA_1</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CA_1</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CA_1</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>CA_1</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CA_1</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>CA_2</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>CA_2</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>CA_2</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>CA_2</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>CA_2</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>CA_2</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>CA_2</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>CA_3</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>CA_3</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>CA_3</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>CA_3</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>CA_3</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>CA_3</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>CA_3</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>CA_4</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>CA_4</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>CA_4</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>CA_4</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>CA_4</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>CA_4</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>CA_4</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>TX_1</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>TX_1</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>TX_1</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>TX_1</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>TX_1</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>TX_1</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>TX_1</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>TX_2</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>TX_2</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>TX_2</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>TX_2</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>TX_2</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>TX_2</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>TX_2</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>TX_3</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>TX_3</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>TX_3</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>TX_3</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>TX_3</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>TX_3</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>TX_3</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>WI_1</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>WI_1</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>WI_1</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>WI_1</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>WI_1</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>WI_1</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>WI_1</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>WI_2</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>WI_2</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>WI_2</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>WI_2</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>WI_2</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>WI_2</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>WI_2</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>WI_3</td>\n","      <td>FOODS_1</td>\n","      <td>216</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>WI_3</td>\n","      <td>FOODS_2</td>\n","      <td>398</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>WI_3</td>\n","      <td>FOODS_3</td>\n","      <td>823</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>WI_3</td>\n","      <td>HOBBIES_1</td>\n","      <td>416</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>WI_3</td>\n","      <td>HOBBIES_2</td>\n","      <td>149</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>WI_3</td>\n","      <td>HOUSEHOLD_1</td>\n","      <td>532</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>WI_3</td>\n","      <td>HOUSEHOLD_2</td>\n","      <td>515</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   store_id      dept_id  count\n","0      CA_1      FOODS_1    216\n","1      CA_1      FOODS_2    398\n","2      CA_1      FOODS_3    823\n","3      CA_1    HOBBIES_1    416\n","4      CA_1    HOBBIES_2    149\n","5      CA_1  HOUSEHOLD_1    532\n","6      CA_1  HOUSEHOLD_2    515\n","7      CA_2      FOODS_1    216\n","8      CA_2      FOODS_2    398\n","9      CA_2      FOODS_3    823\n","10     CA_2    HOBBIES_1    416\n","11     CA_2    HOBBIES_2    149\n","12     CA_2  HOUSEHOLD_1    532\n","13     CA_2  HOUSEHOLD_2    515\n","14     CA_3      FOODS_1    216\n","15     CA_3      FOODS_2    398\n","16     CA_3      FOODS_3    823\n","17     CA_3    HOBBIES_1    416\n","18     CA_3    HOBBIES_2    149\n","19     CA_3  HOUSEHOLD_1    532\n","20     CA_3  HOUSEHOLD_2    515\n","21     CA_4      FOODS_1    216\n","22     CA_4      FOODS_2    398\n","23     CA_4      FOODS_3    823\n","24     CA_4    HOBBIES_1    416\n","25     CA_4    HOBBIES_2    149\n","26     CA_4  HOUSEHOLD_1    532\n","27     CA_4  HOUSEHOLD_2    515\n","28     TX_1      FOODS_1    216\n","29     TX_1      FOODS_2    398\n","30     TX_1      FOODS_3    823\n","31     TX_1    HOBBIES_1    416\n","32     TX_1    HOBBIES_2    149\n","33     TX_1  HOUSEHOLD_1    532\n","34     TX_1  HOUSEHOLD_2    515\n","35     TX_2      FOODS_1    216\n","36     TX_2      FOODS_2    398\n","37     TX_2      FOODS_3    823\n","38     TX_2    HOBBIES_1    416\n","39     TX_2    HOBBIES_2    149\n","40     TX_2  HOUSEHOLD_1    532\n","41     TX_2  HOUSEHOLD_2    515\n","42     TX_3      FOODS_1    216\n","43     TX_3      FOODS_2    398\n","44     TX_3      FOODS_3    823\n","45     TX_3    HOBBIES_1    416\n","46     TX_3    HOBBIES_2    149\n","47     TX_3  HOUSEHOLD_1    532\n","48     TX_3  HOUSEHOLD_2    515\n","49     WI_1      FOODS_1    216\n","50     WI_1      FOODS_2    398\n","51     WI_1      FOODS_3    823\n","52     WI_1    HOBBIES_1    416\n","53     WI_1    HOBBIES_2    149\n","54     WI_1  HOUSEHOLD_1    532\n","55     WI_1  HOUSEHOLD_2    515\n","56     WI_2      FOODS_1    216\n","57     WI_2      FOODS_2    398\n","58     WI_2      FOODS_3    823\n","59     WI_2    HOBBIES_1    416\n","60     WI_2    HOBBIES_2    149\n","61     WI_2  HOUSEHOLD_1    532\n","62     WI_2  HOUSEHOLD_2    515\n","63     WI_3      FOODS_1    216\n","64     WI_3      FOODS_2    398\n","65     WI_3      FOODS_3    823\n","66     WI_3    HOBBIES_1    416\n","67     WI_3    HOBBIES_2    149\n","68     WI_3  HOUSEHOLD_1    532\n","69     WI_3  HOUSEHOLD_2    515"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# pd.set_option('display.max_rows', None)\n","# df_combinations"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["#pd.set_option('display.max_rows', None)\n","# df_combinations[df_combinations['store_id'] == 'TX_1'].sort_values('count').tail(50)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Return a df with all unique combinations of store_id and dept_id\n","def get_combinations(df_all_data):\n","    # get all store_id and dept_id combinations\n","    df_combinations_store_dep = df_all_data[['store_id','dept_id']].drop_duplicates().reset_index(drop=True)\n","    # get the length of the df_combinations_store_dep\n","    df_length = len(df_combinations_store_dep)\n","\n","    return df_combinations_store_dep, df_length"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Filter df down to only the current store_id and dept_id combination\n","def filter_df(df_combinations_store_dep, df_all_data, i):\n","    store_id = df_combinations_store_dep.loc[i, 'store_id']\n","    dept_id = df_combinations_store_dep.loc[i, 'dept_id']\n","    ids = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)]['id'].drop_duplicates().values\n","    filtered_df = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)].reset_index(drop=True)\n","    filtered_df.reset_index(drop=True, inplace=True) ##################################################????\n","\n","    # Remove all unused columns\n","    filtered_df.drop(NOT_NEEDED_COLS, axis=1, inplace=True)\n","\n","    # Get the number of block items\n","    num_block_items = len(ids)\n","\n","    # Get the number of features\n","    num_features = len(ONCE_ONLY_FEATURES) + len(REPEATED_FEATURES) * num_block_items # Calculate the number of features\n","\n","    # Get the input shape later on for the model\n","    input_shape = (DAYS_PER_SEQUENCE, num_features)\n","\n","    return filtered_df, num_block_items, num_features, input_shape"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def calc_vocab_size(filtered_df, embedding_dims_max=50):\n","    vocab_size=[]\n","    embedding_dims=[]\n","    # count the unique entries of event_name_1 event_type_1 event_name_2 event_type_2\n","    # append the number of unique entries to the list vocab_size\n","    vocab_size.append(len(filtered_df['event_name_1'].unique()))\n","    vocab_size.append(len(filtered_df['event_type_1'].unique()))\n","    vocab_size.append(len(filtered_df['event_name_2'].unique()))\n","    vocab_size.append(len(filtered_df['event_type_2'].unique()))\n","    \n","    # loop over all other indices and calculate the embedding dimensions\n","    for i in range(0, len(vocab_size)):\n","        embedding_dims.append(int(embedding_dims_max * (vocab_size[i]/max(vocab_size))))\n","\n","    return vocab_size, embedding_dims"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Normalize numerical columns\n","def prepare_df(df_all_data):\n","    # Define categorical and numerical columns\n","    categorical_cols = ['id'] #'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'snap_CA', 'snap_TX', 'snap_WI', 'is_available'\n","    \n","    numerical_cols = ['sell_price']\n","\n","    # Convert categorical columns to category dtype and encode with cat.codes\n","    for col in categorical_cols:\n","        df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","    # Adjust the event cols\n","    # 1. Create an encoder instance for each column\n","    encoders = {col: LabelEncoder() for col in EVENT_COLS}\n","\n","    # Apply encoding to each column\n","    for col, encoder in encoders.items():\n","        df_all_data[col] = encoder.fit_transform(df_all_data[col].astype(str)).astype('int8')\n","\n","    # Normalize numerical columns\n","    scaler_numerical = MinMaxScaler()\n","    df_all_data[numerical_cols] = scaler_numerical.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","\n","    # scaler_target = MinMaxScaler() #not used any more\n","    # df_all_data[SALES_AMOUNT_COLS] = scaler_target.fit_transform(df_all_data[SALES_AMOUNT_COLS].astype(np.float64))\n","    df_all_data[SALES_AMOUNT_COLS] = df_all_data[SALES_AMOUNT_COLS].apply(np.log1p)\n","\n","    return df_all_data#, scaler_target"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.326509Z","iopub.status.busy":"2024-02-04T10:29:02.326211Z","iopub.status.idle":"2024-02-04T10:29:02.341825Z","shell.execute_reply":"2024-02-04T10:29:02.340817Z","shell.execute_reply.started":"2024-02-04T10:29:02.326485Z"},"trusted":true},"outputs":[],"source":["def train_test_split(df_all_data):\n","    # For training split up between train and validation dataset, else use all for training and create test dataset\n","    if STATUS=='training':\n","        df_train = df_all_data[df_all_data['d'] <= TRAIN_END].reset_index(drop=True)\n","        df_val   = df_all_data[(df_all_data['d'] > TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] <= VAL_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_test  = None\n","        \n","    elif STATUS=='production':\n","        df_train = df_all_data[df_all_data['d'] <= VAL_END].reset_index(drop=True)\n","        df_test  = df_all_data[(df_all_data['d'] > VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] <= TEST_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_val   = None\n","\n","    # Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","    del df_all_data\n","\n","    return df_train, df_val, df_test"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["### Create x and y in one go without the generator version autogeneration ###\n","def create_x_y(df, num_block_items):\n","    length_days = len(df) // num_block_items\n","    x = []\n","    y = []\n","    events = []\n","\n","    for i in range(0, length_days - DAYS_PER_SEQUENCE):\n","        start_ind = i * num_block_items\n","        end_ind = start_ind + num_block_items * (DAYS_PER_SEQUENCE)  # predict the next day after the sequence\n","\n","        # Extract once-only features for all days in the sequence at once\n","        once_features = df.iloc[start_ind:end_ind:num_block_items][ONCE_ONLY_FEATURES].to_numpy()\n","\n","        # Get event columns\n","        event_features = df.iloc[start_ind:end_ind:num_block_items][EVENT_COLS].to_numpy()\n","\n","        # Extract repeated features for all items and days at once\n","        repeated_features_stack = df.iloc[start_ind:end_ind][REPEATED_FEATURES].to_numpy() # 210,000 items, 10 features\n","\n","        # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","        reshaped_3d = repeated_features_stack.reshape(DAYS_PER_SEQUENCE, num_block_items, len(REPEATED_FEATURES))\n","\n","        # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","        final_array = reshaped_3d.reshape(DAYS_PER_SEQUENCE, -1)\n","\n","        # Combine once-only and repeated features\n","        batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","        # Extract targets\n","        batch_targets = df.iloc[end_ind:end_ind + num_block_items][[TARGET_COL]].to_numpy().flatten()\n","\n","        # Append to x, y and events\n","        x.append(batch_sequences)\n","        events.append(event_features)\n","        y.append(batch_targets)\n","\n","    train_x = np.array(x)\n","    train_event_x = np.array(events)\n","    train_y = np.array(y)\n","\n","    train_x = [train_x, train_event_x[:,:,0], train_event_x[:,:,1], train_event_x[:,:,2], train_event_x[:,:,3]]\n","\n","    return train_x, train_y\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.355635Z","iopub.status.busy":"2024-02-04T10:29:02.355210Z","iopub.status.idle":"2024-02-04T10:29:02.365871Z","shell.execute_reply":"2024-02-04T10:29:02.364973Z","shell.execute_reply.started":"2024-02-04T10:29:02.355590Z"},"trusted":true},"outputs":[],"source":["# Get the training data and labels array for the LSTM model\n","def get_x_and_y(df_train, df_val, df_test, num_block_items):\n","    train_x, train_y = create_x_y(df_train, num_block_items)\n","\n","    if STATUS=='training':\n","        val_x, val_y = create_x_y(df_val, num_block_items)\n","    elif STATUS=='production': \n","        val_x, val_y = None, None\n","\n","    # df_train not needed anymore\n","    del df_train\n","\n","    return train_x, train_y, val_x, val_y"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.386793Z","iopub.status.busy":"2024-02-04T10:29:02.386108Z","iopub.status.idle":"2024-02-04T10:29:02.392710Z","shell.execute_reply":"2024-02-04T10:29:02.391436Z","shell.execute_reply.started":"2024-02-04T10:29:02.386757Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:04.259644Z","iopub.status.busy":"2024-02-04T10:29:04.259265Z","iopub.status.idle":"2024-02-04T10:29:04.264825Z","shell.execute_reply":"2024-02-04T10:29:04.263758Z","shell.execute_reply.started":"2024-02-04T10:29:04.259591Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Perform feature engineering\n","\"\"\"\n","#####- these columns have to be update in the next notebook based on the predictions made by the model #####\n","- 1 days lag #float 64\n","- moving average for 7 and 28 days #float 64\n","- is there a price reduction?\n","- is there a price increase?\n","- adjust for inflation?\n","- consumer sentiment\n","- holiday\n","- weather\n","- \n","\"\"\"\n","def feature_engineering(df, num_block_items): \n","    ################## lag 1 day sales amount ##############################################################################\n","    # After shifting the first days values are NAN but not important as we skip them because we start with the second day\n","    df['sales_amount_lag_1'] = df['sales_amount'].shift(num_block_items)\n","    ########################################################################################################################\n","\n","    ################## moving average 7 and 28 days #########################\n","    df['sales_amount_moving_avg_7'] = df.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=7).mean())\n","    df['sales_amount_moving_avg_7'] = df['sales_amount_moving_avg_7'].fillna(method='bfill')\n","\n","    df['sales_amount_moving_avg_28'] = df.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=28).mean())\n","    df['sales_amount_moving_avg_28'] = df['sales_amount_moving_avg_28'].fillna(method='bfill')\n","    #########################################################################\n","\n","    ################# days consecutive zero sales and if an entry means that this is a zero sale  #########################\n","    # Step 1: Mark zero sales days where item is available\n","    df['zero_sales_available'] = np.where((df['sales_amount'] == 0) & (df['is_available'] == 1), 1, 0).astype(np.int8)\n","\n","    # Function to apply to each group\n","    def calculate_consecutive_zeros(group):\n","        # Step 2: Identify change points to reset the count for consecutive zeros\n","        group['block'] = (group['zero_sales_available'] == 0).cumsum().astype(np.int16)\n","        \n","        # Step 3: Count consecutive zeros within each block\n","        group['consecutive_zero_sales'] = group.groupby('block').cumcount()\n","        \n","        # Reset count where 'zero_sales_available' is 0, as these are not zero sales days or the item is not available\n","        group['consecutive_zero_sales'] = np.where(group['zero_sales_available'] == 1, group['consecutive_zero_sales'], 0).astype(np.int16)\n","        \n","        return group\n","\n","    # Apply the function to each item group\n","    df = df.groupby('id', group_keys=False).apply(calculate_consecutive_zeros)\n","\n","    # Drop the 'block' column because no longer needed\n","    del df['block']\n","\n","    return df\n","########################################################################################################################"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["# for each store_id and dept_id call get whole data, filter for store_id and dept_id\n","def lstm_pipeline(verbose, num_loop_start, num_loop_end):\n","    # Delete directory for a clean new run and logging\n","    remove_directory(LOGGING_DIR)\n","\n","    #Get all data\n","    df_all_data = get_whole_data()\n","\n","    # Get all store_id and dept_id combinations\n","    df_combinations_store_dep, num_combinations = get_combinations(df_all_data)\n","\n","    # define the number of loops\n","    # num_loop = 1 if verbose == 1 else num_combinations\n","\n","    # Loop over all store_id and dept_id combinations, create a model, train it, create the prediction and save it to a file\n","    for counter in range(num_loop_start, num_loop_end):\n","        ############## For debugging purposes ##############\n","        store_id = df_combinations_store_dep.loc[counter, \"store_id\"]\n","        dept_id = df_combinations_store_dep.loc[counter, \"dept_id\"]\n","\n","        print(f'Processing {counter+1} of {num_combinations}: store_id {store_id} and dept_id {dept_id}')\n","        ####################################################\n","\n","        # Filter df down to only the current store_id and dept_id combination\n","        filtered_df, num_block_items, num_features, input_shape = filter_df(df_combinations_store_dep, df_all_data, counter)\n","\n","        ############## For debugging purposes ##############\n","        print(f'Number of ids in this batch: {num_block_items}')\n","        ####################################################\n","\n","        # Calculate the vocab size for the embedding layers later when model is defined\n","        vocab_sizes, embedding_dims = calc_vocab_size(filtered_df) # Funktioniert nur, wenn num_batches 1 ist, sonst muss komplexere Berechnung innerhalb des loops erfolgen\n","\n","        # Prepare the data for training\n","        filtered_df = prepare_df(filtered_df)\n","\n","        # Split the data into train, validation and test set\n","        df_train, df_val, df_test = train_test_split(filtered_df)\n","\n","        # Create training, validation and test data arrays from the dataframes\n","        train_x, train_y, val_x, val_y = get_x_and_y(df_train, df_val, df_test, num_block_items)\n","\n","        # Do grid search and log to file\n","        start_search(train_x, train_y,\n","                     val_x, val_y,\n","                     input_shape, num_block_items, vocab_sizes,\n","                     embedding_dims, counter, store_id, dept_id,\n","                     csv_path=CSV_PATH)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Remove the logging directory such that not old states are used before new grid search runs begin\n","def remove_directory(dir_path):\n","    dir = Path(dir_path)\n","    if dir.exists() and dir.is_dir():\n","        shutil.rmtree(dir)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Cleanup of checkpoint files which are quite large after each run\n","def delete_checkpoint_files(checkpoint_dir):\n","    # Pattern matching the checkpoint files, take too much space to manually delete them\n","    pattern = os.path.join(checkpoint_dir, \"**\", \"checkpoint.data*\")    \n","    # Find and delete the files\n","    for file in glob.glob(pattern, recursive=True):\n","        os.remove(file)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["class TweedieLoss(tf.keras.losses.Loss):\n","    def __init__(self, p, name=\"TweedieLoss\"):\n","        super().__init__(name=name)\n","        self.p = p\n","\n","    def call(self, y_true, y_pred):\n","        # Ensure predictions are strictly positive\n","        epsilon = 1e-8\n","        y_pred = tf.maximum(y_pred, epsilon)\n","\n","        # Tweedie loss calculation\n","        loss = -y_true * tf.pow(y_pred, 1 - self.p) / (1 - self.p) + \\\n","               tf.pow(y_pred, 2 - self.p) / (2 - self.p)\n","        return tf.reduce_mean(loss)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def quantile_loss(q, y_true, y_pred):\n","    e = y_true - y_pred\n","    return tf.reduce_mean(tf.maximum(q * e, (q - 1) * e), axis=-1)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def custom_loss_wrapper(p_value, q_value, loss_choice):\n","    def custom_loss(y_true, y_pred):\n","        if loss_choice == 'tweedie':\n","            return TweedieLoss(p_value)(y_true, y_pred)\n","        elif loss_choice == 'mean_absolute_error':\n","            return MeanAbsoluteError()(y_true, y_pred)\n","        elif loss_choice == 'huber':\n","            return Huber()(y_true, y_pred)\n","        elif loss_choice == 'quantile_loss':\n","            return quantile_loss(q_value, y_true, y_pred)\n","    return custom_loss"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Prepare model and model params for hyperparameter tuning with grid search\n","class LSTMTuningModel(HyperModel):\n","    def __init__(self, input_shape, num_block_items, vocab_sizes, embedding_dims, metrics, batch_size):\n","        self.input_shape = input_shape\n","        self.num_block_items = num_block_items\n","        self.vocab_sizes = vocab_sizes\n","        self.embedding_dims = embedding_dims\n","        self.metrics = metrics\n","        self.batch_size = batch_size\n","    \n","    def build(self, hp):\n","        ################################ Hyperparameters ################################\n","        # Embeddings\n","        # embedding_dim_options = [16, 32, 64]\n","\n","        # Give global min max values for layers\n","        num_lstm_layers_min = 0\n","        num_lstm_layers_max = 3\n","\n","        lstm_units_min      = 32\n","        lstm_units_max      = 256\n","        lstm_step_size      = 32\n","\n","        activation_choice   = ['relu', 'tanh', 'sigmoid']\n","        activation_choice_bidirectional = ['tanh', 'sigmoid']\n","        self_att_activation = ['sigmoid', 'softmax', 'tanh', 'linear']\n","\n","        recurr_dropout_min  = 0.1\n","        recurr_dropout_max  = 0.5\n","        recurr_dropout_step = 0.1\n","\n","        l2_reg_min          = 1e-4\n","        l2_reg_max          = 1e-2\n","\n","        dropout_rate_min    = 0.1\n","        dropout_rate_max    = 0.5\n","        dropout_rate_step   = 0.1\n","\n","        # Learning rate\n","        initial_learning_rate = hp.Float('initial_learning_rate', 1e-4, 1e-2, sampling='log') * (math.sqrt(self.batch_size / 32) * 2 - 1)\n","\n","        # Loss\n","        p_value               = hp.Float('p_value', 1.1, 1.2, step=0.1)\n","        q_value               = hp.Float('quantile', 0.8, 0.95, step=0.05)\n","        loss_choice           = hp.Choice('loss_function', values=['tweedie']) #'quantile_loss', 'mean_absolute_error', 'huber', \n","        ####################################################################################\n","\n","        ############################# Model architecture #####################################\n","        # Inputs\n","        numerical_input = Input(shape=self.input_shape, name='numerical_input')\n","        event_input = [Input(shape=(DAYS_PER_SEQUENCE,), name=f'event_input_{i}') for i in range(1, 5)]\n","        \n","        initializer = GlorotNormal(seed=42)\n","\n","        # Embeddings\n","        cat_embeddings = [Embedding(input_dim=self.vocab_sizes[i], \n","                                    output_dim=self.embedding_dims[i], \n","                                    input_length=DAYS_PER_SEQUENCE, \n","                                    embeddings_initializer=initializer)(event_input[i]) for i in range(4)]\n","\n","        combined_input = concatenate([numerical_input] + cat_embeddings)\n","\n","        # Define the layers, at least 1 Bidirectional layer\n","        lstm_out = Bidirectional(\n","                    LSTM(units=hp.Int('lstm_units_bidirectional', lstm_units_min, lstm_units_max, lstm_step_size),\n","                        activation=hp.Choice('activation_bidirectional', values=activation_choice_bidirectional),\n","                        return_sequences=True,\n","                        recurrent_dropout=hp.Float('recurrent_dropout_bidirectional', recurr_dropout_min, recurr_dropout_max, recurr_dropout_step), \n","                        kernel_regularizer=l2(hp.Float('l2_reg_bidirectional', l2_reg_min, l2_reg_max, sampling='log')), \n","                        kernel_initializer=initializer))(combined_input)\n","        lstm_out = Dropout(hp.Float('dropout_1', dropout_rate_min, dropout_rate_max, dropout_rate_step))(lstm_out)\n","\n","        # Varying numbers of following LSTM layers possible\n","        for i in range(0, hp.Int('num_lstm_layers', num_lstm_layers_min, num_lstm_layers_max)):\n","            lstm_out = LSTM(\n","                units=hp.Int(f'lstm_units_layer_{i}', lstm_units_min, lstm_units_max, lstm_step_size),\n","                activation=hp.Choice(f'activation_lstm_layer_{i}', values=activation_choice),  \n","                return_sequences=True,\n","                recurrent_dropout=hp.Float(f'lstm_dropout_layer_{i}', recurr_dropout_min, recurr_dropout_max, recurr_dropout_step),\n","                kernel_regularizer=l2(hp.Float(f'l2_reg_lstm_layer_{i}', l2_reg_min, l2_reg_max, sampling='log')),\n","                kernel_initializer=initializer)(lstm_out)\n","            lstm_out = Dropout(hp.Float(f'dropout_{i}', dropout_rate_min, dropout_rate_max, dropout_rate_step))(lstm_out)\n","\n","        # Attention layer\n","        attention_out = SeqSelfAttention(\n","                            attention_activation=hp.Choice('activation_self_att', \n","                            values=self_att_activation), \n","                            kernel_initializer=initializer)(lstm_out)\n","        pooled_output = GlobalAveragePooling1D()(attention_out)\n","        output = Dense(\n","                    self.num_block_items, \n","                    kernel_regularizer=l2(hp.Float('l2_reg_dense', l2_reg_min, l2_reg_max, sampling='log')),\n","                    kernel_initializer=initializer)(pooled_output)\n","\n","        model = Model(inputs=[numerical_input] + event_input, outputs=output)\n","        \n","        ####################################################################################\n","\n","        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=initial_learning_rate,\n","            decay_steps=int(1000*math.sqrt(self.batch_size / 32)),\n","            decay_rate=0.9)\n","\n","        model.compile(\n","            optimizer=Adam(learning_rate=lr_schedule, clipvalue=0.5),\n","            loss=custom_loss_wrapper(p_value, q_value, loss_choice),\n","            metrics=[self.metrics])\n","\n","        return model"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def create_tuner(input_shape, num_block_items, vocab_sizes, embedding_dims, counter, store_id, dept_id, batch_size, directory):\n","    hypermodel = LSTMTuningModel(\n","        input_shape=input_shape, \n","        num_block_items=num_block_items, \n","        vocab_sizes=vocab_sizes, \n","        embedding_dims=embedding_dims, \n","        metrics=MeanAbsoluteError(),\n","        batch_size=batch_size)\n","\n","    # Setting up the tuner, for example, using RandomSearch\n","    tuner = BayesianOptimization(\n","        hypermodel,\n","        objective='val_loss',\n","        max_trials=100, #The maximum number of hyperparameter combinations to test\n","        executions_per_trial=1,\n","        directory=directory,\n","        project_name='tuning_combination_' + str(counter) + '_storeId_' + str(store_id) + '_deptId_' + str(dept_id) + '_batch_size_' + str(batch_size))\n","\n","    return tuner"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def start_search(train_x, train_y, val_x, val_y, input_shape, num_block_items, vocab_sizes, embedding_dims, counter, store_id, dept_id, csv_path):\n","    batch_sizes = [32, 64, 128, 256]\n","    search_results = []\n","    \n","    early_stopping = EarlyStopping(\n","        monitor='val_loss', \n","        patience=3, \n","        min_delta=0.4,      # Minimum change to qualify as an improvement\n","        restore_best_weights=True)\n","\n","    for batch_size in batch_sizes:\n","        # Create and configure your hypermodel and tuner for the current combination\n","        tuner = create_tuner(input_shape, \n","                             num_block_items,\n","                             vocab_sizes,\n","                             embedding_dims,\n","                             counter,\n","                             store_id,\n","                             dept_id,\n","                             batch_size=batch_size,\n","                             directory=LOGGING_DIR)\n","        # Start the search\n","        tuner.search(\n","            train_x, \n","            train_y,\n","            epochs=25,\n","            batch_size=batch_size,\n","            validation_data=(val_x, val_y),\n","            callbacks=[early_stopping],\n","            verbose=1)\n","\n","        # Get the best 3 trials\n","        best_trials = tuner.oracle.get_best_trials(num_trials=3)\n","\n","        for rank, trial in enumerate(best_trials, start=1):\n","            best_hyperparams = trial.hyperparameters.values\n","            # Create a dictionary to store the results for each trial, including its rank\n","            result = {\n","                'rank': rank,\n","                'store_id': store_id,\n","                'dept_id': dept_id,\n","                'batch_size': batch_size,\n","                'val_loss': trial.score\n","            }\n","\n","            # Add each hyperparameter to the result dictionary\n","            for param, value in best_hyperparams.items():\n","                result[param] = value\n","\n","            search_results.append(result)\n","\n","        delete_checkpoint_files(LOGGING_DIR)\n","\n","    # Write results to a csv file\n","    append_search_results_to_csv(search_results, csv_path)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def append_search_results_to_csv(search_results, csv_path):\n","    # Convert search results to a DataFrame\n","    search_results_df = pd.DataFrame(search_results)\n","    \n","    # Check if the CSV file already exists\n","    if os.path.exists(csv_path):\n","        # Append without writing the header\n","        search_results_df.to_csv(csv_path, mode='a', header=False, index=False)\n","    else:\n","        # Create a new file with header\n","        search_results_df.to_csv(csv_path, mode='w', header=True, index=False)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["forecast_df = lstm_pipeline(verbose=0, num_loop_start=0, num_loop_end=70)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# os.remove(src_dir + 'state.db')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
