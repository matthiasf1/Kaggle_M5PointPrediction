{"cells":[{"cell_type":"markdown","metadata":{},"source":["- Ggf. zu float16 konvertieren checken, ob finaler df mit time slices dann deutlich kleiner und performance testen\n","- column 'd' in training df löschen?\n","- paralletl computing einstellen\n","- use_multiprocessing in keras auf true setzen (model.fit agument)\n","- Cross validation?\n","- Ensemble learning?"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:51.146960Z","iopub.status.busy":"2024-01-10T10:27:51.146517Z","iopub.status.idle":"2024-01-10T10:27:51.559407Z","shell.execute_reply":"2024-01-10T10:27:51.558325Z","shell.execute_reply.started":"2024-01-10T10:27:51.146917Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:51.561607Z","iopub.status.busy":"2024-01-10T10:27:51.560980Z","iopub.status.idle":"2024-01-10T10:27:51.568273Z","shell.execute_reply":"2024-01-10T10:27:51.567094Z","shell.execute_reply.started":"2024-01-10T10:27:51.561565Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","#code_env = 'kaggle'\n","code_env = 'local'\n","\n","\n","if code_env=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v2/'"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:51.570270Z","iopub.status.busy":"2024-01-10T10:27:51.569792Z","iopub.status.idle":"2024-01-10T10:27:51.585843Z","shell.execute_reply":"2024-01-10T10:27:51.584501Z","shell.execute_reply.started":"2024-01-10T10:27:51.570239Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","BASE     = prc_dir +'df_1.pkl'\n","CALENDAR = prc_dir +'df_2.pkl'\n","NUM_ITEMS = 30490 # Number of items per each day"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:51.590934Z","iopub.status.busy":"2024-01-10T10:27:51.590507Z","iopub.status.idle":"2024-01-10T10:27:55.678882Z","shell.execute_reply":"2024-01-10T10:27:55.677426Z","shell.execute_reply.started":"2024-01-10T10:27:51.590898Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense\n","from keras.optimizers import Adam\n","from keras import backend as K"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:55.681316Z","iopub.status.busy":"2024-01-10T10:27:55.680367Z","iopub.status.idle":"2024-01-10T10:27:57.845377Z","shell.execute_reply":"2024-01-10T10:27:57.843885Z","shell.execute_reply.started":"2024-01-10T10:27:55.681266Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:57.847150Z","iopub.status.busy":"2024-01-10T10:27:57.846774Z","iopub.status.idle":"2024-01-10T10:28:07.386122Z","shell.execute_reply":"2024-01-10T10:28:07.384994Z","shell.execute_reply.started":"2024-01-10T10:27:57.847118Z"},"trusted":true},"outputs":[],"source":["# Define categorical and numerical columns\n","categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n","numerical_cols = ['sell_price']\n","\n","target_col = 'sales_amount'\n","\n","# Convert categorical columns to category dtype and encode with cat.codes\n","for col in categorical_cols:\n","    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","# Normalize numerical columns\n","scaler = MinMaxScaler()\n","df_all_data[numerical_cols] = scaler.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","df_all_data[target_col] = scaler.fit_transform(df_all_data[[target_col]].astype(np.float32))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:28:07.388832Z","iopub.status.busy":"2024-01-10T10:28:07.387576Z","iopub.status.idle":"2024-01-10T10:28:16.141470Z","shell.execute_reply":"2024-01-10T10:28:16.140402Z","shell.execute_reply.started":"2024-01-10T10:28:07.388757Z"},"trusted":true},"outputs":[],"source":["# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","TRAIN_END = 1913 - 28 # 1885 -> Train only until the 28 days before the end of the data\n","VAL_END = 1913\n","\n","df_train = df_all_data[df_all_data['d'] < TRAIN_END]\n","df_val = df_all_data[(df_all_data['d'] >= TRAIN_END) & (df_all_data['d'] < VAL_END)]\n","df_test = df_all_data[df_all_data['d'] >= VAL_END]\n","\n","del df_all_data"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:28:16.144144Z","iopub.status.busy":"2024-01-10T10:28:16.143279Z","iopub.status.idle":"2024-01-10T10:28:16.153399Z","shell.execute_reply":"2024-01-10T10:28:16.152138Z","shell.execute_reply.started":"2024-01-10T10:28:16.144099Z"},"trusted":true},"outputs":[],"source":["# Custom Generator Function\n","def lstm_data_generator(df, target, days_per_sequence=1, batch_size=32):\n","    total_sequences = (len(df) - NUM_ITEMS * days_per_sequence) // NUM_ITEMS # 1878 for train, 21 for val and test\n","    while True:  # Loop indefinitely, the model's fit method will break the loop\n","        for i in range(0, total_sequences, batch_size): # 0, 32, 64, ...1878\n","            batch_sequences = []\n","            batch_targets = []\n","            for b in range(batch_size): # 0, 1, 2,... 32\n","                if i + b < total_sequences: # 0, 0; 0, 1; 0, 2; ...; 0, 32; 32, 0; 32, 1; ...\n","                    start_idx = (i + b) * NUM_ITEMS\n","                    end_idx = start_idx + NUM_ITEMS * days_per_sequence\n","                    batch_sequences.append(df.iloc[start_idx:end_idx, :].drop(target, axis=1).to_numpy()) #drop target column, Only the values in the DataFrame will be returned, the axes labels will be removed.\n","                    batch_targets.append(df.iloc[end_idx:end_idx + NUM_ITEMS][target].to_numpy())\n","            yield np.array(batch_sequences), np.array(batch_targets)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:28:16.155432Z","iopub.status.busy":"2024-01-10T10:28:16.155114Z","iopub.status.idle":"2024-01-10T10:28:16.163243Z","shell.execute_reply":"2024-01-10T10:28:16.162255Z","shell.execute_reply.started":"2024-01-10T10:28:16.155405Z"},"trusted":true},"outputs":[],"source":["# Usage\n","time_steps = 7  # Number of days per sequence\n","batch_size = 32  # Size of each batch\n","epochs= 2\n","train_gen = lstm_data_generator(df_train, target_col, time_steps, batch_size)\n","val_gen = lstm_data_generator(df_val, target_col, time_steps, batch_size)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:29:59.577604Z","iopub.status.busy":"2024-01-10T10:29:59.577105Z","iopub.status.idle":"2024-01-10T10:29:59.583179Z","shell.execute_reply":"2024-01-10T10:29:59.581998Z","shell.execute_reply.started":"2024-01-10T10:29:59.577565Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:30:07.348537Z","iopub.status.busy":"2024-01-10T10:30:07.348114Z","iopub.status.idle":"2024-01-10T10:30:07.664360Z","shell.execute_reply":"2024-01-10T10:30:07.663177Z","shell.execute_reply.started":"2024-01-10T10:30:07.348503Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-10 17:56:28.921923: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-01-10 17:56:28.922553: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-01-10 17:56:28.923210: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"]}],"source":["# Define the LSTM Model with the functional API to account for a more complex architecture\n","# input_shape = (time_steps, len(df_train.columns) - 1)\n","# model_input = Input(shape=input_shape)\n","# x = LSTM(50, activation='tanh')(model_input)\n","# x = Dense(1)(x)\n","# model = Model(inputs=model_input, outputs=x)\n","\n","# model.compile(optimizer=Adam(), loss=rmse) # first placed solution used RMSE as loss function\n","\n","\n","\n","# Assuming you have your LSTM model defined\n","input_shape = (time_steps, len(df_train.columns) - 1)\n","model = Sequential([\n","    LSTM(50, activation='tanh', input_shape=(input_shape)),\n","    Dense(1)\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss=rmse)\n","# model possibly ergänzen um metrics=['accuracy']"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:28:16.874593Z","iopub.status.busy":"2024-01-10T10:28:16.874270Z","iopub.status.idle":"2024-01-10T10:28:16.898990Z","shell.execute_reply":"2024-01-10T10:28:16.897993Z","shell.execute_reply.started":"2024-01-10T10:28:16.874566Z"},"trusted":true},"outputs":[],"source":["# df head with all columns displayed\n","#pd.set_option('display.max_columns', None)\n","#df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:30:12.049115Z","iopub.status.busy":"2024-01-10T10:30:12.048653Z","iopub.status.idle":"2024-01-10T13:20:45.471124Z","shell.execute_reply":"2024-01-10T13:20:45.467342Z","shell.execute_reply.started":"2024-01-10T10:30:12.049079Z"},"trusted":true},"outputs":[],"source":["# Train the model using the generator\n","# model.fit(x=train_gen, steps_per_epoch=(len(df_train) // (batch_size * NUM_ITEMS)), epochs=3) # x: In case of a generator the target y will be obtained from x; steps_per_epoch: 57mio // (32*30490) = 58\n","\n","# Fit the model and store history for later evaluation\n","history = model.fit(\n","    x=train_gen,\n","    epochs=epochs,\n","    steps_per_epoch=(len(df_train) // (batch_size * NUM_ITEMS)),\n","    validation_data=val_gen,\n","    validation_steps=(len(df_train) // (batch_size * NUM_ITEMS))\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T13:20:47.566799Z","iopub.status.busy":"2024-01-10T13:20:47.566068Z","iopub.status.idle":"2024-01-10T13:20:47.590351Z","shell.execute_reply":"2024-01-10T13:20:47.589421Z","shell.execute_reply.started":"2024-01-10T13:20:47.566755Z"},"trusted":true},"outputs":[],"source":["# Save the model to a specified directory\n","if code_env=='local':\n","    ###local###\n","    model.save('/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction/models/V1_1layer_LSTM.h5')\n","    \n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    model.save('/kaggle/working/models/V1_1layer_LSTM.h5')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-10 17:50:00.990982: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-01-10 17:50:00.991492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-01-10 17:50:00.991955: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"]}],"source":["# Start from here if you want to load the model\n","from keras.models import load_model\n","\n","# Load the model from a specified directory\n","if code_env=='local':\n","    ###local###\n","    model = load_model('/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction/src/models/V1_1layer_LSTM.h5', custom_objects={'rmse': rmse})\n","\n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    model = load_model('/kaggle/working/models/V1_1layer_LSTM.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T13:25:08.931943Z","iopub.status.busy":"2024-01-10T13:25:08.930706Z","iopub.status.idle":"2024-01-10T13:25:09.215835Z","shell.execute_reply":"2024-01-10T13:25:09.214588Z","shell.execute_reply.started":"2024-01-10T13:25:08.931897Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No history to plot\n"]}],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:28:19.990527Z","iopub.status.idle":"2024-01-10T10:28:19.990989Z","shell.execute_reply":"2024-01-10T10:28:19.990764Z","shell.execute_reply.started":"2024-01-10T10:28:19.990744Z"},"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_model_values(model, df_test, target_col, time_steps, batch_size, h, n):\n","    test_gen = lstm_data_generator(df_test, target_col, time_steps, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    return y_pred_original"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate_model_values(model, df_test, df_train, df_val, batch_size, time_steps, VAL_END)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:28:19.992214Z","iopub.status.idle":"2024-01-10T10:28:19.992611Z","shell.execute_reply":"2024-01-10T10:28:19.992440Z","shell.execute_reply.started":"2024-01-10T10:28:19.992421Z"},"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, time_steps, n):\n","    test_gen = lstm_data_generator(df_test, target_col, time_steps, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:28:19.994039Z","iopub.status.idle":"2024-01-10T10:28:19.994425Z","shell.execute_reply":"2024-01-10T10:28:19.994260Z","shell.execute_reply.started":"2024-01-10T10:28:19.994241Z"},"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, time_steps, VAL_END)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4280642,"sourceId":7368162,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
