{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ggf. zu float16 konvertieren checken, ob finaler df mit time slices dann deutlich kleiner und performance testen\n",
    "- column 'd' in training df löschen?\n",
    "- paralletl computing einstellen\n",
    "- use_multiprocessing in keras auf true setzen (model.fit agument)\n",
    "- Cross validation?\n",
    "- Ensemble learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify directories\n",
    "#code_env = 'kaggle'\n",
    "code_env = 'local'\n",
    "\n",
    "\n",
    "if code_env=='local':\n",
    "    ###local###\n",
    "    #get parent folder of current directory\n",
    "    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n",
    "\n",
    "    #Directory resources\n",
    "    res_dir = parent_dir + '/res/'\n",
    "    src_dir = parent_dir + '/src/'\n",
    "    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n",
    "\n",
    "if code_env=='kaggle':\n",
    "    ###On Kaggle###\n",
    "    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n",
    "    prc_dir = '/kaggle/input/processed-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables\n",
    "BASE     = prc_dir +'df_1.pkl'\n",
    "CALENDAR = prc_dir +'df_2.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in df_train_conv from pickle file\n",
    "df_all_data = pd.concat([pd.read_pickle(BASE),\n",
    "           pd.read_pickle(CALENDAR)], \n",
    "           axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n",
    "                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n",
    "                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n",
    "numerical_cols = ['sell_price']\n",
    "\n",
    "target_col = 'sales_amount'\n",
    "\n",
    "# Convert categorical columns to category dtype and encode with cat.codes\n",
    "for col in categorical_cols:\n",
    "    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n",
    "\n",
    "# Normalize numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "df_all_data[numerical_cols] = scaler.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n",
    "df_all_data[target_col] = scaler.fit_transform(df_all_data[[target_col]].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data in train, validation and test set\n",
    "TRAIN_END = 1913 - 28 # 1885 -> Train only until the 28 days before the end of the data\n",
    "VAL_END = 1913\n",
    "\n",
    "df_train = df_all_data[df_all_data['d'] <= TRAIN_END]\n",
    "df_val = df_all_data[(df_all_data['d'] > TRAIN_END) & (df_all_data['d'] <= VAL_END)]\n",
    "df_test = df_all_data[df_all_data['d'] > VAL_END]\n",
    "\n",
    "del df_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Generator Function\n",
    "def lstm_data_generator(df, target, days_per_sequence=1, batch_size=32):\n",
    "    num_items = df[df['d'] == 1].shape[0]  # 30490 -> Number of items per day\n",
    "    total_sequences = (len(df) - num_items * days_per_sequence) // num_items # 1878\n",
    "    while True:  # Loop indefinitely, the model's fit method will break the loop\n",
    "        for i in range(0, total_sequences, batch_size): # 0, 32, 64, ...1878\n",
    "            batch_sequences = []\n",
    "            batch_targets = []\n",
    "            for b in range(batch_size): # 0, 1, 2,... 32\n",
    "                if i + b < total_sequences: # 0, 0; 0, 1; 0, 2; ...; 0, 32; 32, 0; 32, 1; ...\n",
    "                    start_idx = (i + b) * num_items\n",
    "                    end_idx = start_idx + num_items * days_per_sequence\n",
    "                    batch_sequences.append(df.iloc[start_idx:end_idx, :].drop(target, axis=1).to_numpy()) #drop target column, Only the values in the DataFrame will be returned, the axes labels will be removed.\n",
    "                    batch_targets.append(df.iloc[end_idx:end_idx + num_items][target].to_numpy())\n",
    "            yield np.array(batch_sequences), np.array(batch_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "time_steps = 7  # Number of days per sequence\n",
    "batch_size = 64  # Size of each batch\n",
    "epochs= 2\n",
    "train_gen = lstm_data_generator(df_train, target_col, time_steps, batch_size)\n",
    "val_gen = lstm_data_generator(df_val, target_col, time_steps, batch_size)\n",
    "num_items = df_train[df_train['d'] == 1].shape[0]  # Number of items per day (day 1 examplary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RMSE loss function\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 17:38:42.375561: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-01-09 17:38:42.376038: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-01-09 17:38:42.376478: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM Model with the functional API to account for a more complex architecture\n",
    "# input_shape = (time_steps, len(df_train.columns) - 1)\n",
    "# model_input = Input(shape=input_shape)\n",
    "# x = LSTM(50, activation='tanh')(model_input)\n",
    "# x = Dense(1)(x)\n",
    "# model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "# model.compile(optimizer=Adam(), loss=rmse) # first placed solution used RMSE as loss function\n",
    "# model possibly ergänzen um metrics=['accuracy']\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have your LSTM model defined\n",
    "input_shape = (time_steps, len(df_train.columns) - 1)\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='tanh', input_shape=(input_shape)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>sales_amount</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>is_available</th>\n",
       "      <th>d</th>\n",
       "      <th>wday</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>mday</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1437</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1438</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1439</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1441</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  dept_id  cat_id  store_id  state_id  sales_amount  sell_price  \\\n",
       "0     1437        3       1         0         0           0.0         0.0   \n",
       "1     1438        3       1         0         0           0.0         0.0   \n",
       "2     1439        3       1         0         0           0.0         0.0   \n",
       "3     1440        3       1         0         0           0.0         0.0   \n",
       "4     1441        3       1         0         0           0.0         0.0   \n",
       "\n",
       "   is_available  d  wday  event_name_1  event_type_1  event_name_2  \\\n",
       "0             0  0     0            -1            -1            -1   \n",
       "1             0  0     0            -1            -1            -1   \n",
       "2             0  0     0            -1            -1            -1   \n",
       "3             0  0     0            -1            -1            -1   \n",
       "4             0  0     0            -1            -1            -1   \n",
       "\n",
       "   event_type_2  snap_CA  snap_TX  snap_WI  mday  week  month  year  \n",
       "0            -1        0        0        0    28     3      0     0  \n",
       "1            -1        0        0        0    28     3      0     0  \n",
       "2            -1        0        0        0    28     3      0     0  \n",
       "3            -1        0        0        0    28     3      0     0  \n",
       "4            -1        0        0        0    28     3      0     0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df head with all columns displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 17:38:48.617501: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-01-09 17:38:48.714140: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-01-09 17:38:48.714855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-01-09 17:38:48.715437: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-01-09 17:38:48.973759: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-01-09 17:38:48.974714: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-01-09 17:38:48.975279: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 2973s 51s/step - loss: 0.3537\n",
      "Epoch 2/3\n",
      "58/58 [==============================] - 3018s 52s/step - loss: 0.2215\n",
      "Epoch 3/3\n",
      "58/58 [==============================] - 2928s 50s/step - loss: 0.0924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x164fd65d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the generator\n",
    "# model.fit(x=train_gen, steps_per_epoch=(len(df_train) // (batch_size * num_items)), epochs=3) # x: In case of a generator the target y will be obtained from x; steps_per_epoch: 57mio // (32*30490) = 58\n",
    "\n",
    "# Fit the model more sophisticated approach\n",
    "history = model.fit(\n",
    "    x=train_gen,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=(len(df_train) // (batch_size * num_items)),\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=(len(df_train) // (batch_size * num_items))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n",
    "def calculate_weights(sales_data, last_n_days=28):\n",
    "    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n",
    "    # Sum sales for each item over the last_n_days\n",
    "    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n",
    "    # Total sales for all items\n",
    "    total_sales = item_sales.sum()\n",
    "    # Calculate weights\n",
    "    weights = item_sales / total_sales\n",
    "    return weights\n",
    "\n",
    "def rmsse(y_true, y_pred, h, y_train):\n",
    "    numerator = np.sum((y_true - y_pred) ** 2) / h\n",
    "    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1)\n",
    "    return np.sqrt(numerator / denominator)\n",
    "\n",
    "def wrmsse(y_trues, y_preds, weights, h, y_trains):\n",
    "    rmsse_values = [rmsse(y_true, y_pred, h, y_train) \n",
    "                    for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n",
    "    return np.sum(np.array(weights) * np.array(rmsse_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, df_test, batch_size, time_steps, n):\n",
    "    test_gen = lstm_data_generator(df_test, target_col, time_steps, batch_size)\n",
    "    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n",
    "    predictions_normalized = model.predict(test_gen, steps=steps)\n",
    "    predictions_original = scaler.inverse_transform(predictions_normalized)  # Inverse-transformed predictions -> Muss jetzt aber auch den Vergleichswert auf die richtige Form bringen!!\n",
    "    # Reshape predictions and actual values for comparison\n",
    "    y_true_normalized = df_test[target_col].values\n",
    "    y_true_original = scaler.inverse_transform(y_true_normalized)\n",
    "    y_pred = predictions_original.flatten()[:len(y_true_original)]\n",
    "    \n",
    "    # Calculate wrmsse score\n",
    "    weights = calculate_weights(sales_data)\n",
    "    wrmsse_score = wrmsse(\n",
    "        y_true=df_test[target_col].values, \n",
    "        y_pred=predictions_original,\n",
    "        weights=df_test['sales_amount'].values,\n",
    "        h=28, # forecast horizon\n",
    "        n=n, # number of historical data points\n",
    "        y_train=df_train[target_col].values\n",
    "    )\n",
    "    print(\"Test WRMSSE: \", wrmsse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate function\n",
    "evaluate_model(model, df_test, batch_size, time_steps, VAL_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_bsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
