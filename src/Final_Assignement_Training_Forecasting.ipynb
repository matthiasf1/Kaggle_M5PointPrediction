{"cells":[{"cell_type":"markdown","metadata":{},"source":["- Ggf. zu float16 konvertieren checken, ob finaler df mit time slices dann deutlich kleiner und performance testen\n","- column 'd' in training df löschen?\n","- paralletl computing einstellen\n","- use_multiprocessing in keras auf true setzen (model.fit agument)\n","- Cross validation?\n","- Ensemble learning?\n","- brauche ich one-hot encoding für categorical features?"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:51.146960Z","iopub.status.busy":"2024-01-10T10:27:51.146517Z","iopub.status.idle":"2024-01-10T10:27:51.559407Z","shell.execute_reply":"2024-01-10T10:27:51.558325Z","shell.execute_reply.started":"2024-01-10T10:27:51.146917Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:51.561607Z","iopub.status.busy":"2024-01-10T10:27:51.560980Z","iopub.status.idle":"2024-01-10T10:27:51.568273Z","shell.execute_reply":"2024-01-10T10:27:51.567094Z","shell.execute_reply.started":"2024-01-10T10:27:51.561565Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","#code_env = 'kaggle'\n","code_env = 'local'\n","\n","\n","if code_env=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v2/'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:51.570270Z","iopub.status.busy":"2024-01-10T10:27:51.569792Z","iopub.status.idle":"2024-01-10T10:27:51.585843Z","shell.execute_reply":"2024-01-10T10:27:51.584501Z","shell.execute_reply.started":"2024-01-10T10:27:51.570239Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","BASE     = prc_dir +'df_1.pkl'\n","CALENDAR = prc_dir +'df_2.pkl'\n","NUM_ITEMS = 30490 # Number of items per each day"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:51.590934Z","iopub.status.busy":"2024-01-10T10:27:51.590507Z","iopub.status.idle":"2024-01-10T10:27:55.678882Z","shell.execute_reply":"2024-01-10T10:27:55.677426Z","shell.execute_reply.started":"2024-01-10T10:27:51.590898Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Masking\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","from keras import backend as K"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:55.681316Z","iopub.status.busy":"2024-01-10T10:27:55.680367Z","iopub.status.idle":"2024-01-10T10:27:57.845377Z","shell.execute_reply":"2024-01-10T10:27:57.843885Z","shell.execute_reply.started":"2024-01-10T10:27:55.681266Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:27:57.847150Z","iopub.status.busy":"2024-01-10T10:27:57.846774Z","iopub.status.idle":"2024-01-10T10:28:07.386122Z","shell.execute_reply":"2024-01-10T10:28:07.384994Z","shell.execute_reply.started":"2024-01-10T10:27:57.847118Z"},"trusted":true},"outputs":[],"source":["# Define categorical and numerical columns\n","categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n","numerical_cols = ['sell_price']\n","\n","target_col = 'sales_amount'\n","\n","# Convert categorical columns to category dtype and encode with cat.codes\n","for col in categorical_cols:\n","    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","# Normalize numerical columns\n","scaler = MinMaxScaler()\n","df_all_data[numerical_cols] = scaler.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","df_all_data[target_col] = scaler.fit_transform(df_all_data[[target_col]].astype(np.float32))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:28:07.388832Z","iopub.status.busy":"2024-01-10T10:28:07.387576Z","iopub.status.idle":"2024-01-10T10:28:16.141470Z","shell.execute_reply":"2024-01-10T10:28:16.140402Z","shell.execute_reply.started":"2024-01-10T10:28:07.388757Z"},"trusted":true},"outputs":[],"source":["# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","TEST_END = 1969 # 1941\n","VAL_END = TEST_END - 28\n","TRAIN_END = VAL_END - 28 # 1885 -> Train only until the 28 days before the end of the data\n","\n","df_train = df_all_data[df_all_data['d'] < TRAIN_END]\n","df_val = df_all_data[(df_all_data['d'] >= TRAIN_END) & (df_all_data['d'] < VAL_END)]\n","df_test = df_all_data[df_all_data['d'] >= VAL_END - 7]\n","\n","del df_all_data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:28:16.144144Z","iopub.status.busy":"2024-01-10T10:28:16.143279Z","iopub.status.idle":"2024-01-10T10:28:16.153399Z","shell.execute_reply":"2024-01-10T10:28:16.152138Z","shell.execute_reply.started":"2024-01-10T10:28:16.144099Z"},"trusted":true},"outputs":[],"source":["# Custom Generator Function\n","def lstm_data_generator(df, target, days_per_sequence=1, batch_size=32):\n","    total_sequences = (len(df) - NUM_ITEMS * days_per_sequence) // NUM_ITEMS # 1878 for train, 21 for val and test\n","    while True: \n","        for i in range(0, total_sequences, batch_size): # 0, 32, 64, ...1878\n","            batch_sequences = []\n","            batch_targets = []\n","            for b in range(batch_size): # 0, 1, 2,... 32\n","                if i + b < total_sequences: # 0, 0; 0, 1; 0, 2; ...; 0, 32; 32, 0; 32, 1; ...\n","                    start_idx = (i + b) * NUM_ITEMS\n","                    end_idx = start_idx + NUM_ITEMS * days_per_sequence\n","                    batch_sequences.append(df.iloc[start_idx:end_idx, :].drop(target, axis=1).to_numpy()) #drop target column, Only the values in the DataFrame will be returned, the axes labels will be removed.\n","                    batch_targets.append(df.iloc[end_idx:end_idx + NUM_ITEMS][target].to_numpy())\n","            yield np.array(batch_sequences), np.array(batch_targets)\n","\n","\n","\n","# def lstm_data_generator(df, target, days_per_sequence=7, batch_size=32):\n","#     total_sequences = (len(df) - NUM_ITEMS * (days_per_sequence + 1)) // NUM_ITEMS\n","#     while True:\n","#         for i in range(0, total_sequences, batch_size):\n","#             batch_sequences = []\n","#             batch_targets = []\n","\n","#             for b in range(batch_size):\n","#                 if i + b < total_sequences:\n","#                     start_idx = (i + b) * NUM_ITEMS\n","#                     end_idx = start_idx + NUM_ITEMS * days_per_sequence\n","#                     next_day_idx = end_idx + NUM_ITEMS\n","\n","#                     # Sequence data with target for past time_step days\n","#                     sequence_data = df.iloc[start_idx:end_idx, :].copy()\n","                    \n","#                     # Adding is_current_day feature to let the model distinguish between the past data and current to predicting day\n","#                     # create a new column 'is_current_day' and fill it with 0s and set datatype to int8\n","#                     sequence_data['is_current_day'] = 0\n","#                     sequence_data['is_current_day'] = sequence_data['is_current_day'].astype(np.int8)\n","\n","#                     # Data for the current to predicting day without target, because in real life we don't have it\n","#                     sixth_day_data = df.iloc[end_idx:next_day_idx, :].copy()\n","#                     #fill column 'sales_amount' with NaNs\n","#                     sixth_day_data[target] = np.nan\n","#                     # Give model info that this is the current day\n","#                     sixth_day_data['is_current_day'] = 1\n","#                     sixth_day_data['is_current_day'] = sixth_day_data['is_current_day'].astype(np.int8)\n","\n","#                     # Combine data\n","#                     sequence_with_sixth_day = pd.concat([sequence_data, sixth_day_data], axis=0)\n","\n","#                     # Append to batch\n","#                     batch_sequences.append(sequence_with_sixth_day.to_numpy())\n","\n","#                     # Target for the 6th day\n","#                     batch_targets.append(df.iloc[end_idx:next_day_idx, :][target].to_numpy())\n","\n","#             yield np.array(batch_sequences), np.array(batch_targets)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:28:16.155432Z","iopub.status.busy":"2024-01-10T10:28:16.155114Z","iopub.status.idle":"2024-01-10T10:28:16.163243Z","shell.execute_reply":"2024-01-10T10:28:16.162255Z","shell.execute_reply.started":"2024-01-10T10:28:16.155405Z"},"trusted":true},"outputs":[],"source":["# Usage\n","time_steps = 7  # Number of days per sequence\n","batch_size = 32  # Size of each batch\n","epochs= 2\n","train_gen = lstm_data_generator(df_train, target_col, time_steps, batch_size)\n","val_gen = lstm_data_generator(df_val, target_col, time_steps, batch_size)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(32, 213430, 20)\n","(32, 30490)\n"]}],"source":["# iter through the generator\n","x, y = next(train_gen)\n","print(x.shape)\n","print(y.shape)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:29:59.577604Z","iopub.status.busy":"2024-01-10T10:29:59.577105Z","iopub.status.idle":"2024-01-10T10:29:59.583179Z","shell.execute_reply":"2024-01-10T10:29:59.581998Z","shell.execute_reply.started":"2024-01-10T10:29:59.577565Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:30:07.348537Z","iopub.status.busy":"2024-01-10T10:30:07.348114Z","iopub.status.idle":"2024-01-10T10:30:07.664360Z","shell.execute_reply":"2024-01-10T10:30:07.663177Z","shell.execute_reply.started":"2024-01-10T10:30:07.348503Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_2 (LSTM)               (None, 50)                14200     \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 51        \n","                                                                 \n","=================================================================\n","Total params: 14,251\n","Trainable params: 14,251\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["2024-01-11 16:09:28.831524: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n","\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n","2024-01-11 16:09:28.832005: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n","\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n","2024-01-11 16:09:28.832594: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n","\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"]}],"source":["# Assuming you have your LSTM model defined\n","#input_shape = (time_steps + 1, len(df_train.columns) + 1) input shape for training data from predicting day\n","input_shape = (time_steps, len(df_train.columns) - 1)\n","model = Sequential([\n","    # Masking(mask_value=np.nan, input_shape=input_shape),\n","    # LSTM(50, activation='tanh'),\n","    LSTM(50, activation='tanh', input_shape=(input_shape)), #stateful=True return state, return sequences\n","    Dense(1)\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', \n","              loss='mse', #rmse\n","              metrics=[RootMeanSquaredError()])\n","model.summary()\n","\n","\n","# Add CNN layer\n","# model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(time_steps, num_features)))\n","# model.add(MaxPooling1D(pool_size=2))\n","# model.add(Flatten())\n","# model.add(LSTM(50, activation='relu'))\n","# model.add(Dense(1))  # or more layers as needed\n","# model.compile()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:28:16.874593Z","iopub.status.busy":"2024-01-10T10:28:16.874270Z","iopub.status.idle":"2024-01-10T10:28:16.898990Z","shell.execute_reply":"2024-01-10T10:28:16.897993Z","shell.execute_reply.started":"2024-01-10T10:28:16.874566Z"},"trusted":true},"outputs":[],"source":["# df head with all columns displayed\n","#pd.set_option('display.max_columns', None)\n","#df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T10:30:12.049115Z","iopub.status.busy":"2024-01-10T10:30:12.048653Z","iopub.status.idle":"2024-01-10T13:20:45.471124Z","shell.execute_reply":"2024-01-10T13:20:45.467342Z","shell.execute_reply.started":"2024-01-10T10:30:12.049079Z"},"trusted":true},"outputs":[],"source":["# Train the model using the generator\n","# model.fit(x=train_gen, steps_per_epoch=(len(df_train) // (batch_size * NUM_ITEMS)), epochs=3) # x: In case of a generator the target y will be obtained from x; steps_per_epoch: 57mio // (32*30490) = 58\n","\n","# Fit the model and store history for later evaluation\n","history = model.fit(\n","    x=train_gen,\n","    epochs=epochs,\n","    steps_per_epoch=(len(df_train) // (batch_size * NUM_ITEMS)),\n","    validation_data=val_gen,\n","    validation_steps=(len(df_train) // (batch_size * NUM_ITEMS))\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T13:20:47.566799Z","iopub.status.busy":"2024-01-10T13:20:47.566068Z","iopub.status.idle":"2024-01-10T13:20:47.590351Z","shell.execute_reply":"2024-01-10T13:20:47.589421Z","shell.execute_reply.started":"2024-01-10T13:20:47.566755Z"},"trusted":true},"outputs":[],"source":["# Save the model to a specified directory\n","if code_env=='local':\n","    ###local###\n","    model.save('/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction/models/V1_1layer_LSTM.h5')\n","    \n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    model.save('/kaggle/working/models/V2_1layer_LSTM.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Start from here if you want to load the model\n","from keras.models import load_model\n","\n","# Load the model from a specified directory\n","if code_env=='local':\n","    ###local###\n","    model = load_model('/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction/src/models/V1_1layer_LSTM.h5', custom_objects={'rmse': rmse})\n","\n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    model = load_model('/kaggle/working/models/V1_1layer_LSTM.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-10T13:25:08.931943Z","iopub.status.busy":"2024-01-10T13:25:08.930706Z","iopub.status.idle":"2024-01-10T13:25:09.215835Z","shell.execute_reply":"2024-01-10T13:25:09.214588Z","shell.execute_reply.started":"2024-01-10T13:25:08.931897Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1.437e+03, 3.000e+00, 1.000e+00, ..., 1.900e+01, 4.000e+00,\n","        5.000e+00],\n","       [1.438e+03, 3.000e+00, 1.000e+00, ..., 1.900e+01, 4.000e+00,\n","        5.000e+00],\n","       [1.439e+03, 3.000e+00, 1.000e+00, ..., 1.900e+01, 4.000e+00,\n","        5.000e+00],\n","       ...,\n","       [1.434e+03, 2.000e+00, 0.000e+00, ..., 1.900e+01, 4.000e+00,\n","        5.000e+00],\n","       [1.435e+03, 2.000e+00, 0.000e+00, ..., 1.900e+01, 4.000e+00,\n","        5.000e+00],\n","       [1.436e+03, 2.000e+00, 0.000e+00, ..., 1.900e+01, 4.000e+00,\n","        5.000e+00]], dtype=float32)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["df_test.iloc[0:7*30490, : ].drop(target_col, axis=1).to_numpy()#.shape()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"in user code:\n\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 7, 20), found shape=(None, 7, 609800)\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m         forecast_output\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mpredict(sequence_reshaped))\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(forecast_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m forecast_output \u001b[38;5;241m=\u001b[39m prepare_forecast_input(df_test, target_col, model, time_steps, NUM_ITEMS)\n","Cell \u001b[0;32mIn[34], line 38\u001b[0m, in \u001b[0;36mprepare_forecast_input\u001b[0;34m(df, target, model, time_steps, num_items)\u001b[0m\n\u001b[1;32m     36\u001b[0m     sequence_reshaped \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, time_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to (1, 7, num_features)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Predict and store the forecast\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     forecast_output\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mpredict(sequence_reshaped))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(forecast_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/var/folders/w1/_96k6h412s5d15f14hdtbbyc0000gn/T/__autograph_generated_filevn2cildi.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 7, 20), found shape=(None, 7, 609800)\n"]}],"source":["# def prepare_forecast_input(df, time_steps, num_items):\n","#     #df_test starts at 1942-7 which we need take into account\n","#     # Prepare input data for forecasting\n","#     forecast_input = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + time_steps * num_items\n","#         sequence = df.iloc[start_idx:end_idx].drop('sales_amount', axis=1).to_numpy()\n","#         forecast_input.append(sequence)\n","#     return np.array(forecast_input)\n","\n","\n","\n","def prepare_forecast_input(df, target, model, time_steps, num_items):\n","    forecast_output = []\n","    for target_day in range(28):\n","        start_idx = target_day * num_items\n","        end_idx = start_idx + time_steps * num_items\n","        sequence = df.iloc[start_idx:end_idx, : ].drop(target, axis=1).to_numpy()\n","        # forecast_output.append(model.predict(sequence))\n","        forecast_output.append(model.predict(sequence.reshape(1, sequence.shape[0], sequence.shape[1])))\n","    return np.array(forecast_output)#.reshape(-1, 1)\n","\n","forecast_output = prepare_forecast_input(df_test, target_col, model, time_steps, NUM_ITEMS)\n","#forecasts_original = scaler.inverse_transform(forecast_output)\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Assuming df_all_data contains all data up to day 1941\n","# forecast_input = prepare_forecast_input(df_test, time_steps, NUM_ITEMS)\n","\n","# Generate forecasts\n","# forecasts = model.predict(forecast_input)\n","# forecasts_original = scaler.inverse_transform(forecasts)\n","\n","# forecasts_original now contains the predicted sales amounts for days 1942 to 1969\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:28:19.990527Z","iopub.status.idle":"2024-01-10T10:28:19.990989Z","shell.execute_reply":"2024-01-10T10:28:19.990764Z","shell.execute_reply.started":"2024-01-10T10:28:19.990744Z"},"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:28:19.992214Z","iopub.status.idle":"2024-01-10T10:28:19.992611Z","shell.execute_reply":"2024-01-10T10:28:19.992440Z","shell.execute_reply.started":"2024-01-10T10:28:19.992421Z"},"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, time_steps, n):\n","    test_gen = lstm_data_generator(df_test, target_col, time_steps, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:28:19.994039Z","iopub.status.idle":"2024-01-10T10:28:19.994425Z","shell.execute_reply":"2024-01-10T10:28:19.994260Z","shell.execute_reply.started":"2024-01-10T10:28:19.994241Z"},"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, time_steps, VAL_END)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4280642,"sourceId":7368162,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
