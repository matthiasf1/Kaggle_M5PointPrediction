{"cells":[{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V3_ohne_Cat_features'\n","CODE_ENV = 'aws' #'kaggle', 'aws', 'local'\n","TEST_END  = 1941 #1969 "]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.393140Z","iopub.status.busy":"2024-01-22T13:02:34.392833Z","iopub.status.idle":"2024-01-22T13:02:47.056364Z","shell.execute_reply":"2024-01-22T13:02:47.055528Z","shell.execute_reply.started":"2024-01-22T13:02:34.393114Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Masking, RepeatVector, Dropout, Reshape\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","from keras import backend as K\n","from keras.callbacks import Callback\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n","True\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.373809Z","iopub.status.busy":"2024-01-22T13:02:34.373363Z","iopub.status.idle":"2024-01-22T13:02:34.379504Z","shell.execute_reply":"2024-01-22T13:02:34.378489Z","shell.execute_reply.started":"2024-01-22T13:02:34.373780Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v3/'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.381096Z","iopub.status.busy":"2024-01-22T13:02:34.380802Z","iopub.status.idle":"2024-01-22T13:02:34.390571Z","shell.execute_reply":"2024-01-22T13:02:34.389570Z","shell.execute_reply.started":"2024-01-22T13:02:34.381070Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","BASE      = prc_dir +'df_1.pkl'\n","CALENDAR  = prc_dir +'df_2.pkl'\n","NUM_ITEMS = 30490 # Number of items per each day\n","# Set time_steps for defining test, train and validation sets\n","DAYS_PER_SEQUENCE = 28  # Length of the sequence\n","target_col = 'sales_amount'"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:47.058053Z","iopub.status.busy":"2024-01-22T13:02:47.057502Z","iopub.status.idle":"2024-01-22T13:03:01.455370Z","shell.execute_reply":"2024-01-22T13:03:01.454363Z","shell.execute_reply.started":"2024-01-22T13:02:47.058028Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:01.457014Z","iopub.status.busy":"2024-01-22T13:03:01.456690Z","iopub.status.idle":"2024-01-22T13:03:09.962427Z","shell.execute_reply":"2024-01-22T13:03:09.961481Z","shell.execute_reply.started":"2024-01-22T13:03:01.456986Z"},"trusted":true},"outputs":[],"source":["# Define categorical and numerical columns\n","categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n","numerical_cols = ['sell_price']\n","\n","# Convert categorical columns to category dtype and encode with cat.codes\n","for col in categorical_cols:\n","    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","# Normalize numerical columns\n","scaler_numerical = MinMaxScaler()\n","df_all_data[numerical_cols] = scaler_numerical.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","\n","scaler_target = MinMaxScaler()\n","df_all_data[target_col] = scaler_target.fit_transform(df_all_data[[target_col]].astype(np.float64))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:09.970478Z","iopub.status.busy":"2024-01-22T13:03:09.970194Z","iopub.status.idle":"2024-01-22T13:03:18.370792Z","shell.execute_reply":"2024-01-22T13:03:18.369916Z","shell.execute_reply.started":"2024-01-22T13:03:09.970453Z"},"trusted":true},"outputs":[],"source":["# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation \n","\n","df_train = df_all_data[df_all_data['d'] < TRAIN_END].reset_index(drop=True)\n","df_val   = df_all_data[(df_all_data['d'] >= TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] < VAL_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","df_test  = df_all_data[(df_all_data['d'] >= VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] < TEST_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","\n","# Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","del df_all_data"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.372402Z","iopub.status.busy":"2024-01-22T13:03:18.372106Z","iopub.status.idle":"2024-01-22T13:03:18.386039Z","shell.execute_reply":"2024-01-22T13:03:18.385156Z","shell.execute_reply.started":"2024-01-22T13:03:18.372378Z"},"trusted":true},"outputs":[],"source":["def lstm_data_generator(df, num_features, target, days_sliding_window, batch_size, once_only_features, repeated_features):\n","    length_days = len(df) // NUM_ITEMS  # 1941 days\n","    while True:\n","        for i in range(0, length_days - days_sliding_window):\n","            start_ind = i * NUM_ITEMS\n","            end_ind = start_ind + NUM_ITEMS * (days_sliding_window)  # predict the next day after the sequence\n","\n","            # Extract once-only features for all days in the sequence at once\n","            once_features = df.iloc[start_ind:end_ind:NUM_ITEMS][once_only_features].to_numpy()\n","            # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","            # Extract repeated features for all items and days at once\n","            repeated_features_stack = df.iloc[start_ind:end_ind][repeated_features].to_numpy() # 210,000 items, 10 features\n","\n","            # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","            reshaped_3d = repeated_features_stack.reshape(days_sliding_window, NUM_ITEMS, len(repeated_features))\n","\n","            # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","            final_array = reshaped_3d.reshape(days_sliding_window, -1)\n","\n","            # Combine once-only and repeated features\n","            batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","            # Reshape batch_sequences to match LSTM input shape\n","            batch_sequences = batch_sequences.reshape(1, days_sliding_window, -1)\n","\n","            # Extract targets\n","            batch_targets = df.iloc[end_ind:end_ind + NUM_ITEMS][target].to_numpy()\n","\n","            # Yield the batch\n","            yield batch_sequences, batch_targets"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Initialize the generator\n","# repeated_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","repeated_features = ['sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","# once_only_features = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","once_only_features = ['snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","num_features = len(once_only_features) + len(repeated_features) * NUM_ITEMS # Calculate the number of features\n","\n","train_generator = lstm_data_generator(df_train, num_features, target_col, days_sliding_window=DAYS_PER_SEQUENCE, batch_size=1, once_only_features=once_only_features, repeated_features=repeated_features)\n","val_generator = lstm_data_generator(df_val, num_features, target_col, days_sliding_window=DAYS_PER_SEQUENCE, batch_size=1, once_only_features=once_only_features, repeated_features=repeated_features)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# For testing purposes: check how large on batch is\n","# next train_generator\n","# x, y = next(train_generator)\n","# # size of memory in mb of x and y\n","# print(x.nbytes / 1e6)\n","# print(y.nbytes / 1e6)\n","\n","# print(x.shape)\n","# print(y.shape)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# # list all columns in df_train\n","# df_train.columns\n","\n","# # call head of df_train displaying all columns without truncation\n","# pd.set_option('display.max_columns', None)\n","# df_train.head()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.428419Z","iopub.status.busy":"2024-01-22T13:03:18.428151Z","iopub.status.idle":"2024-01-22T13:03:18.437168Z","shell.execute_reply":"2024-01-22T13:03:18.436284Z","shell.execute_reply.started":"2024-01-22T13:03:18.428395Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.438535Z","iopub.status.busy":"2024-01-22T13:03:18.438246Z","iopub.status.idle":"2024-01-22T13:03:19.715259Z","shell.execute_reply":"2024-01-22T13:03:19.714216Z","shell.execute_reply.started":"2024-01-22T13:03:18.438511Z"},"trusted":true},"outputs":[],"source":["# This is a sequence-to-sequence model: errors can propagate through the sequence\n","# model = Sequential()\n","\n","# model.add(LSTM(units=30,\n","#                activation='tanh', #relu\n","#                return_sequences=False,\n","#                stateful=True))\n","\n","# model.add(RepeatVector(28))\n","\n","# model.add(LSTM(units=30, \n","#                activation='tanh', \n","#                return_sequences=True, \n","#                stateful=True))\n","\n","# model.add(Dense(units=1))\n","\n","# model.compile(optimizer='adam', loss='mean_squared_error')"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.389636Z","iopub.status.busy":"2024-01-22T13:03:18.389317Z","iopub.status.idle":"2024-01-22T13:03:18.416778Z","shell.execute_reply":"2024-01-22T13:03:18.415805Z","shell.execute_reply.started":"2024-01-22T13:03:18.389597Z"},"trusted":true},"outputs":[],"source":["# Model parameters\n","epochs = 5\n","batch_size = 1\n","lr = 0.01"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["# Neu: Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","Units_LSTM_1 = 500\n","Units_LSTM_2 = 300\n","Units_LSTM_3 = 200\n","Units_LSTM_4 = 200\n","Units_Dense_1 = 200\n","\n","model = Sequential()\n","\n","# First LSTM layer with more units and return sequences\n","model.add(LSTM(units=Units_LSTM_1, \n","               activation='tanh', \n","               return_sequences=True, \n","               stateful=True,\n","               input_shape=(DAYS_PER_SEQUENCE, num_features),\n","               batch_input_shape=(batch_size, DAYS_PER_SEQUENCE, num_features)))\n","model.add(Dropout(0.3))\n","\n","# Second LSTM layer with less units and return sequences\n","model.add(LSTM(units=Units_LSTM_2, \n","               activation='tanh', \n","               return_sequences=True, \n","               stateful=True))\n","model.add(Dropout(0.3))\n","\n","# Third LSTM layer with less units and return sequences\n","model.add(LSTM(units=Units_LSTM_3, \n","               activation='tanh', \n","               return_sequences=True, \n","               stateful=True))\n","model.add(Dropout(0.3))\n","\n","# Additional LSTM Layer\n","model.add(LSTM(units=Units_LSTM_4, \n","               activation='tanh', \n","               return_sequences=False, \n","               stateful=True))\n","model.add(Dropout(0.3))\n","\n","# Dense layer\n","model.add(Dense(units=Units_Dense_1, \n","                activation='relu'))\n","\n","# Final Dense layer for output\n","model.add(Dense(units=NUM_ITEMS))\n","\n","# Reshape the output to be (number of items)\n","#model.add(Reshape((NUM_ITEMS,))) # Eigentlich mÃ¼sste das vorherige Layer bereits die richtige Form haben\n","\n","model.compile(optimizer=Adam(learning_rate=lr), \n","              loss=rmse, \n","              metrics=[RootMeanSquaredError()])"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_4 (LSTM)               (1, 28, 500)              183948000 \n","                                                                 \n"," dropout_4 (Dropout)         (1, 28, 500)              0         \n","                                                                 \n"," lstm_5 (LSTM)               (1, 28, 300)              961200    \n","                                                                 \n"," dropout_5 (Dropout)         (1, 28, 300)              0         \n","                                                                 \n"," lstm_6 (LSTM)               (1, 28, 200)              400800    \n","                                                                 \n"," dropout_6 (Dropout)         (1, 28, 200)              0         \n","                                                                 \n"," lstm_7 (LSTM)               (1, 200)                  320800    \n","                                                                 \n"," dropout_7 (Dropout)         (1, 200)                  0         \n","                                                                 \n"," dense_2 (Dense)             (1, 200)                  40200     \n","                                                                 \n"," dense_3 (Dense)             (1, 30490)                6128490   \n","                                                                 \n","=================================================================\n","Total params: 191799490 (731.66 MB)\n","Trainable params: 191799490 (731.66 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# For tracking purposes: check the models parameters\n","model.summary()\n","\n","# Print input shape of the layers\n","# for layer in model.layers:\n","#     print(layer.input_shape)"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:19.716971Z","iopub.status.busy":"2024-01-22T13:03:19.716580Z","iopub.status.idle":"2024-01-22T13:03:19.722552Z","shell.execute_reply":"2024-01-22T13:03:19.721412Z","shell.execute_reply.started":"2024-01-22T13:03:19.716930Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T21:41:47.956270Z","iopub.status.busy":"2024-01-22T21:41:47.955997Z","iopub.status.idle":"2024-01-22T21:41:48.300127Z","shell.execute_reply":"2024-01-22T21:41:48.298879Z","shell.execute_reply.started":"2024-01-22T21:41:47.956244Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1857/1857 [==============================] - 218s 114ms/step - loss: 0.0054 - root_mean_squared_error: 0.0057 - val_loss: 0.0048 - val_root_mean_squared_error: 0.0049\n","Epoch 2/5\n","1857/1857 [==============================] - 210s 113ms/step - loss: 0.0052 - root_mean_squared_error: 0.0053 - val_loss: 0.0049 - val_root_mean_squared_error: 0.0049\n","Epoch 3/5\n","1857/1857 [==============================] - 210s 113ms/step - loss: 0.0052 - root_mean_squared_error: 0.0053 - val_loss: 0.0049 - val_root_mean_squared_error: 0.0049\n","Epoch 4/5\n","1857/1857 [==============================] - 210s 113ms/step - loss: 0.0052 - root_mean_squared_error: 0.0054 - val_loss: 0.0050 - val_root_mean_squared_error: 0.0050\n","Epoch 5/5\n","1857/1857 [==============================] - 210s 113ms/step - loss: 0.0052 - root_mean_squared_error: 0.0053 - val_loss: 0.0049 - val_root_mean_squared_error: 0.0049\n"]}],"source":["# Training the model\n","history = model.fit(x=train_generator,\n","          steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","          validation_data=val_generator,\n","          validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","          epochs=epochs,\n","          callbacks=[ResetStatesCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train and validation df not needed anymore\n","del df_train\n","del df_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:30:39.354772Z","iopub.status.busy":"2024-01-18T14:30:39.353690Z","iopub.status.idle":"2024-01-18T14:30:39.384792Z","shell.execute_reply":"2024-01-18T14:30:39.383711Z","shell.execute_reply.started":"2024-01-18T14:30:39.354715Z"},"trusted":true},"outputs":[],"source":["# Save the model to a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')\n","    \n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model.save('/kaggle/working/' + MODEL_NAME + '.h5')\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Start from here if you want to load the model\n","from keras.models import load_model\n","\n","# Load the model from a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model = load_model(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model = load_model('/kaggle/input/v1-model/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:00.674762Z","iopub.status.busy":"2024-01-18T14:34:00.673913Z","iopub.status.idle":"2024-01-18T14:34:00.949572Z","shell.execute_reply":"2024-01-18T14:34:00.948466Z","shell.execute_reply.started":"2024-01-18T14:34:00.674717Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["x, y = next(val_generator)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n"]}],"source":["prediction_original = model.predict(x)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.00310882, 0.00330687, 0.00304901, ..., 0.00324354, 0.0032471 ,\n","        0.00324578]], dtype=float32)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["prediction_original"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.        , 0.        , 0.        , ..., 0.00524246, 0.        ,\n","       0.        ])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0., 0., 0., ..., 4., 0., 0.]])"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["scaler_target.inverse_transform([y])"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["array([[2.3720267, 2.5231385, 2.3263931, ..., 2.4748173, 2.4775355,\n","        2.4765294]], dtype=float32)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["scaler_target.inverse_transform(prediction_original)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction_original"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict_next_day(model, last_window_data, num_features):\n","    # Predict the next day\n","    next_day_prediction = model.predict(last_window_data.reshape(1, 7, num_features))\n","    return next_day_prediction\n","\n","# Assuming you have a function to update your data with new predictions\n","def update_data_with_prediction(data, new_prediction):\n","    # Logic to update your dataset with the new_prediction\n","    # This could involve shifting the window and inserting the new prediction\n","    pass\n","\n","# Starting with actual historical data\n","last_window_data = get_last_window_of_actual_data()  # Shape: (7, num_features)\n","\n","for i in range(num_future_days):\n","    # Predict the next day\n","    next_day_prediction = predict_next_day(model, last_window_data, num_features)\n","\n","    # Update your data with this new prediction\n","    last_window_data = update_data_with_prediction(last_window_data, next_day_prediction)\n","\n","    # Now last_window_data contains the most recent prediction, which will be used in the next iteration\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def prepare_forecast_input(df, DAYS_PER_SEQUENCE, num_items):\n","#     #df_test starts at 1942-7 which we need take into account\n","#     # Prepare input data for forecasting\n","#     forecast_input = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx].drop('sales_amount', axis=1).to_numpy()\n","#         forecast_input.append(sequence)\n","#     return np.array(forecast_input)\n","\n","\n","# Custom function for input to prepare forecasts input for model\n","# def prepare_forecast_input(df, target, model, DAYS_PER_SEQUENCE, num_items):\n","#     forecast_output = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + DAYS_PER_SEQUENCE * num_items\n","#         sequence = df.iloc[start_idx:end_idx, : ].drop(target, axis=1).to_numpy()\n","#         # forecast_output.append(model.predict(sequence))\n","#         forecast_output.append(model.predict(sequence.reshape(1, sequence.shape[0], sequence.shape[1])))\n","#     return np.array(forecast_output)#.reshape(-1, 1)\n","# forecast_output = prepare_forecast_input(df_test, target_col, model, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","#forecasts_original = scaler.inverse_transform(forecast_output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming df_all_data contains all data up to day 1941\n","# forecast_input = prepare_forecast_input(df_test, DAYS_PER_SEQUENCE, NUM_ITEMS)\n","\n","# Generate forecasts\n","# forecasts = model.predict(forecast_input)\n","# forecasts_original = scaler.inverse_transform(forecasts)\n","\n","# forecasts_original now contains the predicted sales amounts for days 1942 to 1969\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:26.857391Z","iopub.status.busy":"2024-01-18T14:34:26.857016Z","iopub.status.idle":"2024-01-18T14:34:27.273805Z","shell.execute_reply":"2024-01-18T14:34:27.272980Z","shell.execute_reply.started":"2024-01-18T14:34:26.857362Z"},"trusted":true},"outputs":[],"source":["# Prepare input for forecasts\n","# I cannot use the custom lstm_data_generator\n","# Prepare 7 day slices each shifted by one day\n","def prepare_forecast_input(df, DAYS_PER_SEQUENCE, target_col):\n","    forecast_input = []\n","    for i in range(0, len(df)//NUM_ITEMS): #i=0; 1, 2, 3, ..., 35?\n","        if i + DAYS_PER_SEQUENCE < (len(df)-1)//NUM_ITEMS: #7, 8, 9, 10, ...\n","            start_idx = i*NUM_ITEMS\n","            end_idx   = start_idx + NUM_ITEMS * DAYS_PER_SEQUENCE\n","            sequence  = df.iloc[start_idx : end_idx, :].drop(target_col, axis=1).to_numpy()\n","            forecast_input.append(sequence)\n","    return np.array(forecast_input)\n","\n","predict_array = prepare_forecast_input(df=df_test, DAYS_PER_SEQUENCE=DAYS_PER_SEQUENCE, target_col=target_col)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, n):\n","    test_gen = lstm_data_generator(df_test, target_col, DAYS_PER_SEQUENCE, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","# evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, DAYS_PER_SEQUENCE, VAL_END)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
