{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V2'\n","CODE_ENV = 'local' #'kaggle', 'aws', 'local'\n","TEST_END  = 1941 #1969 "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.393140Z","iopub.status.busy":"2024-01-22T13:02:34.392833Z","iopub.status.idle":"2024-01-22T13:02:47.056364Z","shell.execute_reply":"2024-01-22T13:02:47.055528Z","shell.execute_reply.started":"2024-01-22T13:02:34.393114Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Masking, RepeatVector\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","from keras import backend as K\n","from keras.callbacks import Callback\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.373809Z","iopub.status.busy":"2024-01-22T13:02:34.373363Z","iopub.status.idle":"2024-01-22T13:02:34.379504Z","shell.execute_reply":"2024-01-22T13:02:34.378489Z","shell.execute_reply.started":"2024-01-22T13:02:34.373780Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v3/'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.381096Z","iopub.status.busy":"2024-01-22T13:02:34.380802Z","iopub.status.idle":"2024-01-22T13:02:34.390571Z","shell.execute_reply":"2024-01-22T13:02:34.389570Z","shell.execute_reply.started":"2024-01-22T13:02:34.381070Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","BASE      = prc_dir +'df_1.pkl'\n","CALENDAR  = prc_dir +'df_2.pkl'\n","NUM_ITEMS = 30490 # Number of items per each day\n","# Set time_steps for defining test, train and validation sets\n","time_steps = 7  # Number of days per sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:47.058053Z","iopub.status.busy":"2024-01-22T13:02:47.057502Z","iopub.status.idle":"2024-01-22T13:03:01.455370Z","shell.execute_reply":"2024-01-22T13:03:01.454363Z","shell.execute_reply.started":"2024-01-22T13:02:47.058028Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:01.457014Z","iopub.status.busy":"2024-01-22T13:03:01.456690Z","iopub.status.idle":"2024-01-22T13:03:09.962427Z","shell.execute_reply":"2024-01-22T13:03:09.961481Z","shell.execute_reply.started":"2024-01-22T13:03:01.456986Z"},"trusted":true},"outputs":[],"source":["# Define categorical and numerical columns\n","categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n","numerical_cols = ['sell_price']\n","\n","target_col = 'sales_amount'\n","\n","# Convert categorical columns to category dtype and encode with cat.codes\n","for col in categorical_cols:\n","    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","# Normalize numerical columns\n","scaler = MinMaxScaler()\n","df_all_data[numerical_cols] = scaler.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","df_all_data[target_col] = scaler.fit_transform(df_all_data[[target_col]].astype(np.float32))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:09.970478Z","iopub.status.busy":"2024-01-22T13:03:09.970194Z","iopub.status.idle":"2024-01-22T13:03:18.370792Z","shell.execute_reply":"2024-01-22T13:03:18.369916Z","shell.execute_reply.started":"2024-01-22T13:03:09.970453Z"},"trusted":true},"outputs":[],"source":["# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END # Depends on whether the whole dataset is used or last the 28 days for validation \n","\n","df_train = df_all_data[df_all_data['d'] < TRAIN_END].reset_index(drop=True)\n","df_val   = df_all_data[(df_all_data['d'] >= TRAIN_END - time_steps) & (df_all_data['d'] < VAL_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","df_test  = df_all_data[(df_all_data['d'] >= VAL_END - time_steps)   & (df_all_data['d'] < TEST_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","\n","# Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","del df_all_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.372402Z","iopub.status.busy":"2024-01-22T13:03:18.372106Z","iopub.status.idle":"2024-01-22T13:03:18.386039Z","shell.execute_reply":"2024-01-22T13:03:18.385156Z","shell.execute_reply.started":"2024-01-22T13:03:18.372378Z"},"trusted":true},"outputs":[],"source":["# Version 1: \n","# x input: 7 days without sales_amount\n","# y labels: only 8th day sales_amount\n","\n","# Custom Generator Function\n","# def lstm_data_generator(df, target, days_per_sequence=7, batch_size=32):\n","#     total_sequences = (len(df) - NUM_ITEMS * days_per_sequence) // NUM_ITEMS # 1878 for train, 21 for val and test; (1941*30490-7*30490)\n","#     while True: \n","#         for i in range(0, total_sequences, batch_size): # 0, 32, 64, ...1878\n","#             batch_sequences = []\n","#             batch_targets = []\n","#             for b in range(batch_size): # 0, 1, 2,... 31\n","#                 if i + b < total_sequences: # 0, 0; 0, 1; 0, 2; ...; 0, 32; 32, 0; 32, 1; ...\n","#                     start_idx = (i + b) * NUM_ITEMS\n","#                     end_idx = start_idx + NUM_ITEMS * days_per_sequence\n","#                     batch_sequences.append(df.iloc[start_idx:end_idx, :].drop(target, axis=1).to_numpy()) #drop target column, Only the values in the DataFrame will be returned, the axes labels will be removed.\n","#                     batch_targets.append(df.iloc[end_idx:end_idx + NUM_ITEMS][target].to_numpy())\n","#             yield np.array(batch_sequences), np.array(batch_targets)\n","\n","\n","\n","# To-Do\n","# -Für bisherigen Generator: prediction erstellen und bei Kaggle einreichen. Völlig egal ob shape passt, hauptsache shape beim training identisch\n","#    - Jeweils für 7 Tage x Werte und 8. Tag y labels\n","#    - 8. Tag X Werte und 8 Tag y labels\n","# -Testen, ob ich generator bauen kann, der 30,490 als sequenz ausgibt (30490,7,20) als input shape ausgibt und total_sequences kann glaube sogar bleiben\n","\n","\n","# Version 2:\n","# This generator creates:\n","# Number of batches: 1878 (for df_train); df_val and df_test have 28 batches\n","# X: (30490, 7, 20) --> 7 days without sales_amount\n","# Y: (30490, 1)     --> only 8th day sales_amount\n","\n","def lstm_data_generator(df, target, days_per_sequence=7, batch_size=30490):\n","    while True:\n","        length_days = len(df) // NUM_ITEMS\n","        for i in range(length_days-time_steps): # 0 - 1877; 0-27\n","            # Initialize arrays for storing sequences and targets\n","            batch_sequences = np.zeros((batch_size, days_per_sequence, df.shape[1] - 1))  # minus 1 for target column; 30490, 7, 20\n","            batch_targets = np.zeros((batch_size, ))\n","\n","            # Loop over all items for the current day\n","            for item_idx in range(batch_size): #(0, 30489)\n","                start_idx = item_idx + (i * NUM_ITEMS) # 0+0*30490; 1+0*30490;...\n","                end_idx = start_idx + (days_per_sequence * NUM_ITEMS)\n","\n","                # Extract sequence for current item\n","                sequence = df.iloc[start_idx:end_idx:NUM_ITEMS].drop(target, axis=1).to_numpy()\n","                batch_sequences[item_idx, :, :] = sequence\n","\n","                # Extract target for current item\n","                target_value = df.iloc[end_idx + NUM_ITEMS][target]\n","                batch_targets[item_idx] = target_value\n","\n","            yield batch_sequences, batch_targets\n","\n","\n","\n","\n","\n","\n","\n","\n","# # Function to create sequences\n","# def create_sequences(dataframe, window_size=7):\n","#     X, Y = [], []\n","#     for i in range(len(dataframe) - window_size):\n","#         # Extract 7 days of data with all 20 features\n","#         x_sequence = dataframe.iloc[i:i+window_size, :].values\n","\n","#         # Extract the 19 known features for the 8th day\n","#         x_8th_day_known = dataframe.iloc[i + window_size, :-1].values.reshape(1, -1)  # Excluding sales amount\n","\n","#         # Add the new binary feature indicating the prediction day\n","#         prediction_day_indicator = np.zeros((window_size, 1))\n","#         prediction_day_indicator_8th_day = np.array([[1]])  # 1 for the 8th day\n","#         x_sequence = np.hstack((x_sequence, prediction_day_indicator))\n","#         x_8th_day_with_indicator = np.hstack((x_8th_day_known, prediction_day_indicator_8th_day))\n","\n","#         # Concatenate 7 days of data with the known features of the 8th day\n","#         x_sequence_with_8th_day = np.concatenate([x_sequence, x_8th_day_with_indicator], axis=0)\n","\n","#         # The target is the sales amount for the 8th day\n","#         y_value = dataframe.iloc[i + window_size, -1]  # Sales amount for the 8th day\n","\n","#         X.append(x_sequence_with_8th_day)\n","#         Y.append(y_value)\n","#     return np.array(X), np.array(Y)\n","\n","# # Create sequences\n","# X, Y = create_sequences(dataframe)\n","\n","# # Convert to tf.data.Dataset for training\n","# dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n","# dataset = dataset.batch(batch_size)  # Define your batch_size\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# def lstm_data_generator(df, target, days_per_sequence=7, batch_size=32):\n","#     total_sequences = (len(df) - NUM_ITEMS * (days_per_sequence + 1)) // NUM_ITEMS\n","#     while True:\n","#         for i in range(0, total_sequences, batch_size):\n","#             batch_sequences = []\n","#             batch_targets = []\n","\n","#             for b in range(batch_size):\n","#                 if i + b < total_sequences:\n","#                     start_idx = (i + b) * NUM_ITEMS\n","#                     end_idx = start_idx + NUM_ITEMS * days_per_sequence\n","#                     next_day_idx = end_idx + NUM_ITEMS\n","\n","#                     # Sequence data with target for past time_step days\n","#                     sequence_data = df.iloc[start_idx:end_idx, :].copy()\n","                    \n","#                     # Adding is_current_day feature to let the model distinguish between the past data and current to predicting day\n","#                     # create a new column 'is_current_day' and fill it with 0s and set datatype to int8\n","#                     sequence_data['is_current_day'] = 0\n","#                     sequence_data['is_current_day'] = sequence_data['is_current_day'].astype(np.int8)\n","\n","#                     # Data for the current to predicting day without target, because in real life we don't have it\n","#                     sixth_day_data = df.iloc[end_idx:next_day_idx, :].copy()\n","#                     #fill column 'sales_amount' with NaNs\n","#                     sixth_day_data[target] = np.nan\n","#                     # Give model info that this is the current day\n","#                     sixth_day_data['is_current_day'] = 1\n","#                     sixth_day_data['is_current_day'] = sixth_day_data['is_current_day'].astype(np.int8)\n","\n","#                     # Combine data\n","#                     sequence_with_sixth_day = pd.concat([sequence_data, sixth_day_data], axis=0)\n","\n","#                     # Append to batch\n","#                     batch_sequences.append(sequence_with_sixth_day.to_numpy())\n","\n","#                     # Target for the 6th day\n","#                     batch_targets.append(df.iloc[end_idx:next_day_idx, :][target].to_numpy())\n","\n","#             yield np.array(batch_sequences), np.array(batch_targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.389636Z","iopub.status.busy":"2024-01-22T13:03:18.389317Z","iopub.status.idle":"2024-01-22T13:03:18.416778Z","shell.execute_reply":"2024-01-22T13:03:18.415805Z","shell.execute_reply.started":"2024-01-22T13:03:18.389597Z"},"trusted":true},"outputs":[],"source":["# Model parameters\n","batch_size = 30490  #Size of each batch\n","epochs = 2\n","num_cols = df_train.shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.418400Z","iopub.status.busy":"2024-01-22T13:03:18.418117Z","iopub.status.idle":"2024-01-22T13:03:18.427041Z","shell.execute_reply":"2024-01-22T13:03:18.426240Z","shell.execute_reply.started":"2024-01-22T13:03:18.418376Z"},"trusted":true},"outputs":[],"source":["# Train and validation generators\n","train_generator = lstm_data_generator(df_train, target_col, time_steps, batch_size)\n","val_generator = lstm_data_generator(df_val, target_col, time_steps, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For testing purposes: check how large on batch is\n","# next train_generator\n","# x, y = next(train_generator)\n","# size of memory in mb of x and y\n","# print(x.nbytes / 1e6)\n","# print(y.nbytes / 1e6)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.428419Z","iopub.status.busy":"2024-01-22T13:03:18.428151Z","iopub.status.idle":"2024-01-22T13:03:18.437168Z","shell.execute_reply":"2024-01-22T13:03:18.436284Z","shell.execute_reply.started":"2024-01-22T13:03:18.428395Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.438535Z","iopub.status.busy":"2024-01-22T13:03:18.438246Z","iopub.status.idle":"2024-01-22T13:03:19.715259Z","shell.execute_reply":"2024-01-22T13:03:19.714216Z","shell.execute_reply.started":"2024-01-22T13:03:18.438511Z"},"trusted":true},"outputs":[],"source":["# This is a sequence-to-sequence model: errors can propagate through the sequence\n","# model = Sequential()\n","\n","# model.add(LSTM(units=30,\n","#                activation='tanh', #relu\n","#                return_sequences=False,\n","#                stateful=True))\n","\n","# model.add(RepeatVector(28))\n","\n","# model.add(LSTM(units=30, \n","#                activation='tanh', \n","#                return_sequences=True, \n","#                stateful=True))\n","\n","# model.add(Dense(units=1))\n","\n","# model.compile(optimizer='adam', loss='mean_squared_error')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","model = Sequential()\n","\n","# First LSTM layer\n","model.add(LSTM(units=50, \n","               activation='tanh',\n","               return_sequences=False,\n","               stateful=True))\n","\n","# Dense layer to make predictions for each day multiplies the number of units by the number of days you want to predict\n","model.add(Dense(units=NUM_ITEMS * TEST_DUR))\n","\n","# Reshape the output to be (number of days, number of items)\n","model.add(Reshape((TEST_DUR, NUM_ITEMS)))\n","\n","model.compile(optimizer='adam', \n","              loss='mse', # rmse\n","              metrics=[RootMeanSquaredError()])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For tracking purposes: check the models parameters\n","#model.summary()\n","\n","# Print input shape of the layers\n","for layer in model.layers:\n","    print(layer.input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:19.716971Z","iopub.status.busy":"2024-01-22T13:03:19.716580Z","iopub.status.idle":"2024-01-22T13:03:19.722552Z","shell.execute_reply":"2024-01-22T13:03:19.721412Z","shell.execute_reply.started":"2024-01-22T13:03:19.716930Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T21:41:47.956270Z","iopub.status.busy":"2024-01-22T21:41:47.955997Z","iopub.status.idle":"2024-01-22T21:41:48.300127Z","shell.execute_reply":"2024-01-22T21:41:48.298879Z","shell.execute_reply.started":"2024-01-22T21:41:47.956244Z"},"trusted":true},"outputs":[],"source":["# Training the model\n","history = model.fit(x=train_generator,\n","          steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","          validation_data=val_generator,\n","          validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","          epochs=epochs,\n","          callbacks=[ResetStatesCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train and validation df not needed anymore\n","del df_train\n","del df_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:30:39.354772Z","iopub.status.busy":"2024-01-18T14:30:39.353690Z","iopub.status.idle":"2024-01-18T14:30:39.384792Z","shell.execute_reply":"2024-01-18T14:30:39.383711Z","shell.execute_reply.started":"2024-01-18T14:30:39.354715Z"},"trusted":true},"outputs":[],"source":["# Save the model to a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model.save(src_dir + 'models/' + MODEL_NAME + '.h5')\n","    \n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model.save('/kaggle/working/V1_without_input_shape-model.h5')\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/V1.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Start from here if you want to load the model\n","from keras.models import load_model\n","\n","# Load the model from a specified directory\n","if CODE_ENV=='local':\n","    ###local###\n","    model = load_model(src_dir + 'models/V2.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    model = load_model('/kaggle/input/v1-model/V2.h5', custom_objects={'rmse': rmse})\n","\n","if CODE_ENV=='aws':\n","    ###aws###\n","    model.save(src_dir + 'models/V2.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:00.674762Z","iopub.status.busy":"2024-01-18T14:34:00.673913Z","iopub.status.idle":"2024-01-18T14:34:00.949572Z","shell.execute_reply":"2024-01-18T14:34:00.948466Z","shell.execute_reply.started":"2024-01-18T14:34:00.674717Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def prepare_forecast_input(df, time_steps, num_items):\n","#     #df_test starts at 1942-7 which we need take into account\n","#     # Prepare input data for forecasting\n","#     forecast_input = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + time_steps * num_items\n","#         sequence = df.iloc[start_idx:end_idx].drop('sales_amount', axis=1).to_numpy()\n","#         forecast_input.append(sequence)\n","#     return np.array(forecast_input)\n","\n","\n","# Custom function for input to prepare forecasts input for model\n","# def prepare_forecast_input(df, target, model, time_steps, num_items):\n","#     forecast_output = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + time_steps * num_items\n","#         sequence = df.iloc[start_idx:end_idx, : ].drop(target, axis=1).to_numpy()\n","#         # forecast_output.append(model.predict(sequence))\n","#         forecast_output.append(model.predict(sequence.reshape(1, sequence.shape[0], sequence.shape[1])))\n","#     return np.array(forecast_output)#.reshape(-1, 1)\n","# forecast_output = prepare_forecast_input(df_test, target_col, model, time_steps, NUM_ITEMS)\n","#forecasts_original = scaler.inverse_transform(forecast_output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming df_all_data contains all data up to day 1941\n","# forecast_input = prepare_forecast_input(df_test, time_steps, NUM_ITEMS)\n","\n","# Generate forecasts\n","# forecasts = model.predict(forecast_input)\n","# forecasts_original = scaler.inverse_transform(forecasts)\n","\n","# forecasts_original now contains the predicted sales amounts for days 1942 to 1969\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:26.857391Z","iopub.status.busy":"2024-01-18T14:34:26.857016Z","iopub.status.idle":"2024-01-18T14:34:27.273805Z","shell.execute_reply":"2024-01-18T14:34:27.272980Z","shell.execute_reply.started":"2024-01-18T14:34:26.857362Z"},"trusted":true},"outputs":[],"source":["# Prepare input for forecasts\n","# I cannot use the custom lstm_data_generator\n","# Prepare 7 day slices each shifted by one day\n","def prepare_forecast_input(df, time_steps, target_col):\n","    forecast_input = []\n","    for i in range(0, len(df)//NUM_ITEMS): #i=0; 1, 2, 3, ..., 35?\n","        if i + time_steps < (len(df)-1)//NUM_ITEMS: #7, 8, 9, 10, ...\n","            start_idx = i*NUM_ITEMS\n","            end_idx   = start_idx + NUM_ITEMS * time_steps\n","            sequence  = df.iloc[start_idx : end_idx, :].drop(target_col, axis=1).to_numpy()\n","            forecast_input.append(sequence)\n","    return np.array(forecast_input)\n","\n","predict_array = prepare_forecast_input(df=df_test, time_steps=time_steps, target_col=target_col)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:29.975040Z","iopub.status.busy":"2024-01-18T14:34:29.974179Z","iopub.status.idle":"2024-01-18T14:34:29.981976Z","shell.execute_reply":"2024-01-18T14:34:29.981017Z","shell.execute_reply.started":"2024-01-18T14:34:29.974993Z"},"trusted":true},"outputs":[],"source":["predict_array.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:35:58.035250Z","iopub.status.busy":"2024-01-18T14:35:58.034255Z","iopub.status.idle":"2024-01-18T14:36:03.405105Z","shell.execute_reply":"2024-01-18T14:36:03.404120Z","shell.execute_reply.started":"2024-01-18T14:35:58.035208Z"},"trusted":true},"outputs":[],"source":["forecast_normalized = model.predict(predict_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:36:45.205555Z","iopub.status.busy":"2024-01-18T14:36:45.205169Z","iopub.status.idle":"2024-01-18T14:36:45.210651Z","shell.execute_reply":"2024-01-18T14:36:45.209587Z","shell.execute_reply.started":"2024-01-18T14:36:45.205524Z"},"trusted":true},"outputs":[],"source":["forecasts_original = scaler.inverse_transform(forecast_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:37:28.194159Z","iopub.status.busy":"2024-01-18T14:37:28.193718Z","iopub.status.idle":"2024-01-18T14:37:28.200519Z","shell.execute_reply":"2024-01-18T14:37:28.199591Z","shell.execute_reply.started":"2024-01-18T14:37:28.194120Z"},"trusted":true},"outputs":[],"source":["forecasts_original.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, time_steps, n):\n","    test_gen = lstm_data_generator(df_test, target_col, time_steps, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","# evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, time_steps, VAL_END)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
