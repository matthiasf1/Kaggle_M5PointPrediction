{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.714019Z","iopub.status.busy":"2024-02-04T10:28:45.713557Z","iopub.status.idle":"2024-02-04T10:28:45.724837Z","shell.execute_reply":"2024-02-04T10:28:45.723860Z","shell.execute_reply.started":"2024-02-04T10:28:45.713975Z"},"trusted":true},"outputs":[],"source":["# Setting to adjust before each run:\n","MODEL_NAME = 'V3_ohne_Cat_features_block_items'\n","CODE_ENV = 'local' #'kaggle', 'aws', 'local'\n","STATUS = 'production' #'training'"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:45.726757Z","iopub.status.busy":"2024-02-04T10:28:45.726461Z","iopub.status.idle":"2024-02-04T10:28:49.601403Z","shell.execute_reply":"2024-02-04T10:28:49.600379Z","shell.execute_reply.started":"2024-02-04T10:28:45.726733Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from keras.models import Sequential, load_model, Model\n","from keras.layers import Input, LSTM, Dense, Embedding, Dropout, Reshape, concatenate, Flatten, Bidirectional, GlobalAveragePooling1D\n","from keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l2\n","# from keras.metrics import RootMeanSquaredError\n","from keras import backend as K\n","from keras.callbacks import Callback\n","import tensorflow as tf\n","from helper_functions import plot_history\n","from keras_self_attention import SeqSelfAttention"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.603096Z","iopub.status.busy":"2024-02-04T10:28:49.602532Z","iopub.status.idle":"2024-02-04T10:28:49.707155Z","shell.execute_reply":"2024-02-04T10:28:49.706050Z","shell.execute_reply.started":"2024-02-04T10:28:49.603066Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n","False\n"]}],"source":["# Check if GPU is available\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","print(tf.test.is_built_with_cuda())"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.710168Z","iopub.status.busy":"2024-02-04T10:28:49.709769Z","iopub.status.idle":"2024-02-04T10:28:49.717151Z","shell.execute_reply":"2024-02-04T10:28:49.716162Z","shell.execute_reply.started":"2024-02-04T10:28:49.710139Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","if CODE_ENV=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files\n","\n","if CODE_ENV=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data/'\n","    src_dir = '/kaggle/working/'\n","    sub_dir = src_dir + 'submissions/'\n","\n","if CODE_ENV=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","    sub_dir = src_dir + 'submissions/' # Directory to save submission files"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.718565Z","iopub.status.busy":"2024-02-04T10:28:49.718293Z","iopub.status.idle":"2024-02-04T10:28:49.727911Z","shell.execute_reply":"2024-02-04T10:28:49.727014Z","shell.execute_reply.started":"2024-02-04T10:28:49.718540Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","VALIDATION_DATA  = prc_dir +'df_1.pkl' # Validation data\n","BASE      = prc_dir +'df_2.pkl' # Base data\n","CALENDAR  = prc_dir +'df_3.pkl' # Calendar data\n","NUM_ITEMS = 30490 # Number of items per each day\n","DAYS_PER_SEQUENCE = 28  # Length of the sequence\n","MAX_BATCH_SIZE = 900 # Maximum number of ids to be used in each batch to avoid memory issues and curse of dimensionality\n","TARGET_COL = 'sales_amount'\n","# REPEATED_FEATURES = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","REPEATED_FEATURES = ['sales_amount', 'sell_price', 'is_available'] # List to hold all feature columns that are used for each item\n","# ONCE_ONLY_FEATURES = ['d', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'mday', 'week', 'month', 'year', 'snap_CA', 'snap_TX', 'snap_WI'] # List to hold feature columns that are not repeated for each item\n","ONCE_ONLY_FEATURES = ['snap_CA', 'snap_TX', 'snap_WI', 'mday_normalized', 'month_sin', 'month_cos', 'wday_sin', 'wday_cos', 'week_sin', 'week_cos', 'year_normalized'] # List to hold feature columns that are not repeated for each item\n","EVENT_COLS = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Set test_end to 1969 in case of production\n","if STATUS=='production':\n","    TEST_END = 1969\n","elif STATUS=='training':\n","    TEST_END = 1941\n","\n","# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END - DAYS_PER_SEQUENCE# Depends on whether the whole dataset is used or last the 28 days for validation "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:28:49.729350Z","iopub.status.busy":"2024-02-04T10:28:49.728998Z","iopub.status.idle":"2024-02-04T10:28:53.163392Z","shell.execute_reply":"2024-02-04T10:28:53.162542Z","shell.execute_reply.started":"2024-02-04T10:28:49.729323Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","def get_whole_data():\n","    df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)\n","    return df_all_data"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Return a df with all unique combinations of store_id and dept_id\n","def get_combinations(df_all_data):\n","    # get all store_id and dept_id combinations\n","    df_combinations_store_dep = df_all_data[['store_id','dept_id']].drop_duplicates().reset_index(drop=True)\n","\n","    return df_combinations_store_dep"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Filter df down to only the current store_id and dept_id combination\n","def filter_df(df_combinations_store_dep, df_all_data, i):\n","    store_id = df_combinations_store_dep.loc[i, 'store_id']\n","    dept_id = df_combinations_store_dep.loc[i, 'dept_id']\n","    ids = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)]['id'].drop_duplicates().values\n","    filtered_df = df_all_data[(df_all_data['store_id']==store_id) & (df_all_data['dept_id']==dept_id)].reset_index(drop=True)\n","    filtered_df.reset_index(drop=True, inplace=True) ##################################################????\n","\n","    # Calculate number of batches\n","    num_batches = int(np.ceil(len(ids)/MAX_BATCH_SIZE))\n","\n","    return filtered_df, ids, num_batches"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def calc_vocab_size(filtered_df, embedding_dims_max=50):\n","    vocab_size=[]\n","    embedding_dims=[]\n","    # count the unique entries of event_name_1 event_type_1 event_name_2 event_type_2\n","    # append the number of unique entries to the list vocab_size\n","    vocab_size.append(len(filtered_df['event_name_1'].unique()))\n","    vocab_size.append(len(filtered_df['event_type_1'].unique()))\n","    vocab_size.append(len(filtered_df['event_name_2'].unique()))\n","    vocab_size.append(len(filtered_df['event_type_2'].unique()))\n","    \n","    # loop over all other indices and calculate the embedding dimensions\n","    for i in range(0, len(vocab_size)):\n","        embedding_dims.append(int(embedding_dims_max * (vocab_size[i]/max(vocab_size))))\n","\n","    return vocab_size, embedding_dims"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def filtered_df_batches(filtered_df, ids, num_batches, counter):\n","    # get ids for the current batch\n","    start_idx = counter * MAX_BATCH_SIZE\n","    if counter < num_batches - 1:\n","        end_idx = (counter + 1) * MAX_BATCH_SIZE\n","        ids_batch = ids[start_idx:end_idx]\n","    else:\n","        ids_batch = ids[start_idx:]\n","\n","    # filter the df for the current batch\n","    filtered_df_batch = filtered_df[filtered_df['id'].isin(ids_batch)].reset_index(drop=True)\n","\n","    # Get the number of block items\n","    num_block_items = len(ids_batch)\n","\n","    # Get the number of features\n","    num_features = len(ONCE_ONLY_FEATURES) + len(REPEATED_FEATURES) * num_block_items # Calculate the number of features\n","\n","    # Get the input shape later on for the model\n","    input_shape = (DAYS_PER_SEQUENCE, num_features)\n","\n","    return filtered_df_batch, num_block_items, num_features, input_shape, ids_batch"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.293651Z","iopub.status.busy":"2024-02-04T10:29:02.293253Z","iopub.status.idle":"2024-02-04T10:29:02.323161Z","shell.execute_reply":"2024-02-04T10:29:02.322166Z","shell.execute_reply.started":"2024-02-04T10:29:02.293598Z"},"trusted":true},"outputs":[],"source":["# create a dataframe that stores only th 5 first items for each day\n","# indices = np.array([np.arange(start, start + num_block_items) for start in range(0, TEST_END * NUM_ITEMS, NUM_ITEMS)]).flatten()\n","# df_all_data = df_all_data.iloc[indices]\n","# df_all_data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Normalize numerical columns\n","def prepare_df(df_all_data):\n","    # Define categorical and numerical columns\n","    categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                        'd', 'snap_CA', 'snap_TX', 'snap_WI']\n","    numerical_cols = ['sell_price']\n","\n","    # Convert categorical columns to category dtype and encode with cat.codes\n","    for col in categorical_cols:\n","        df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","    # Adjust the event cols\n","    # 1. Create an encoder instance for each column\n","    encoders = {col: LabelEncoder() for col in EVENT_COLS}\n","\n","    # Apply encoding to each column\n","    for col, encoder in encoders.items():\n","        df_all_data[col] = encoder.fit_transform(df_all_data[col].astype(str))\n","\n","    # Normalize numerical columns\n","    scaler_numerical = MinMaxScaler()\n","    df_all_data[numerical_cols] = scaler_numerical.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","\n","    scaler_target = MinMaxScaler()\n","    df_all_data[TARGET_COL] = scaler_target.fit_transform(df_all_data[[TARGET_COL]].astype(np.float64))\n","\n","    return df_all_data, scaler_target"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.326509Z","iopub.status.busy":"2024-02-04T10:29:02.326211Z","iopub.status.idle":"2024-02-04T10:29:02.341825Z","shell.execute_reply":"2024-02-04T10:29:02.340817Z","shell.execute_reply.started":"2024-02-04T10:29:02.326485Z"},"trusted":true},"outputs":[],"source":["def train_test_split(df_all_data):\n","    # For training split up between train and validation dataset, else use all for training and create test dataset\n","    if STATUS=='training':\n","        df_train = df_all_data[df_all_data['d'] < TRAIN_END].reset_index(drop=True)\n","        df_val   = df_all_data[(df_all_data['d'] >= TRAIN_END - DAYS_PER_SEQUENCE) & (df_all_data['d'] < VAL_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_test  = None\n","    elif STATUS=='production':\n","        df_train = df_all_data[df_all_data['d'] < VAL_END].reset_index(drop=True)\n","        df_test  = df_all_data[(df_all_data['d'] >= VAL_END - DAYS_PER_SEQUENCE)   & (df_all_data['d'] < TEST_END)].reset_index(drop=True) #more than 28 days because of the time_steps shift\n","        df_val   = None\n","\n","    # Delete df_all_data to free up memory as data is now stored in df_train, df_val and df_test\n","    del df_all_data\n","\n","    return df_train, df_val, df_test"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["### Create x and y in one go without the generator version autogeneration ###\n","def create_x_y(df, num_block_items):\n","    length_days = len(df) // num_block_items\n","    x = []\n","    y = []\n","    events = []\n","\n","    for i in range(0, length_days - DAYS_PER_SEQUENCE):\n","        start_ind = i * num_block_items\n","        end_ind = start_ind + num_block_items * (DAYS_PER_SEQUENCE)  # predict the next day after the sequence\n","\n","        # Extract once-only features for all days in the sequence at once\n","        once_features = df.iloc[start_ind:end_ind:num_block_items][ONCE_ONLY_FEATURES].to_numpy()\n","\n","        # Get event columns\n","        event_features = df.iloc[start_ind:end_ind:num_block_items][EVENT_COLS].to_numpy()\n","\n","        # Extract repeated features for all items and days at once\n","        repeated_features_stack = df.iloc[start_ind:end_ind][REPEATED_FEATURES].to_numpy() # 210,000 items, 10 features\n","\n","        # Reshape to a 3D array: 7 days, 30,000 items per day, 10 features\n","        reshaped_3d = repeated_features_stack.reshape(DAYS_PER_SEQUENCE, num_block_items, len(REPEATED_FEATURES))\n","\n","        # Reshape to a 2D array: 7 days, 30,000 items * 10 features each\n","        final_array = reshaped_3d.reshape(DAYS_PER_SEQUENCE, -1)\n","\n","        # Combine once-only and repeated features\n","        batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","        # Extract targets\n","        batch_targets = df.iloc[end_ind:end_ind + num_block_items][[TARGET_COL]].to_numpy().flatten()\n","\n","        # Append to x, y and events\n","        x.append(batch_sequences)\n","        events.append(event_features)\n","        y.append(batch_targets)\n","\n","\n","    return np.array(x), np.array(events), np.array(y)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.343749Z","iopub.status.busy":"2024-02-04T10:29:02.343338Z","iopub.status.idle":"2024-02-04T10:29:02.353726Z","shell.execute_reply":"2024-02-04T10:29:02.352895Z","shell.execute_reply.started":"2024-02-04T10:29:02.343711Z"},"trusted":true},"outputs":[],"source":["### Use for batch generation input to model ###\n","def lstm_data_generator(df, num_block_items):\n","    length_days = len(df) // num_block_items  # 1941 days\n","    while True:\n","        for i in range(0, length_days - DAYS_PER_SEQUENCE):\n","            start_ind = i * num_block_items\n","            end_ind = start_ind + num_block_items * (DAYS_PER_SEQUENCE)  # predict the next day after the sequence\n","\n","            # Extract once-only features for all days in the sequence at once\n","            once_features = df.iloc[start_ind:end_ind:num_block_items][ONCE_ONLY_FEATURES].to_numpy() # 0,5,10,...295 --> len(once_features)=DAYS_PER_SEQUENCE (60); [3 cols]\n","            # once_features = np.tile(once_features, (NUM_ITEMS, 1, 1)).transpose(1, 0, 2)\n","\n","            # Extract repeated features for all items and days at once\n","            repeated_features_stack = df.iloc[start_ind:end_ind][REPEATED_FEATURES].to_numpy() # 0:300 --> len(repeated_features_stack)=300 ;[3 cols]\n","\n","            # Reshape to a 3D array: 60 days, 5 items ,3 repeated features\n","            reshaped_3d = repeated_features_stack.reshape(DAYS_PER_SEQUENCE, num_block_items, len(REPEATED_FEATURES))\n","\n","            # Reshape to a 2D array: 60 days,  5 items * 3 features each (15)\n","            final_array = reshaped_3d.reshape(DAYS_PER_SEQUENCE, -1)\n","\n","            # Combine once-only and repeated features\n","            batch_sequences = np.concatenate((once_features, final_array), axis=1)\n","\n","            # Reshape batch_sequences to match LSTM input shape\n","            batch_sequences = batch_sequences.reshape(1, DAYS_PER_SEQUENCE, -1)\n","\n","            # Extract targets\n","            batch_targets = df.iloc[end_ind:end_ind + num_block_items][[TARGET_COL]].to_numpy().flatten()\n","\n","            # Yield the batch\n","            yield batch_sequences, batch_targets"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.355635Z","iopub.status.busy":"2024-02-04T10:29:02.355210Z","iopub.status.idle":"2024-02-04T10:29:02.365871Z","shell.execute_reply":"2024-02-04T10:29:02.364973Z","shell.execute_reply.started":"2024-02-04T10:29:02.355590Z"},"trusted":true},"outputs":[],"source":["# Get the training data and labels array for the LSTM model\n","def get_x_and_y(df_train, df_val, df_test, num_block_items):\n","    # For generator use:\n","    # train_generator = lstm_data_generator(df_train)\n","    # val_generator = lstm_data_generator(df_val)\n","\n","    # For single batch input use:\n","    train_x, train_event_x, train_y = create_x_y(df_train, num_block_items)\n","\n","    if STATUS=='training':\n","        val_x, val_event_x, val_y = create_x_y(df_val, num_block_items)\n","        test_x, test_event_x, test_y = None, None, None\n","    elif STATUS=='production': \n","        test_x, test_event_x, test_y = create_x_y(df_test, num_block_items)\n","        val_x, val_event_x, val_y = None, None, None\n","\n","    # df_train not needed anymore\n","    del df_train\n","\n","    return train_x, train_event_x, train_y, val_x, val_event_x, val_y, test_x, test_event_x, test_y"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.386793Z","iopub.status.busy":"2024-02-04T10:29:02.386108Z","iopub.status.idle":"2024-02-04T10:29:02.392710Z","shell.execute_reply":"2024-02-04T10:29:02.391436Z","shell.execute_reply.started":"2024-02-04T10:29:02.386757Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:04.259644Z","iopub.status.busy":"2024-02-04T10:29:04.259265Z","iopub.status.idle":"2024-02-04T10:29:04.264825Z","shell.execute_reply":"2024-02-04T10:29:04.263758Z","shell.execute_reply.started":"2024-02-04T10:29:04.259591Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# When transfer learning is used, the model should be recompiled with a new, lower learning rate which this function does\n","def prepare_model_tl(model, new_learning_rate, frozen_layers):\n","    # Instantiate a new optimizer with the desired learning rate\n","    new_optimizer = Adam(learning_rate=new_learning_rate)\n","\n","    if frozen_layers:\n","        # Freeze the layers\n","        for layer in frozen_layers:\n","            model.layers[layer].trainable = False\n","\n","    # Recompile the model with the new optimizer\n","    model.compile(optimizer=new_optimizer, loss=loss, metrics=metrics)\n","\n","    return model"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:04.266821Z","iopub.status.busy":"2024-02-04T10:29:04.266222Z","iopub.status.idle":"2024-02-04T10:30:29.341563Z","shell.execute_reply":"2024-02-04T10:30:29.340754Z","shell.execute_reply.started":"2024-02-04T10:29:04.266784Z"},"trusted":true},"outputs":[],"source":["def model_training(model, _train_x, train_event_x, train_y, _val_x, val_event_x, val_y, epochs, use_embeddings_events):\n","    # Training the model in batches\n","    # history = model.fit(x=train_generator,\n","    #                      steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","    #                      validation_data=val_generator,\n","    #                      validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","    #                      epochs=epochs,\n","    #                      callbacks=[ResetStatesCallback()])\n","\n","    # Train in one go\n","    if STATUS=='training':\n","        if use_embeddings_events:\n","            train_x = [_train_x, train_event_x[:, 0], train_event_x[:, 1], train_event_x[:, 2], train_event_x[:, 3]]\n","            val_x = [_val_x, val_event_x[:, 0], val_event_x[:, 1], val_event_x[:, 2], val_event_x[:, 3]]\n","        else:\n","            train_x = _train_x\n","            val_x = _val_x\n","\n","        history = model.fit(x=train_x,  # Entire training dataset\n","                        y=train_y,  # Corresponding training labels\n","                        validation_data=(val_x, val_y),  # Entire validation dataset and labels\n","                        epochs=epochs)\n","        \n","    elif STATUS=='production':\n","        if use_embeddings_events:\n","            train_x = [_train_x, train_event_x[:,:,0], train_event_x[:,:,1], train_event_x[:,:,2], train_event_x[:,:,3]]\n","        else:\n","            train_x = _train_x\n","\n","        history = model.fit(x=train_x,  # Entire training dataset\n","                        y=train_y,  # Corresponding training labels\n","                        epochs=epochs)\n","        \n","    return model, history"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def eval(val_x, val_y, model, num_features, scaler_target):\n","    df_eval = pd.DataFrame(columns=['day', 'prediction', 'actual'])\n","    for i in range(0, len(val_x)):\n","        # create new dataframe with the current day, the actual value and the prediction\n","        df_temp = pd.DataFrame({'day': i, 'prediction': model.predict(val_x[i].reshape(1, DAYS_PER_SEQUENCE, num_features), verbose=0).flatten(), 'actual': val_y[i]})\n","        df_eval = pd.concat([df_eval, df_temp], axis=0, ignore_index=True)\n","        # new column with the difference between actual and prediction\n","        df_eval['difference'] = df_eval['actual'] - df_eval['prediction']\n","        # new columns with inverse transformation of actual and prediction\n","        df_eval['actual_inv'] = scaler_target.inverse_transform(df_eval[['actual']]).astype(int)\n","        df_eval['prediction_inv'] = scaler_target.inverse_transform(df_eval[['prediction']]).round(0).astype(int)\n","        # new columns with the difference between actual and prediction\n","        df_eval['difference_inv'] = df_eval['actual_inv'] - df_eval['prediction_inv']\n","    return df_eval"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:52:42.174980Z","iopub.status.busy":"2024-02-04T10:52:42.174264Z","iopub.status.idle":"2024-02-04T10:52:42.181344Z","shell.execute_reply":"2024-02-04T10:52:42.180389Z","shell.execute_reply.started":"2024-02-04T10:52:42.174943Z"},"trusted":true},"outputs":[],"source":["# Evaluation for generator batches\n","def test_eval(val_generator, model, scaler_target):\n","    x, y = next(val_generator)\n","    \n","    prediction_original = model.predict(x)\n","\n","    true_array = scaler_target.inverse_transform(y).flatten()\n","    predicted_array = scaler_target.inverse_transform(prediction_original)[0]\n","    \n","    d = {\"true_array\": true_array, \"predicted_array\": predicted_array}\n","    df = pd.DataFrame(d)\n","    df['predicted_array_rounded'] = df['predicted_array'].round().astype(int)\n","    df['Difference'] = df['true_array'] - df['predicted_array']\n","\n","    print(df)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["################################### Function to forecast the next 28 days (This function for case all data in one batch) ###################################\n","def rolling_forecast(model, df_test, df_val, test_x, test_event_x, test_y , val_x, val_y, val_event_x, scaler_target, num_features, num_block_items):\n","    # Set the df_copy, x_copy and y_copy to the correct dataset\n","    if STATUS=='production':\n","        df_copy = df_test.copy()\n","        x_copy = test_x.copy()\n","        y_copy = test_y.copy()\n","        events_copy = test_event_x.copy()    \n","    \n","    elif STATUS=='training':\n","        df_copy = df_val.copy()\n","        x_copy = val_x.copy()\n","        y_copy = val_y.copy()\n","        events_copy = val_event_x.copy()\n","\n","    # return x_copy, events_copy --> (28,28,1259); (28, 28, 4)\n","\n","    # Predict the next 28 days\n","    for i in range(TEST_DUR):\n","        prediction_normalized = model.predict([x_copy[i].reshape(1, DAYS_PER_SEQUENCE, num_features)] + [events_copy[i][:,j].reshape(1, DAYS_PER_SEQUENCE) for j in range(4)], verbose=0).flatten() \n","\n","        # Impractical to adjust the prepared array, so we will update the df_test copy and use it to create a new array with the updated prediction values\n","        start_idx = DAYS_PER_SEQUENCE*num_block_items+(i*num_block_items)\n","        end_idx = start_idx + num_block_items - 1\n","        df_copy.loc[start_idx:end_idx, TARGET_COL] = prediction_normalized\n","\n","        # Create new df for x and y\n","        x_copy, events_copy, _ = create_x_y(df_copy, num_block_items)\n","\n","        # Update the y array with the new prediction\n","        y_copy[i] = prediction_normalized\n","    \n","    # Inverse transform the predictions\n","    predictions_original = scaler_target.inverse_transform(y_copy).round(0).astype(int)\n","\n","    # Make sure no negative values are returned\n","    predictions_original[predictions_original < 0] = 0\n","        \n","    return predictions_original\n","#########################################################################################################"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Create a DataFrame for predictions\n","def prepare_fc_to_file(forecast_df, forecast_array, ids):\n","    # Transpose predictions to match the sample submission format\n","    forecast_array = forecast_array.T\n","\n","    # Create array to write to df\n","    forecast_array = np.concatenate((ids.reshape(len(ids),1), forecast_array), axis=1)\n","\n","    # Create a DataFrame for your predictions\n","    forecast_tmp_df = pd.DataFrame(forecast_array, columns=['id'] + [f'F{i+1}' for i in range(28)])\n","\n","    # concatenate forecast to forecast_df\n","    forecast_df = pd.concat([forecast_df, forecast_tmp_df], axis=0, ignore_index=True)\n","\n","    return forecast_df"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def write_to_csv(forecast_df, dir):\n","    # Get validation data\n","    val_df = pd.read_pickle(VALIDATION_DATA)\n","\n","    # Combine forecast with validation data\n","    forecast_df = pd.concat([val_df, forecast_df], axis=0, ignore_index=True)\n","\n","    # Save the forecast to a csv file\n","    forecast_df.to_csv(dir, index=False)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["# def tweedie_loss_func(p):\n","#     def tweedie_loglikelihood(y, y_hat):\n","#         loss = - y * tf.pow(y_hat, 1 - p) / (1 - p) + \\\n","#                tf.pow(y_hat, 2 - p) / (2 - p)\n","#         return tf.reduce_mean(loss)\n","#     return tweedie_loglikelihood\n","\n","\n","def tweedie_loss_func(p=1.5):\n","    def loss(y_true, y_pred):\n","        # Ensure predictions are strictly positive\n","        epsilon = 1e-10\n","        y_pred = tf.maximum(y_pred, epsilon)\n","\n","        # Tweedie loss calculation\n","        loss = -y_true * tf.pow(y_pred, 1-p) / (1-p) + \\\n","               tf.pow(y_pred, 2-p) / (2-p)\n","        return tf.reduce_mean(loss)\n","    return loss"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.401718Z","iopub.status.busy":"2024-02-04T10:29:02.401392Z","iopub.status.idle":"2024-02-04T10:29:02.411354Z","shell.execute_reply":"2024-02-04T10:29:02.410459Z","shell.execute_reply.started":"2024-02-04T10:29:02.401688Z"},"trusted":true},"outputs":[],"source":["# Model parameters\n","initial_epochs = 12\n","subsequent_epochs = 6\n","# batch_size = 1\n","initial_lr = 0.001\n","subsequent_lr = 0.005 # Reduce learning rate by factor of 10 for transfer learning\n","clipvalue = 0.5\n","\n","# Model compile parameters\n","loss = tweedie_loss_func(p=1.5)#rmse\n","initial_optimizer = Adam(learning_rate=initial_lr, clipvalue=clipvalue)\n","metrics = tf.keras.metrics.MeanAbsoluteError()"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# Use functional API to create a model\n","def create_lstm_model_embedding(numerical_input_shape, num_block_items, vocab_sizes, embedding_dims): \n","    numerical_input = Input(shape=numerical_input_shape, name='numerical_input')\n","    event_input = [Input(shape=(DAYS_PER_SEQUENCE,), name=f'event_input_{i}') for i in range(1, 5)]\n","\n","    cat_embeddings = [Embedding(input_dim=vocab_sizes[i], output_dim=embedding_dims[i], input_length=DAYS_PER_SEQUENCE)(event_input[i]) for i in range(0, 4)]\n","\n","    # Combine numerical input and embeddings\n","    combined_input = concatenate([numerical_input] + cat_embeddings)\n","\n","    # LSTM layer\n","    lstm_out = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1, kernel_regularizer=l2(0.01)))(combined_input)\n","    dropout = Dropout(0.2)(lstm_out)\n","    # lstm_out = Bidirectional(LSTM(units=14, recurrent_dropout=0.1, kernel_regularizer=l2(0.01)))(dropout)\n","    # dropout = Dropout(0.2)(lstm_out)\n","    attention_out = SeqSelfAttention(attention_activation='sigmoid')(dropout)\n","\n","    # Aggregate sequence information\n","    pooled_output = GlobalAveragePooling1D()(attention_out)\n","\n","    # Output layer\n","    output = Dense(num_block_items, kernel_regularizer=l2(0.01))(pooled_output)\n","\n","    # Create and compile the model\n","    model = Model(inputs=[numerical_input] + event_input, outputs=output)\n","\n","    model.compile(optimizer=initial_optimizer, loss=loss, metrics=metrics)\n","\n","    return model"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T10:29:02.412895Z","iopub.status.busy":"2024-02-04T10:29:02.412579Z","iopub.status.idle":"2024-02-04T10:29:04.224334Z","shell.execute_reply":"2024-02-04T10:29:04.223506Z","shell.execute_reply.started":"2024-02-04T10:29:02.412871Z"},"trusted":true},"outputs":[],"source":["# Neu: Architecture to setup when predicting single day steps ahead and not using the repeat vector\n","def create_lstm_model(input_shape, num_block_items):\n","   model = Sequential([\n","      LSTM(units=80, activation='tanh', return_sequences=True, recurrent_dropout=0.1, input_shape=input_shape),\n","      Dropout(0.3),\n","      LSTM(units=40, activation='tanh', return_sequences=False, recurrent_dropout=0.1),\n","      Dropout(0.3),\n","      # LSTM(units=40, activation='tanh', return_sequences=False, recurrent_dropout=0.1),\n","      # Dropout(0.1),\n","      Dense(units=num_block_items, activation='tanh'), # activation='relu', 'softmax; Final Dense layer for output\n","      Reshape((num_block_items, 1))]) # Reshape the output to be (number of items)\n","\n","   model.compile(optimizer=initial_optimizer, loss=loss, metrics=metrics)\n","\n","   # For tracking purposes: check the models parameters\n","   # model.summary()\n","\n","   return model"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# for each store_id and dept_id call get whole data, filter for store_id and dept_id\n","def lstm_pipeline(verbose, use_embeddings_events):\n","    df_all_data = get_whole_data()\n","\n","    # Get all store_id and dept_id combinations\n","    df_combinations_store_dep = get_combinations(df_all_data)\n","\n","    # Create empty dataframe to store the forecast\n","    forecast_df = pd.DataFrame(columns=['id'] + [f'F{i+1}' for i in range(28)])\n","\n","    # define the number of loops\n","    num_loop = 1 if verbose == 1 else len(df_combinations_store_dep)\n","\n","    # Loop over all store_id and dept_id combinations, create a model, train it, create the prediction and save it to a file\n","    for i in range(0, num_loop):\n","        print(f'Processing {i+1} of {len(df_combinations_store_dep)}: store_id {df_combinations_store_dep.loc[i, \"store_id\"]} and dept_id {df_combinations_store_dep.loc[i, \"dept_id\"]}')\n","        # Filter df down to only the current store_id and dept_id combination\n","        filtered_df, ids, num_batches = filter_df(df_combinations_store_dep, df_all_data, i)\n","\n","        # Calculate the vocab size for the embedding layers later when model is defined\n","        vocab_sizes, embedding_dims = calc_vocab_size(filtered_df) # Funktioniert nur, wenn num_batches 1 ist, sonst muss komplexere Berechnung innerhalb des loops erfolgen\n","\n","        # Loop over all batches\n","        for counter in range(num_batches):\n","            print(f'Processing batch {counter+1} of {num_batches}')\n","                \n","            # Create batches for the current store_id and dept_id combination to avoid memory issues and curse of dimensionality\n","            filtered_df_batch, num_block_items, num_features, input_shape, ids_batch = filtered_df_batches(filtered_df, ids, num_batches, counter)\n","\n","            # Prepare the data for training\n","            filtered_df_batch, scaler_target = prepare_df(filtered_df_batch)\n","\n","            # Split the data into train, validation and test set\n","            df_train, df_val, df_test = train_test_split(filtered_df_batch)\n","\n","            # Create training, validation and test data arrays from the dataframes\n","            train_x, train_event_x, train_y, val_x, val_event_x, val_y, test_x, test_event_x, test_y = get_x_and_y(df_train, df_val, df_test, num_block_items)\n","\n","            # If this is the first batch create the model, for subsequent batches retrain the current model with smaller learning rate\n","            if counter == 0 or counter == num_batches - 1:\n","                # Create the model\n","                model = create_lstm_model_embedding(input_shape, num_block_items, vocab_sizes, embedding_dims)\n","\n","                # model = create_lstm_model(input_shape, num_block_items)\n","                epochs = initial_epochs\n","            else:\n","                model = prepare_model_tl(model, subsequent_lr, [])\n","                epochs = subsequent_epochs\n","\n","            model_trained, history = model_training(model, train_x, train_event_x, train_y, \n","                                                    val_x, val_event_x, val_y, \n","                                                    epochs, use_embeddings_events)\n","\n","            # Testing the model\n","            if verbose == 1:\n","                # Call eval function to get the evaluation dataframe and some feeling for the results\n","                df_eval = eval(val_x, val_y, model_trained, num_features, scaler_target)\n","\n","                # Test output for generator\n","                # test_data = test_eval(val_generator, model_trained, scaler_target)\n","\n","            if verbose == 0:\n","                # Create the forecast\n","                # x, events = rolling_forecast(model_trained, df_test, df_val, test_x, test_event_x, test_y, val_x, val_y, val_event_x, scaler_target, num_features, num_block_items)\n","                predictions_original = rolling_forecast(model_trained, df_test, df_val, test_x, test_event_x, test_y, val_x, val_y, val_event_x, scaler_target, num_features, num_block_items)\n","\n","                forecast_df = prepare_fc_to_file(forecast_df, predictions_original, ids_batch)\n","            print(\"####################################################\\n\")\n","\n","    if verbose == 0:\n","        write_to_csv(forecast_df, sub_dir + 'sample_submission.csv')\n","        return forecast_df\n","    \n","    if verbose == 1:\n","        return df_eval"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing 1 of 70: store_id CA_1 and dept_id HOBBIES_1\n","Processing batch 1 of 1\n"]},{"name":"stderr","output_type":"stream","text":["/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/12\n","60/60 [==============================] - 13s 112ms/step - loss: 6.7363 - mean_absolute_error: 0.2561\n","Epoch 2/12\n","60/60 [==============================] - 7s 116ms/step - loss: 4.6561 - mean_absolute_error: 0.1203\n","Epoch 3/12\n","60/60 [==============================] - 7s 117ms/step - loss: 4.6525 - mean_absolute_error: 0.0841\n","Epoch 4/12\n","60/60 [==============================] - 7s 119ms/step - loss: 4.4157 - mean_absolute_error: 0.0858\n","Epoch 5/12\n","60/60 [==============================] - 7s 114ms/step - loss: 4.6942 - mean_absolute_error: 0.0790\n","Epoch 6/12\n","60/60 [==============================] - 7s 114ms/step - loss: 4.6667 - mean_absolute_error: 0.0706\n","Epoch 7/12\n","60/60 [==============================] - 7s 113ms/step - loss: 4.6222 - mean_absolute_error: 0.0690\n","Epoch 8/12\n","60/60 [==============================] - 7s 118ms/step - loss: 4.3969 - mean_absolute_error: 0.0638\n","Epoch 9/12\n","60/60 [==============================] - 7s 113ms/step - loss: 4.4219 - mean_absolute_error: 0.0626\n","Epoch 10/12\n","60/60 [==============================] - 7s 116ms/step - loss: 4.3597 - mean_absolute_error: 0.0636\n","Epoch 11/12\n","60/60 [==============================] - 7s 115ms/step - loss: 4.2777 - mean_absolute_error: 0.0582\n","Epoch 12/12\n","60/60 [==============================] - 7s 119ms/step - loss: 4.2159 - mean_absolute_error: 0.0583\n","####################################################\n","\n","Processing 2 of 70: store_id CA_1 and dept_id HOBBIES_2\n","Processing batch 1 of 1\n","Epoch 1/12\n"]},{"name":"stderr","output_type":"stream","text":["/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["60/60 [==============================] - 7s 66ms/step - loss: 7.7319 - mean_absolute_error: 0.3699\n","Epoch 2/12\n","60/60 [==============================] - 4s 73ms/step - loss: 6.1290 - mean_absolute_error: 0.2830\n","Epoch 3/12\n","60/60 [==============================] - 5s 81ms/step - loss: 5.7843 - mean_absolute_error: 0.1279\n","Epoch 4/12\n","60/60 [==============================] - 5s 91ms/step - loss: 5.7506 - mean_absolute_error: 0.0879\n","Epoch 5/12\n","60/60 [==============================] - 6s 94ms/step - loss: 5.6921 - mean_absolute_error: 0.0801\n","Epoch 6/12\n","60/60 [==============================] - 5s 83ms/step - loss: 5.6296 - mean_absolute_error: 0.0698\n","Epoch 7/12\n","60/60 [==============================] - 5s 89ms/step - loss: 5.5953 - mean_absolute_error: 0.0679\n","Epoch 8/12\n","60/60 [==============================] - 5s 81ms/step - loss: 5.6031 - mean_absolute_error: 0.0676\n","Epoch 9/12\n","60/60 [==============================] - 5s 83ms/step - loss: 5.5761 - mean_absolute_error: 0.0688\n","Epoch 10/12\n","60/60 [==============================] - 5s 84ms/step - loss: 5.5406 - mean_absolute_error: 0.0621\n","Epoch 11/12\n","60/60 [==============================] - 5s 88ms/step - loss: 5.5273 - mean_absolute_error: 0.0619\n","Epoch 12/12\n","60/60 [==============================] - 7s 112ms/step - loss: 5.5196 - mean_absolute_error: 0.0619\n","####################################################\n","\n","Processing 3 of 70: store_id CA_1 and dept_id HOUSEHOLD_1\n","Processing batch 1 of 1\n"]},{"name":"stderr","output_type":"stream","text":["/Users/mf/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/12\n","60/60 [==============================] - 10s 111ms/step - loss: 9.0615 - mean_absolute_error: 0.1916\n","Epoch 2/12\n","60/60 [==============================] - 9s 154ms/step - loss: 6.5253 - mean_absolute_error: 0.1504\n","Epoch 3/12\n","60/60 [==============================] - 9s 147ms/step - loss: 6.2270 - mean_absolute_error: 0.1038\n","Epoch 4/12\n","60/60 [==============================] - 9s 144ms/step - loss: 6.4263 - mean_absolute_error: 0.1008\n","Epoch 5/12\n","60/60 [==============================] - 8s 129ms/step - loss: 6.2403 - mean_absolute_error: 0.0993\n","Epoch 6/12\n","60/60 [==============================] - 9s 146ms/step - loss: 6.0623 - mean_absolute_error: 0.0932\n","Epoch 7/12\n","60/60 [==============================] - 9s 158ms/step - loss: 5.9625 - mean_absolute_error: 0.0804\n","Epoch 8/12\n","60/60 [==============================] - 8s 133ms/step - loss: 6.0409 - mean_absolute_error: 0.0822\n","Epoch 9/12\n","60/60 [==============================] - 8s 134ms/step - loss: 6.0680 - mean_absolute_error: 0.0828\n","Epoch 10/12\n","42/60 [====================>.........] - ETA: 2s - loss: 5.9900 - mean_absolute_error: 0.0850"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# x, events = lstm_pipeline(verbose=0, use_embeddings_events=True)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m forecast_df \u001b[38;5;241m=\u001b[39m lstm_pipeline(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, use_embeddings_events\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[31], line 50\u001b[0m, in \u001b[0;36mlstm_pipeline\u001b[0;34m(verbose, use_embeddings_events)\u001b[0m\n\u001b[1;32m     47\u001b[0m     model \u001b[38;5;241m=\u001b[39m prepare_model_tl(model, subsequent_lr, [])\n\u001b[1;32m     48\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m subsequent_epochs\n\u001b[0;32m---> 50\u001b[0m model_trained, history \u001b[38;5;241m=\u001b[39m model_training(model, train_x, train_event_x, train_y, \n\u001b[1;32m     51\u001b[0m                                         val_x, val_event_x, val_y, \n\u001b[1;32m     52\u001b[0m                                         epochs, use_embeddings_events)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Testing the model\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Call eval function to get the evaluation dataframe and some feeling for the results\u001b[39;00m\n","Cell \u001b[0;32mIn[22], line 30\u001b[0m, in \u001b[0;36mmodel_training\u001b[0;34m(model, _train_x, train_event_x, train_y, _val_x, val_event_x, val_y, epochs, use_embeddings_events)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         train_x \u001b[38;5;241m=\u001b[39m _train_x\n\u001b[0;32m---> 30\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mtrain_x,  \u001b[38;5;66;03m# Entire training dataset\u001b[39;00m\n\u001b[1;32m     31\u001b[0m                     y\u001b[38;5;241m=\u001b[39mtrain_y,  \u001b[38;5;66;03m# Corresponding training labels\u001b[39;00m\n\u001b[1;32m     32\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m~/opt/anaconda3/envs/final_bsc/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# x, events = lstm_pipeline(verbose=0, use_embeddings_events=True)\n","forecast_df = lstm_pipeline(verbose=0, use_embeddings_events=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# x[0].shape\n","# events.shape\n","\n","# [x[0].reshape(1, DAYS_PER_SEQUENCE, 1259)]+[events[0][:,j] for j in range(4)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test output\n","forecast_df.head(30)\n","# every 5h row\n","# forecast_df.iloc[4::16,]\n","#how many rows with day = 0"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
