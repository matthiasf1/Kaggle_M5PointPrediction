{"cells":[{"cell_type":"markdown","metadata":{},"source":["- TPU nutzen und direkt aufrufen\n","- mutiprocessing\n","- gpu nutzen (CUDA aufrufen)\n","- Wie zum laufen bekommen?:\n","-   Encoder-Decoder Model --> https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243\n","-   https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/144243 --> 30490 als batch input nutzen, dann aber Problem, dass scheinbar nur Abhängigkeiten von einem auf den anderen Tag getrackt werden und keine Muster zwischen Zeitsequenzen gefunden werden können\n","\n","\n","- Progress bars mit tqdm anzeigen\n","- Test, Validierung und Trainingzeitraum sollten sich nicht überlappen, ist aber ggf. der Fall?\n","- Ggf. zu float16 konvertieren checken, ob finaler df mit time slices dann deutlich kleiner und performance testen\n","- column 'd' in training df löschen?\n","- paralletl computing einstellen\n","- use_multiprocessing in keras auf true setzen (model.fit agument)\n","- Cross validation?\n","- Ensemble learning?\n","- brauche ich one-hot encoding für categorical features?"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:33.992771Z","iopub.status.busy":"2024-01-22T13:02:33.992087Z","iopub.status.idle":"2024-01-22T13:02:34.371455Z","shell.execute_reply":"2024-01-22T13:02:34.370396Z","shell.execute_reply.started":"2024-01-22T13:02:33.992733Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n"]}],"source":["# Check if GPU is available\n","pip install --upgrade pip\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","#print(tf.test.is_built_with_cuda())\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.373809Z","iopub.status.busy":"2024-01-22T13:02:34.373363Z","iopub.status.idle":"2024-01-22T13:02:34.379504Z","shell.execute_reply":"2024-01-22T13:02:34.378489Z","shell.execute_reply.started":"2024-01-22T13:02:34.373780Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","#code_env = 'kaggle'\n","#code_env = 'local'\n","code_env = 'aws'\n","\n","\n","if code_env=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data-v3/'\n","\n","if code_env=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.381096Z","iopub.status.busy":"2024-01-22T13:02:34.380802Z","iopub.status.idle":"2024-01-22T13:02:34.390571Z","shell.execute_reply":"2024-01-22T13:02:34.389570Z","shell.execute_reply.started":"2024-01-22T13:02:34.381070Z"},"trusted":true},"outputs":[],"source":["# Create variables\n","BASE      = prc_dir +'df_1.pkl'\n","CALENDAR  = prc_dir +'df_2.pkl'\n","NUM_ITEMS = 30490 # Number of items per each day"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:34.393140Z","iopub.status.busy":"2024-01-22T13:02:34.392833Z","iopub.status.idle":"2024-01-22T13:02:47.056364Z","shell.execute_reply":"2024-01-22T13:02:47.055528Z","shell.execute_reply.started":"2024-01-22T13:02:34.393114Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-25 13:20:03.890526: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Input, LSTM, Dense, Masking\n","from keras.optimizers import Adam\n","from keras.metrics import RootMeanSquaredError\n","from keras import backend as K\n","from keras.callbacks import Callback\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:02:47.058053Z","iopub.status.busy":"2024-01-22T13:02:47.057502Z","iopub.status.idle":"2024-01-22T13:03:01.455370Z","shell.execute_reply":"2024-01-22T13:03:01.454363Z","shell.execute_reply.started":"2024-01-22T13:02:47.058028Z"},"trusted":true},"outputs":[],"source":["# Read in df_train_conv from pickle file\n","df_all_data = pd.concat([pd.read_pickle(BASE),\n","           pd.read_pickle(CALENDAR)], \n","           axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:01.457014Z","iopub.status.busy":"2024-01-22T13:03:01.456690Z","iopub.status.idle":"2024-01-22T13:03:09.962427Z","shell.execute_reply":"2024-01-22T13:03:09.961481Z","shell.execute_reply.started":"2024-01-22T13:03:01.456986Z"},"trusted":true},"outputs":[],"source":["# Define categorical and numerical columns\n","categorical_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'is_available',\n","                    'd', 'wday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n","                    'snap_CA', 'snap_TX', 'snap_WI', 'mday', 'week', 'month', 'year']\n","numerical_cols = ['sell_price']\n","\n","target_col = 'sales_amount'\n","\n","# Convert categorical columns to category dtype and encode with cat.codes\n","for col in categorical_cols:\n","    df_all_data[col] = df_all_data[col].astype('category').cat.codes\n","\n","# Normalize numerical columns\n","scaler = MinMaxScaler()\n","df_all_data[numerical_cols] = scaler.fit_transform(df_all_data[numerical_cols].astype(np.float32))\n","df_all_data[target_col] = scaler.fit_transform(df_all_data[[target_col]].astype(np.float32))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:09.963987Z","iopub.status.busy":"2024-01-22T13:03:09.963662Z","iopub.status.idle":"2024-01-22T13:03:09.968786Z","shell.execute_reply":"2024-01-22T13:03:09.967676Z","shell.execute_reply.started":"2024-01-22T13:03:09.963958Z"},"trusted":true},"outputs":[],"source":["# Set time_steps for defining test, train and validation sets\n","time_steps = 7  # Number of days per sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:09.970478Z","iopub.status.busy":"2024-01-22T13:03:09.970194Z","iopub.status.idle":"2024-01-22T13:03:18.370792Z","shell.execute_reply":"2024-01-22T13:03:18.369916Z","shell.execute_reply.started":"2024-01-22T13:03:09.970453Z"},"trusted":true},"outputs":[],"source":["# Splitting the data in train, validation and test set; days are now 0 based, so have to shift by 1\n","# Define duration in days of each set\n","VAL_DUR   = 28\n","TEST_DUR  = 28\n","\n","# Define end days of training set for each set\n","TEST_END  = 1941 #1969 \n","VAL_END   = TEST_END - TEST_DUR\n","TRAIN_END = VAL_END - VAL_DUR # 1885 -> Train only until the 28 days before the end of the data\n","\n","# Finally define duration in days for the train set\n","TRAIN_DUR = TRAIN_END # Depends on whether the whole dataset is used or last the 28 days for validation \n","\n","df_train = df_all_data[df_all_data['d'] < TRAIN_END].reset_index(drop=True)\n","df_val   = df_all_data[(df_all_data['d'] >= TRAIN_END - time_steps) & (df_all_data['d'] < VAL_END)].reset_index(drop=True) #35 days because of the time_steps shift\n","df_test  = df_all_data[df_all_data['d'] >= VAL_END - time_steps].reset_index(drop=True) #35 days because of the time_steps shift\n","\n","del df_all_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.372402Z","iopub.status.busy":"2024-01-22T13:03:18.372106Z","iopub.status.idle":"2024-01-22T13:03:18.386039Z","shell.execute_reply":"2024-01-22T13:03:18.385156Z","shell.execute_reply.started":"2024-01-22T13:03:18.372378Z"},"trusted":true},"outputs":[],"source":["# Version 1: \n","# x input: 7 days without sales_amount\n","# y labels: only 8th day sales_amount\n","# Custom Generator Function\n","# def lstm_data_generator(df, target, days_per_sequence=7, batch_size=32):\n","#     total_sequences = (len(df) - NUM_ITEMS * days_per_sequence) // NUM_ITEMS # 1878 for train, 21 for val and test; (1941*30490-7*30490)\n","#     while True: \n","#         for i in range(0, total_sequences, batch_size): # 0, 32, 64, ...1878\n","#             batch_sequences = []\n","#             batch_targets = []\n","#             for b in range(batch_size): # 0, 1, 2,... 31\n","#                 if i + b < total_sequences: # 0, 0; 0, 1; 0, 2; ...; 0, 32; 32, 0; 32, 1; ...\n","#                     start_idx = (i + b) * NUM_ITEMS\n","#                     end_idx = start_idx + NUM_ITEMS * days_per_sequence\n","#                     batch_sequences.append(df.iloc[start_idx:end_idx, :].drop(target, axis=1).to_numpy()) #drop target column, Only the values in the DataFrame will be returned, the axes labels will be removed.\n","#                     batch_targets.append(df.iloc[end_idx:end_idx + NUM_ITEMS][target].to_numpy())\n","#             yield np.array(batch_sequences), np.array(batch_targets)\n","\n","\n","\n","# To-Do\n","# -Für bisherigen Generator: prediction erstellen und bei Kaggle einreichen. Völlig egal ob shape passt, hauptsache shape beim training identisch\n","#    - Jeweils für 7 Tage x Werte und 8. Tag y labels\n","#    - 8. Tag X Werte und 8 Tag y labels\n","# -Testen, ob ich generator bauen kann, der 30,490 als sequenz ausgibt (30490,7,20) als input shape ausgibt und total_sequences kann glaube sogar bleiben\n","\n","# Version 2:\n","# 30490 batch size\n","# x input: 7 days without sales_amount\n","# y labels: only 8th day sales_amount\n","# Custom Generator Function\n","\n","def lstm_data_generator(df, target, days_per_sequence=7, batch_size=30490):\n","    while True:\n","        length_days = len(df) // NUM_ITEMS\n","        for i in range(length_days-time_steps): # 0 - 1877; 0-27\n","            # Initialize arrays for storing sequences and targets\n","            batch_sequences = np.zeros((batch_size, days_per_sequence, df.shape[1] - 1))  # minus 1 for target column; 30490, 7, 20\n","            batch_targets = np.zeros((batch_size, ))\n","\n","            # Loop over all items for the current day\n","            for item_idx in range(batch_size): #(0, 30489)\n","                start_idx = item_idx + (i * NUM_ITEMS) # 0+0*30490; 1+0*30490;...\n","                end_idx = start_idx + (days_per_sequence * NUM_ITEMS)\n","\n","                # Extract sequence for current item\n","                sequence = df.iloc[start_idx:end_idx:NUM_ITEMS].drop(target, axis=1).to_numpy()\n","                batch_sequences[item_idx, :, :] = sequence\n","\n","                # Extract target for current item\n","                target_value = df.iloc[end_idx + NUM_ITEMS][target]\n","                batch_targets[item_idx] = target_value\n","\n","            yield batch_sequences, batch_targets\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# import numpy as np\n","# import tensorflow as tf\n","\n","# # Assuming 'dataframe' is your DataFrame\n","\n","# # Function to create sequences\n","# def create_sequences(dataframe, window_size=7):\n","#     X, Y = [], []\n","#     for i in range(len(dataframe) - window_size):\n","#         # Extract 7 days of data with all 20 features\n","#         x_sequence = dataframe.iloc[i:i+window_size, :].values\n","\n","#         # Extract the 19 known features for the 8th day\n","#         x_8th_day_known = dataframe.iloc[i + window_size, :-1].values.reshape(1, -1)  # Excluding sales amount\n","\n","#         # Add the new binary feature indicating the prediction day\n","#         prediction_day_indicator = np.zeros((window_size, 1))\n","#         prediction_day_indicator_8th_day = np.array([[1]])  # 1 for the 8th day\n","#         x_sequence = np.hstack((x_sequence, prediction_day_indicator))\n","#         x_8th_day_with_indicator = np.hstack((x_8th_day_known, prediction_day_indicator_8th_day))\n","\n","#         # Concatenate 7 days of data with the known features of the 8th day\n","#         x_sequence_with_8th_day = np.concatenate([x_sequence, x_8th_day_with_indicator], axis=0)\n","\n","#         # The target is the sales amount for the 8th day\n","#         y_value = dataframe.iloc[i + window_size, -1]  # Sales amount for the 8th day\n","\n","#         X.append(x_sequence_with_8th_day)\n","#         Y.append(y_value)\n","#     return np.array(X), np.array(Y)\n","\n","# # Create sequences\n","# X, Y = create_sequences(dataframe)\n","\n","# # Convert to tf.data.Dataset for training\n","# dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n","# dataset = dataset.batch(batch_size)  # Define your batch_size\n","\n","\n","\n","# # Define the LSTM model\n","# model = tf.keras.models.Sequential([\n","#     # LSTM layer - adjust the number of units and input shape as needed\n","#     tf.keras.layers.LSTM(units=50, return_sequences=True, input_shape=(8, 21)),  # 8 time steps, 21 features (20 original + 1 indicator)\n","#     tf.keras.layers.LSTM(units=50),\n","#     # You can add more layers if needed\n","#     tf.keras.layers.Dense(units=1)  # Output layer - predicting sales amount\n","# ])\n","\n","# model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# # Assuming 'dataset' is the tf.data.Dataset we prepared earlier\n","# # and 'batch_size' is defined\n","\n","# # Fit the model\n","# history = model.fit(dataset, epochs=10, batch_size=batch_size)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# def lstm_data_generator(df, target, days_per_sequence=7, batch_size=32):\n","#     total_sequences = (len(df) - NUM_ITEMS * (days_per_sequence + 1)) // NUM_ITEMS\n","#     while True:\n","#         for i in range(0, total_sequences, batch_size):\n","#             batch_sequences = []\n","#             batch_targets = []\n","\n","#             for b in range(batch_size):\n","#                 if i + b < total_sequences:\n","#                     start_idx = (i + b) * NUM_ITEMS\n","#                     end_idx = start_idx + NUM_ITEMS * days_per_sequence\n","#                     next_day_idx = end_idx + NUM_ITEMS\n","\n","#                     # Sequence data with target for past time_step days\n","#                     sequence_data = df.iloc[start_idx:end_idx, :].copy()\n","                    \n","#                     # Adding is_current_day feature to let the model distinguish between the past data and current to predicting day\n","#                     # create a new column 'is_current_day' and fill it with 0s and set datatype to int8\n","#                     sequence_data['is_current_day'] = 0\n","#                     sequence_data['is_current_day'] = sequence_data['is_current_day'].astype(np.int8)\n","\n","#                     # Data for the current to predicting day without target, because in real life we don't have it\n","#                     sixth_day_data = df.iloc[end_idx:next_day_idx, :].copy()\n","#                     #fill column 'sales_amount' with NaNs\n","#                     sixth_day_data[target] = np.nan\n","#                     # Give model info that this is the current day\n","#                     sixth_day_data['is_current_day'] = 1\n","#                     sixth_day_data['is_current_day'] = sixth_day_data['is_current_day'].astype(np.int8)\n","\n","#                     # Combine data\n","#                     sequence_with_sixth_day = pd.concat([sequence_data, sixth_day_data], axis=0)\n","\n","#                     # Append to batch\n","#                     batch_sequences.append(sequence_with_sixth_day.to_numpy())\n","\n","#                     # Target for the 6th day\n","#                     batch_targets.append(df.iloc[end_idx:next_day_idx, :][target].to_numpy())\n","\n","#             yield np.array(batch_sequences), np.array(batch_targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.389636Z","iopub.status.busy":"2024-01-22T13:03:18.389317Z","iopub.status.idle":"2024-01-22T13:03:18.416778Z","shell.execute_reply":"2024-01-22T13:03:18.415805Z","shell.execute_reply.started":"2024-01-22T13:03:18.389597Z"},"trusted":true},"outputs":[],"source":["# Usage\n","batch_size = 30490  # Size of each batch\n","epochs = 2\n","num_cols = df_train.shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.418400Z","iopub.status.busy":"2024-01-22T13:03:18.418117Z","iopub.status.idle":"2024-01-22T13:03:18.427041Z","shell.execute_reply":"2024-01-22T13:03:18.426240Z","shell.execute_reply.started":"2024-01-22T13:03:18.418376Z"},"trusted":true},"outputs":[],"source":["# Train and validation generators\n","train_generator = lstm_data_generator(df_train, target_col, time_steps, batch_size)\n","val_generator = lstm_data_generator(df_val, target_col, time_steps, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.428419Z","iopub.status.busy":"2024-01-22T13:03:18.428151Z","iopub.status.idle":"2024-01-22T13:03:18.437168Z","shell.execute_reply":"2024-01-22T13:03:18.436284Z","shell.execute_reply.started":"2024-01-22T13:03:18.428395Z"},"trusted":true},"outputs":[],"source":["# Custom RMSE loss function\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:18.438535Z","iopub.status.busy":"2024-01-22T13:03:18.438246Z","iopub.status.idle":"2024-01-22T13:03:19.715259Z","shell.execute_reply":"2024-01-22T13:03:19.714216Z","shell.execute_reply.started":"2024-01-22T13:03:18.438511Z"},"trusted":true},"outputs":[],"source":["model = Sequential()\n","model.add(LSTM(units=50, stateful=True, batch_input_shape=(NUM_ITEMS, time_steps, num_cols-1)))\n","model.add(Dense(units=1)) \n","\n","model.compile(optimizer='adam', loss='mean_squared_error')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T13:03:19.716971Z","iopub.status.busy":"2024-01-22T13:03:19.716580Z","iopub.status.idle":"2024-01-22T13:03:19.722552Z","shell.execute_reply":"2024-01-22T13:03:19.721412Z","shell.execute_reply.started":"2024-01-22T13:03:19.716930Z"},"trusted":true},"outputs":[],"source":["class ResetStatesCallback(Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.reset_states()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-22T21:41:47.956270Z","iopub.status.busy":"2024-01-22T21:41:47.955997Z","iopub.status.idle":"2024-01-22T21:41:48.300127Z","shell.execute_reply":"2024-01-22T21:41:48.298879Z","shell.execute_reply.started":"2024-01-22T21:41:47.956244Z"},"trusted":true},"outputs":[],"source":["# Training the model\n","model.fit(x=train_generator,\n","          steps_per_epoch=TRAIN_DUR,  # total number of sequences in the training set\n","          validation_data=val_generator,\n","          validation_steps=VAL_DUR,  # total number of sequences in the validation set\n","          epochs=epochs,\n","          callbacks=[ResetStatesCallback()])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T13:54:21.140090Z","iopub.status.busy":"2024-01-18T13:54:21.139706Z","iopub.status.idle":"2024-01-18T13:54:21.163886Z","shell.execute_reply":"2024-01-18T13:54:21.163012Z","shell.execute_reply.started":"2024-01-18T13:54:21.140060Z"},"trusted":true},"outputs":[],"source":["# The input to every LSTM layer must be three-dimensional.\n","# The three dimensions of this input are:\n","#   Samples. One sequence is one sample. A batch is comprised of one or more samples.\n","#   Time Steps. One time step is one point of observation in the sample.\n","#   Features. One feature is one observation at a time step.\n","\n","\n","\n","# number_samples = 7 (7 Zeitschritte bzw. batch von 7)\n","# n_steps = 3\n","# number of parallel time series = parallel series = number of variables = features\n","# number_features = 2\n","\n","\n","\n","\n","\n","#input_shape = (time_steps + 1, len(df_train.columns) + 1) input shape for training data from predicting day\n","input_shape = (time_steps, len(df_train.columns) - 1)\n","model = Sequential([\n","    # Masking(mask_value=np.nan, input_shape=input_shape),\n","    # LSTM(50, activation='tanh'),\n","    LSTM(50, activation='tanh'),#, input_shape=(input_shape)), #return_state=False, return_sequences=False\n","    Dense(1)\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', \n","              loss='mse', #rmse\n","              metrics=[RootMeanSquaredError()])\n","\n","# Add CNN layer\n","# model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(time_steps, num_features)))\n","# model.add(MaxPooling1D(pool_size=2))\n","# model.add(Flatten())\n","# model.add(LSTM(50, activation='relu'))\n","# model.add(Dense(1))  # or more layers as needed\n","# model.compile()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#model.summary()\n","\n","# Print input shape of the layers\n","for layer in model.layers:\n","    print(layer.input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# df head with all columns displayed\n","#pd.set_option('display.max_columns', None)\n","#df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T13:54:25.578448Z","iopub.status.busy":"2024-01-18T13:54:25.578095Z","iopub.status.idle":"2024-01-18T14:29:55.381231Z","shell.execute_reply":"2024-01-18T14:29:55.380119Z","shell.execute_reply.started":"2024-01-18T13:54:25.578420Z"},"trusted":true},"outputs":[],"source":["# Train the model using the generator\n","# model.fit(x=train_gen, steps_per_epoch=(len(df_train) // (batch_size * NUM_ITEMS)), epochs=3) # x: In case of a generator the target y will be obtained from x; steps_per_epoch: 57mio // (32*30490) = 58\n","\n","# Fit the model and store history for later evaluation\n","history = model.fit(\n","    x=train_gen,\n","    epochs=epochs,\n","    steps_per_epoch=(len(df_train) // (batch_size * NUM_ITEMS)),\n","    validation_data=val_gen,\n","    validation_steps=(len(df_train) // (batch_size * NUM_ITEMS))\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train and validation df not needed anymore\n","del df_train\n","del df_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:30:39.354772Z","iopub.status.busy":"2024-01-18T14:30:39.353690Z","iopub.status.idle":"2024-01-18T14:30:39.384792Z","shell.execute_reply":"2024-01-18T14:30:39.383711Z","shell.execute_reply.started":"2024-01-18T14:30:39.354715Z"},"trusted":true},"outputs":[],"source":["# Save the model to a specified directory\n","if code_env=='local':\n","    ###local###\n","    model.save('/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction/src/models/V1.h5')\n","    \n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    model.save('/kaggle/working/V1_without_input_shape-model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Start from here if you want to load the model\n","from keras.models import load_model\n","\n","# Load the model from a specified directory\n","if code_env=='local':\n","    ###local###\n","    model = load_model('/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction/src/models/V1.h5', custom_objects={'rmse': rmse})\n","\n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    model = load_model('/kaggle/input/v1-model/V1.h5', custom_objects={'rmse': rmse})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:00.674762Z","iopub.status.busy":"2024-01-18T14:34:00.673913Z","iopub.status.idle":"2024-01-18T14:34:00.949572Z","shell.execute_reply":"2024-01-18T14:34:00.948466Z","shell.execute_reply.started":"2024-01-18T14:34:00.674717Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","try:\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","except:\n","    print('No history to plot')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def prepare_forecast_input(df, time_steps, num_items):\n","#     #df_test starts at 1942-7 which we need take into account\n","#     # Prepare input data for forecasting\n","#     forecast_input = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + time_steps * num_items\n","#         sequence = df.iloc[start_idx:end_idx].drop('sales_amount', axis=1).to_numpy()\n","#         forecast_input.append(sequence)\n","#     return np.array(forecast_input)\n","\n","\n","# Custom function for input to prepare forecasts input for model\n","# def prepare_forecast_input(df, target, model, time_steps, num_items):\n","#     forecast_output = []\n","#     for target_day in range(28):\n","#         start_idx = target_day * num_items\n","#         end_idx = start_idx + time_steps * num_items\n","#         sequence = df.iloc[start_idx:end_idx, : ].drop(target, axis=1).to_numpy()\n","#         # forecast_output.append(model.predict(sequence))\n","#         forecast_output.append(model.predict(sequence.reshape(1, sequence.shape[0], sequence.shape[1])))\n","#     return np.array(forecast_output)#.reshape(-1, 1)\n","# forecast_output = prepare_forecast_input(df_test, target_col, model, time_steps, NUM_ITEMS)\n","#forecasts_original = scaler.inverse_transform(forecast_output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Assuming df_all_data contains all data up to day 1941\n","# forecast_input = prepare_forecast_input(df_test, time_steps, NUM_ITEMS)\n","\n","# Generate forecasts\n","# forecasts = model.predict(forecast_input)\n","# forecasts_original = scaler.inverse_transform(forecasts)\n","\n","# forecasts_original now contains the predicted sales amounts for days 1942 to 1969\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:26.857391Z","iopub.status.busy":"2024-01-18T14:34:26.857016Z","iopub.status.idle":"2024-01-18T14:34:27.273805Z","shell.execute_reply":"2024-01-18T14:34:27.272980Z","shell.execute_reply.started":"2024-01-18T14:34:26.857362Z"},"trusted":true},"outputs":[],"source":["# Prepare input for forecasts\n","# I cannot use the custom lstm_data_generator\n","# Prepare 7 day slices each shifted by one day\n","def prepare_forecast_input(df, time_steps, target_col):\n","    forecast_input = []\n","    for i in range(0, len(df)//NUM_ITEMS): #i=0; 1, 2, 3, ..., 35?\n","        if i + time_steps < (len(df)-1)//NUM_ITEMS: #7, 8, 9, 10, ...\n","            start_idx = i*NUM_ITEMS\n","            end_idx   = start_idx + NUM_ITEMS * time_steps\n","            sequence  = df.iloc[start_idx : end_idx, :].drop(target_col, axis=1).to_numpy()\n","            forecast_input.append(sequence)\n","    return np.array(forecast_input)\n","\n","predict_array = prepare_forecast_input(df=df_test, time_steps=time_steps, target_col=target_col)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:34:29.975040Z","iopub.status.busy":"2024-01-18T14:34:29.974179Z","iopub.status.idle":"2024-01-18T14:34:29.981976Z","shell.execute_reply":"2024-01-18T14:34:29.981017Z","shell.execute_reply.started":"2024-01-18T14:34:29.974993Z"},"trusted":true},"outputs":[],"source":["predict_array.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:35:58.035250Z","iopub.status.busy":"2024-01-18T14:35:58.034255Z","iopub.status.idle":"2024-01-18T14:36:03.405105Z","shell.execute_reply":"2024-01-18T14:36:03.404120Z","shell.execute_reply.started":"2024-01-18T14:35:58.035208Z"},"trusted":true},"outputs":[],"source":["forecast_normalized = model.predict(predict_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:36:45.205555Z","iopub.status.busy":"2024-01-18T14:36:45.205169Z","iopub.status.idle":"2024-01-18T14:36:45.210651Z","shell.execute_reply":"2024-01-18T14:36:45.209587Z","shell.execute_reply.started":"2024-01-18T14:36:45.205524Z"},"trusted":true},"outputs":[],"source":["forecasts_original = scaler.inverse_transform(forecast_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-18T14:37:28.194159Z","iopub.status.busy":"2024-01-18T14:37:28.193718Z","iopub.status.idle":"2024-01-18T14:37:28.200519Z","shell.execute_reply":"2024-01-18T14:37:28.199591Z","shell.execute_reply.started":"2024-01-18T14:37:28.194120Z"},"trusted":true},"outputs":[],"source":["forecasts_original.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Now, let's define a function to calculate WRMSSE by calculating the RMSSE for each series and then multiplying by the weights and summing them up. \n","def calculate_weights(sales_data, last_n_days=28):\n","    # sales_data: DataFrame with columns ['item_id', 'day', 'sales']\n","    # Sum sales for each item over the last_n_days\n","    item_sales = sales_data[sales_data['day'] > sales_data['day'].max() - last_n_days].groupby('item_id')['sales'].sum()\n","    # Total sales for all items\n","    total_sales = item_sales.sum()\n","    # Calculate weights\n","    weights = item_sales / total_sales\n","    return weights\n","\n","def rmsse(y_true, y_pred, h, y_train):\n","    numerator = np.sum((y_true - y_pred) ** 2) / h\n","    denominator = np.sum(np.diff(y_train) ** 2) / (len(y_train) - 1) # np.diff to calc the diff for consecutive elements\n","    return np.sqrt(numerator / denominator)\n","\n","def wrmsse(y_trues, y_preds, weights, h, y_trains):\n","    rmsse_values = [rmsse(y_true, y_pred, h, y_train) for y_true, y_pred, y_train in zip(y_trues, y_preds, y_trains)]\n","    return np.sum(np.array(weights) * np.array(rmsse_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate the model on the test set\n","def evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, time_steps, n):\n","    test_gen = lstm_data_generator(df_test, target_col, time_steps, batch_size)\n","    steps = max(1, len(df_test) // (batch_size * n))  # Ensure at least 1 step\n","    y_pred_normalized = model.predict(test_gen, steps=steps)\n","    y_pred_original = scaler.inverse_transform(y_pred_normalized)\n","    y_true_normalized = df_test[target_col].values\n","    y_true_original = scaler.inverse_transform(y_true_normalized)\n","    \n","    #First concatenate all elements used for training (df_train and df_val)\n","    y_train_all_normalized = pd.concat([df_train[target_col], df_val[target_col]], axis=0).values\n","    y_train_all_original = scaler.inverse_transform(y_train_all_normalized)\n","    \n","    # Reshape the predictions and actuals to separate each item's time series\n","    y_pred_series = [y_pred_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","    y_true_series = [y_true_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Similarly reshape the training data for RMSSE calculation\n","    y_train_all_series = [y_train_all_original[i::NUM_ITEMS] for i in range(NUM_ITEMS)]\n","\n","    # Check - can be deleted later on\n","    print('len y_pred_series: ' + len(y_pred_series))\n","    print('len y_true_series: ' + len(y_true_series))\n","    print('len y_train_all_series: ' + len(y_train_all_series))\n","    \n","    # Calculate WRMSSE\n","    weights = calculate_weights(sales_data)\n","    wrmsse_score = wrmsse(y_trues=y_true_series, y_preds=y_pred_series, weights=weights, h=28, y_trains=y_train_all_series)\n","\n","    print(\"Test WRMSSE: \", wrmsse_score)\n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    \n","    # Calculate wrmsse score\n","    wrmsse_score = wrmsse(\n","        y_trues=y_true_original,\n","        y_preds=y_pred_original,\n","        weights=calculate_weights(sales_data),\n","        h=28, # forecast horizon\n","        y_train=y_train_all_original\n","    )\n","    print(\"Test WRMSSE: \", wrmsse_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Call the evaluate function\n","# evaluate_model_wrmsse(model, df_test, df_train, df_val, batch_size, time_steps, VAL_END)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"},{"datasetId":4320670,"sourceId":7425680,"sourceType":"datasetVersion"},{"datasetId":4322354,"sourceId":7427996,"sourceType":"datasetVersion"},{"datasetId":4322850,"sourceId":7428689,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
