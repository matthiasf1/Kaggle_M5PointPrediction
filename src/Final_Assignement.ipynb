{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1><center>Final Assignement</center></h1>\n","\n","Student number: 200420653\n","\n","<a class=\"anchor\" id=\"0\"></a>\n","## Table of content\n","1. [Introduction and objectives](#1)<br>\n","    1.1 [Introduction](#1.1)<br>\n","\n","2. [Data](#2)<br>\n","    2.1 [Data sources](#2.1)<br>\n","    2.2 [Data cleaning](#2.2)<br>\n","    2.3 [Data preparation](#2.3)<br>\n","\n","4. [Summary](#4)<br>"]},{"cell_type":"markdown","metadata":{},"source":["**Doings:** \n","<br>\n","\n","Code\n","### - zeigen, dass Vorgehensweise dem 7 Schritte Ansatz von Chollet entspricht\n","- check if after first transformations a reset_index(true) is needed\n","- plotting the data to get some insights\n","- show picture of formula for RMSSE from directory ../res/misc/Formula.png\n","- Feature Selection: Reduce the dimensionality of data by selecting only the most important features - may involve domain knowledge or techniques like PCA or feature importance scoring.\n","- merge df with weekdays\n","- only read in data in dataframes that is needed. In case something not needed, delete it\n","-create classes; use special skills like the super class or an inheritance from another class; ask gpt which special skills could make sense\n","- If I reduce the size of the df and hash the values to integers the question remains if the models predictions would have been better without that conversion + check if float32 conversion decreases accuracy of models\n","- Use scalene to measure the performance of the code\n","- clarify how the 7 step approach from chollet book is used\n","- dataframe: hash id's to decrease size of df drastically. Could speed up all computations\n","- maybe normalization makes sense but beware of fact that only reading operations are done on the data\n","- it seems like I have use tensorflow, so keep in mind that it is used somewhere. Maybe just keras works\n","\n","Report\n","- correct citation style with ACM and Mendeley\n","\n","Information\n","- sales_amount values are originally d_1, d_2 and so on and either int16 or int8, so should not be a float value"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:31.740721Z","iopub.status.busy":"2023-11-26T20:51:31.740270Z","iopub.status.idle":"2023-11-26T20:51:32.890826Z","shell.execute_reply":"2023-11-26T20:51:32.889907Z","shell.execute_reply.started":"2023-11-26T20:51:31.740681Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:32.893727Z","iopub.status.busy":"2023-11-26T20:51:32.892858Z","iopub.status.idle":"2023-11-26T20:51:32.901413Z","shell.execute_reply":"2023-11-26T20:51:32.900136Z","shell.execute_reply.started":"2023-11-26T20:51:32.893685Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","\n","###local###\n","#get parent folder of current directory\n","parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","#Directory resources\n","res_dir = parent_dir + '/res/'\n","src_dir = parent_dir + '/src/'\n","\n","###On Kaggle###\n","#res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","# ?? src_dir = '/kaggle/input/m5-forecasting-accuracy/src/'\n","\n","\n","print('Parent directory of current notebook: ' + parent_dir)\n","print('Resource directory: ' + res_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:32.904091Z","iopub.status.busy":"2023-11-26T20:51:32.903157Z","iopub.status.idle":"2023-11-26T20:51:43.752634Z","shell.execute_reply":"2023-11-26T20:51:43.751552Z","shell.execute_reply.started":"2023-11-26T20:51:32.904048Z"},"trusted":true},"outputs":[],"source":["# Import the provided csv files\n","df_cal = pd.read_csv(res_dir + 'calendar.csv')\n","df_prices = pd.read_csv(res_dir + 'sell_prices.csv')\n","df_train_eval = pd.read_csv(res_dir + 'sales_train_evaluation.csv')\n","#df_train_val = pd.read_csv(res_dir + 'sales_train_validation.csv')\n","#df_sample_subm = pd.read_csv(res_dir + 'sample_submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.784407Z","iopub.status.busy":"2023-11-26T20:51:43.784078Z","iopub.status.idle":"2023-11-26T20:51:43.794336Z","shell.execute_reply":"2023-11-26T20:51:43.793427Z","shell.execute_reply.started":"2023-11-26T20:51:43.784381Z"},"trusted":true},"outputs":[],"source":["#Inspect dataframes\n","# print('\\nCalendar dataframe: ')\n","# print(df_cal.head())\n","# print('\\nPrices dataframe: ')\n","# print(df_prices.head())\n","# print('\\nTrain evaluation dataframe: ')\n","# print(df_train_eval.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.796360Z","iopub.status.busy":"2023-11-26T20:51:43.795756Z","iopub.status.idle":"2023-11-26T20:51:43.808072Z","shell.execute_reply":"2023-11-26T20:51:43.807340Z","shell.execute_reply.started":"2023-11-26T20:51:43.796326Z"},"trusted":true},"outputs":[],"source":["#Helper function to reduce memory usage of dataframes\n","def reduce_df_mem_usage(df, df_name):\n","    \"\"\" \n","    Helper function to iterate all columns of given dataframe and check and set for smallest dtype to reduce memory usage\n","    Taken and adapted from the widely used function which is available for instance here: \n","    https://www.kaggle.com/code/gemartin/load-data-reduce-memory-usage/notebook\n","    There is a flaw in the integer section in the publicly available function which is that it possibly introduces rounding errors when converting to a smaller dtype\n","    \"\"\"\n","\n","    #Print original memory usage\n","    print('Dataframe ' + df_name + ' is being processed...')\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","\n","    #Iterate through each column\n","    for col in df.columns:\n","        #Get dtype\n","        col_type = df[col].dtype\n","        \n","        #try except throw block to try if the following code works, otherwise skip this loop\n","        try:\n","\n","            #If type is not an object, therefore numerical, get biggest and smallest values\n","            if col_type != object:\n","                c_min = df[col].min()\n","                c_max = df[col].max()\n","                \n","                #If type is int\n","                if str(col_type)[:3] == 'int':\n","                    #If min value is greater than min value of given dtype and max value is smaller than max value of given dtype -> adjust dtype\n","                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                        df[col] = df[col].astype(np.int8)\n","                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                        df[col] = df[col].astype(np.int16)\n","                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                        df[col] = df[col].astype(np.int32)\n","                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                        df[col] = df[col].astype(np.int64)\n","                #Downcasting of float values leads to rounding errors -> as precision is crucial for float values, no downcasting is performed\n","\n","            #If none of the above, then assumption that  finite set of possible values -> convert to category which is internally stored as int but when queried returns the string\n","            else:\n","                df[col] = df[col].astype('category')\n","        \n","        except:\n","            pass\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    print('---------------------------------------------------\\n')\n","\n","    return df\n","\n","\n","# Reduce size for float when precision is not really needed\n","            # else:\n","                # if str(col_type)[:7] == 'float64':\n","                #     #calculate difference between original column and downcasted column\n","                #     diff_64_32 = np.abs(df[col] - df[col].astype(np.float32)).round(2)\n","                #     # If the max difference is below the threshold, downcast column\n","                #     if diff_64_32.max() < prec_threshold:\n","                #         diff_32_16 = np.abs(df[col] - df[col].astype(np.float16)).round(2)\n","                #         if diff_32_16.max() < prec_threshold:\n","                #             df[col] = df[col].astype(np.float16).round(2)\n","                #         else: df[col] = df[col].astype(np.float32).round(2)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-26T09:49:24.642134Z","iopub.status.busy":"2023-11-26T09:49:24.641622Z","iopub.status.idle":"2023-11-26T09:49:24.657587Z","shell.execute_reply":"2023-11-26T09:49:24.656283Z","shell.execute_reply.started":"2023-11-26T09:49:24.642104Z"}},"source":["## Memory Reducer\n","# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n","# :verbose                                        # type: bool\n","def reduce_df_mem_usage(df, df_name, verbose=True):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2    \n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                       df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)    \n","    end_mem = df.memory_usage().sum() / 1024**2\n","    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:02:33.016464Z","iopub.status.busy":"2023-11-26T10:02:33.016127Z","iopub.status.idle":"2023-11-26T10:02:35.531633Z","shell.execute_reply":"2023-11-26T10:02:35.530083Z","shell.execute_reply.started":"2023-11-26T10:02:33.016436Z"}},"outputs":[],"source":["#Read in all data used later on\n","df_prices = reduce_df_mem_usage(df_prices, 'df_prices')\n","df_cal = reduce_df_mem_usage(df_cal, 'df_cal')\n","df_train_eval = reduce_df_mem_usage(df_train_eval, 'df_train_eval')\n","\n","#delete dataframe from memory or just don't read in\n","#del df_sample_subm\n","#get name of dataframe\n"]},{"cell_type":"markdown","metadata":{},"source":["In the following I will create 3 dataframes that will be stored in pickle format so that unused dataframes can be deleted from memory and the information is clearly separated.\n","The dataframes are:\n","- 1. df_conv: base grid with the main train information that contains the conversion of the sales data from wide to long format and merge of the sales price\n","- 2. calendar: contains calendar data and some generated date features\n","- 3. XXX: Feature engineering\n","\n","First convert from wide to long format of the training dataframe. Right now all sales days are in columns. We want them in rows for processing in ML / DL models."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# countrows df_train_eval\n","#df_train_eval.shape\n","#df_train_eval.head()\n","#df_cal.head()\n","#df_prices.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.809854Z","iopub.status.busy":"2023-11-26T20:51:43.808988Z","iopub.status.idle":"2023-11-26T20:52:35.029576Z","shell.execute_reply":"2023-11-26T20:52:35.028394Z","shell.execute_reply.started":"2023-11-26T20:51:43.809826Z"},"trusted":true},"outputs":[],"source":["# Convert from wide to long format\n","df_conv = pd.melt(df_train_eval,\n","                    id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n","                    var_name='d', \n","                    value_name='sales_amount')\n","\n","# delete df_train_eval from memory because no longer needed\n","del df_train_eval \n","\n","# Reduce memory usage of df_conv\n","df_conv = reduce_df_mem_usage(df_conv, 'df_conv')"]},{"cell_type":"markdown","metadata":{},"source":["Now we have the issue, that 0 values in the sales_amount column could mean that there were no sales, it could also mean that the sales start of the product was later and the product was just not available.\n","The sales start date can be infered from the price df. If there is no sales week for a product then that most likely means it wasn't available for sale.\n","I will first try to delete all rows from df_conv where there were no weeks available for from the price df. This will be done by adding the week information from prices through a left join. Then the week information is available to make a right join on the prices table and thereby filter out all rows where there was no sales week available (the product didn't exist yet).\n","\n","The reason why I didn't convert float types is that they loose precision. That is absolutely crucial for sales prediction. That is why now the dataframe is too large for the merge operation in the following. Even 30GB RAM machines run out of memory on this computation as it require much more memory than the original ~680MB for df_conv and ~85MB for Prices. The solution for me is using DASK which handles operations on dataframes in smaller chunks."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["####### Cited from the 1st solution (exact citing) #######\n","## Merging by concat to not lose dtypes\n","def merge_by_concat_left(df1, df2, merge_on):\n","    merged_gf = df1[merge_on]\n","    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n","    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n","    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n","    return df1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:52:35.075641Z","iopub.status.busy":"2023-11-26T20:52:35.075337Z","iopub.status.idle":"2023-11-26T20:52:48.114594Z","shell.execute_reply":"2023-11-26T20:52:48.113714Z","shell.execute_reply.started":"2023-11-26T20:52:35.075615Z"},"trusted":true},"outputs":[],"source":["# Combine the converted df with the calendar df\n","df_conv = merge_by_concat_left(df_conv, df_cal[['d', 'wm_yr_wk']], ['d'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:53:02.535926Z","iopub.status.busy":"2023-11-26T20:53:02.535537Z","iopub.status.idle":"2023-11-26T20:53:02.557826Z","shell.execute_reply":"2023-11-26T20:53:02.556583Z","shell.execute_reply.started":"2023-11-26T20:53:02.535895Z"},"trusted":true},"outputs":[],"source":["# Check output of operation\n","df_conv.head()"]},{"cell_type":"markdown","metadata":{},"source":["Merge with the prices df to get the sales start date for each product. Then filter out all rows where there was no price available (the product didn't exist yet)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature engineering for prices can be done here, and then can in the next step be merged with the df_conv dataframe and deleted from memory\n","# ...\n","\n","\n","\n","# Combine the converted df with the prices df\n","df_conv = merge_by_concat_left(df_conv, df_prices, ['store_id', 'item_id', 'wm_yr_wk'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a new column is_available where when column sell_price is NaN the value is 0 (indicates item was not available) and otherwiese 1 (indicates the item was available that day)\n","df_conv['is_available'] = np.where(df_conv['sell_price'].isna(), 0, 1).astype(np.int8)\n","\n","# Also set column sales_amount to NaN when sell_price is NaN\n","df_conv['sales_amount'] = np.where(df_conv['sell_price'].isna(), np.nan, df_conv['sales_amount'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check output of operation\n","df_conv.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# delete df_prices from memory because no longer needed\n","del df_prices\n","del df_conv['wm_yr_wk']\n","df_conv.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Muss ich folgenden Schritt aus Koreanischer LÃ¶sung nehmen? -->Comment MF: create a new empty dataframe which gets appended to the original one for the next 28 days (prediction period) and fill with NaNs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Base steps are done, now lets save to pickle file\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_1.pkl')\n","del df_conv"]},{"cell_type":"markdown","metadata":{},"source":["Before we move on to the merge the infomation from the calendar to the df_conv dataframe, it makes sense to perform some feature engineering based on the prices information as long as the df_conv is smaller without the calendar information."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# <<< Perform feature engineering based on prices on the df_conv dataframe as was done in the leading solution >>>"]},{"cell_type":"markdown","metadata":{},"source":["### Create the second dataframe with the calendar information\n","Now lets get the calendar information and do some feature engineering for a third dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read df_conv from pickle file \n","df_conv = pd.read_pickle(src_dir + 'processed_data/' + 'df_1.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Retrieve only identifying columns of df_conv to keep df small and to merge with df_conv\n","df_conv = df_conv[['id', 'd']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pick all useful columns for merge with df_conv\n","icols = ['date',\n","         'd',\n","         'wday',\n","         'event_name_1',\n","         'event_type_1',\n","         'event_name_2',\n","         'event_type_2',\n","         'snap_CA',\n","         'snap_TX',\n","         'snap_WI']\n","\n","# Now merge the df_conv with useful features from the prices df\n","df_conv = merge_by_concat_left(df_conv, df_cal[icols], ['d'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the date column to datetime format\n","df_conv['date'] = pd.to_datetime(df_conv['date'])\n","\n","# Do feature engineering on the df_conv dataframe; more features are possible\n","df_conv['mday'] = df_conv['date'].dt.day.astype(np.int8)\n","df_conv['week'] = df_conv['date'].dt.isocalendar().week.astype(np.int8)\n","df_conv['month'] = df_conv['date'].dt.month.astype(np.int8)\n","df_conv['year'] = df_conv['date'].dt.year.astype(np.int16)\n","# df_conv['tm_y'] = (df_conv['tm_y'] - df_conv['tm_y'].min()).astype(np.int8)\n","# df_conv['tm_wm'] = df_conv['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n","\n","# df_conv['tm_dw'] = df_conv['date'].dt.dayofweek.astype(np.int8) \n","# df_conv['tm_w_end'] = (df_conv['tm_dw']>=5).astype(np.int8)\n","\n","df_conv['d'] = df_conv['d'].str.replace('d_', '').astype(np.int16)\n","\n","# Date not needed anymore for final training df\n","del df_conv['date']\n","del df_conv['id']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save to pickle and delete df_cal from memory because no longer needed\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_2.pkl')\n","del df_cal\n","del df_conv\n","\n","# Now the date column with the old format can be deleted from the df_conv dataframe\n","df_conv = pd.read_pickle(src_dir + 'processed_data/' + 'df_1.pkl')\n","del df_conv['d']\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_1.pkl')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
