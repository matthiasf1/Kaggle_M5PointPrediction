{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1><center>Final Assignement</center></h1>\n","\n","<a class=\"anchor\" id=\"0\"></a>\n","### Table of content\n","1. [Loading in and reading data](#1)<br>\n","2. [Feature Engineering](#2)<br>\n","3. [Loading in and reading data](#3)<br>"]},{"cell_type":"markdown","metadata":{},"source":["This is the first Notebook. It's used to retrieve the data from the M5 challenge, merges dataframes, does feature engineering and then writes the output to pickle files which can then be used in the next script as a starting point. Questions 1-3 of the methodology of Deep Learning with Python (\"Define the Problem\", \"Choosing a measure of success\", \"Deciding on an evaluation protocol\") are answered in the report itself. I will start with step 4: \"Preparing your data\" from here."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:31.740721Z","iopub.status.busy":"2023-11-26T20:51:31.740270Z","iopub.status.idle":"2023-11-26T20:51:32.890826Z","shell.execute_reply":"2023-11-26T20:51:32.889907Z","shell.execute_reply.started":"2023-11-26T20:51:31.740681Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np\n","import requests\n","import helper_functions as hf"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Environment: local\n"]}],"source":["# Set variables\n","NUM_ITEMS = 30490\n","\n","#Specify directories\n","CODE_ENV = hf.detect_environment()\n","dirs = hf.get_paths(CODE_ENV)\n","parent_dir = dirs['parent_dir']\n","res_dir = dirs['res_dir']\n","src_dir = dirs['src_dir']\n","prc_dir = dirs['prc_dir']"]},{"cell_type":"markdown","metadata":{},"source":["<br>\n","<a id='1'></a>\n","<h2>1. Loading data from CSV files, reduce memory usage, convert and merge with other dataframes</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:32.904091Z","iopub.status.busy":"2023-11-26T20:51:32.903157Z","iopub.status.idle":"2023-11-26T20:51:43.752634Z","shell.execute_reply":"2023-11-26T20:51:43.751552Z","shell.execute_reply.started":"2023-11-26T20:51:32.904048Z"},"trusted":true},"outputs":[],"source":["# Import the provided csv files\n","df_cal = pd.read_csv(res_dir + 'calendar.csv')\n","df_prices = pd.read_csv(res_dir + 'sell_prices.csv')\n","df_train_eval = pd.read_csv(res_dir + 'sales_train_evaluation.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.784407Z","iopub.status.busy":"2023-11-26T20:51:43.784078Z","iopub.status.idle":"2023-11-26T20:51:43.794336Z","shell.execute_reply":"2023-11-26T20:51:43.793427Z","shell.execute_reply.started":"2023-11-26T20:51:43.784381Z"},"trusted":true},"outputs":[],"source":["# Inspect dataframes\n","# print('\\nCalendar dataframe: ')\n","# print(df_cal.head())\n","# print('\\nPrices dataframe: ')\n","# print(df_prices.head())\n","# print('\\nTrain evaluation dataframe: ')\n","# print(df_train_eval.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.796360Z","iopub.status.busy":"2023-11-26T20:51:43.795756Z","iopub.status.idle":"2023-11-26T20:51:43.808072Z","shell.execute_reply":"2023-11-26T20:51:43.807340Z","shell.execute_reply.started":"2023-11-26T20:51:43.796326Z"},"trusted":true},"outputs":[],"source":["# Helper function to reduce memory usage of dataframes\n","def reduce_df_mem_usage(df, df_name):\n","    \"\"\" \n","    Helper function to iterate all columns of given dataframe and check and set for smallest dtype to reduce memory usage\n","    Taken and adapted from the widely used function which is available for instance here: \n","    https://www.kaggle.com/code/gemartin/load-data-reduce-memory-usage/notebook\n","    There is a flaw in the integer section in the publicly available function which is that it possibly introduces rounding errors when converting to a smaller dtype\n","    \"\"\"\n","\n","    #Print original memory usage\n","    print('Dataframe ' + df_name + ' is being processed...')\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","\n","    #Iterate through each column\n","    for col in df.columns:\n","        #Get dtype\n","        col_type = df[col].dtype\n","        \n","        #try except throw block to try if the following code works, otherwise skip this loop\n","        try:\n","\n","            #If type is not an object, therefore numerical, get biggest and smallest values\n","            if col_type != object:\n","                c_min = df[col].min()\n","                c_max = df[col].max()\n","                \n","                #If type is int\n","                if str(col_type)[:3] == 'int':\n","                    #If min value is greater than min value of given dtype and max value is smaller than max value of given dtype -> adjust dtype\n","                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                        df[col] = df[col].astype(np.int8)\n","                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                        df[col] = df[col].astype(np.int16)\n","                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                        df[col] = df[col].astype(np.int32)\n","                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                        df[col] = df[col].astype(np.int64)\n","                #Downcasting of float values leads to rounding errors -> as precision is crucial for float values, no downcasting is performed\n","\n","            #If none of the above, then assumption that  finite set of possible values -> convert to category which is internally stored as int but when queried returns the string\n","            else:\n","                df[col] = df[col].astype('category')\n","        \n","        except:\n","            pass\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    print('---------------------------------------------------\\n')\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:02:33.016464Z","iopub.status.busy":"2023-11-26T10:02:33.016127Z","iopub.status.idle":"2023-11-26T10:02:35.531633Z","shell.execute_reply":"2023-11-26T10:02:35.530083Z","shell.execute_reply.started":"2023-11-26T10:02:33.016436Z"}},"outputs":[],"source":["# Read in all data used later on\n","df_prices = reduce_df_mem_usage(df_prices, 'df_prices')\n","df_cal = reduce_df_mem_usage(df_cal, 'df_cal')\n","df_train_eval = reduce_df_mem_usage(df_train_eval, 'df_train_eval')"]},{"cell_type":"markdown","metadata":{},"source":["The final submission dataframe should also contain the validation data. The df_train_eval already contains this information so it will be extracted and transformed into the suitable output format. It will then be written out to a pkl file and read in and concatenated with the predictions from the model in the next notebook"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get column id and all columns from d_1914 to d_1941\n","df_val = df_train_eval.iloc[:, [0] + list(range(-28, 0))]\n","# rename day columns to F_1 to F_28\n","# Generate the new column names for columns 1 to 29\n","new_column_names = {df_val.columns[i]: 'F' + str(i) for i in range(1, 29)}\n","\n","# Rename the columns in the DataFrame\n","df_val.rename(columns=new_column_names, inplace=True)\n","\n","# Replace string evaluation with validation in the id column\n","df_val['id'] = df_val['id'].str.replace('evaluation', 'validation')\n","\n","# write out\n","df_val.to_pickle(src_dir + 'processed_data/' + 'df_1.pkl')\n","\n","# delete\n","del df_val"]},{"cell_type":"markdown","metadata":{},"source":["In the following I will create 3 dataframes that will be stored in pickle format so that unused dataframes can be deleted from memory and the information is clearly separated.\n","The dataframes are:\n","- 1. df_1: base grid with the main train information that contains the conversion of the sales data from wide to long format and merge of the sales price\n","- 2. df_2: Feature engineering\n","- 3. df_3: contains calendar data and some calendar related generated date features\n","\n","First convert from wide to long format of the training dataframe. Right now all sales days are in columns. We want them in rows for processing in ML / DL models."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.809854Z","iopub.status.busy":"2023-11-26T20:51:43.808988Z","iopub.status.idle":"2023-11-26T20:52:35.029576Z","shell.execute_reply":"2023-11-26T20:52:35.028394Z","shell.execute_reply.started":"2023-11-26T20:51:43.809826Z"},"trusted":true},"outputs":[],"source":["# Convert from wide to long format\n","df_conv = pd.melt(df_train_eval,\n","                    id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n","                    var_name='d', \n","                    value_name='sales_amount')\n","\n","# delete df_train_eval from memory because no longer needed\n","del df_train_eval \n","\n","# Reduce memory usage of df_conv\n","df_conv = reduce_df_mem_usage(df_conv, 'df_conv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create a dataframe for the next 28 days such that a slice of all 30490 items for day 1 will be copied, the columns remain the same and only for day a new value with d_1942 - d_1969 will be generated with a NaN value for sales_amount\n","df_future_base = df_conv[df_conv['d']=='d_1'].copy()\n","# set sales_amount to NaN\n","df_future_base['sales_amount'] = np.nan\n","#create an empty dataframe that will hold the future 28 day values\n","df_future = pd.DataFrame()\n","#loop 28 times to concat 28 days\n","for i in range(28):\n","    df_future_base['d'] = 'd_' + str(1942 + i)\n","    df_future = pd.concat([df_future, df_future_base.copy()], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now concat the future 28 days to the original dataframe\n","df_conv = pd.concat([df_conv, df_future], axis=0)\n","df_conv.reset_index(drop=True, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["Now we have the issue, that 0 values in the sales_amount column could mean that there were no sales, it could also mean that the sales start of the product was later and the product was just not available.\n","The sales start date can be infered from the price df. If there is no sales week for a product then that most likely means it wasn't available for sale.\n","I will first try to delete all rows from df_conv where there were no weeks available for from the price df. This will be done by adding the week information from prices through a left join. Then the week information is available to make a right join on the prices table and thereby filter out all rows where there was no sales week available (the product didn't exist yet)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["####### Cited from the 1st solution (exact citing) #######\n","## Merging by concat to not lose dtypes\n","def merge_by_concat_left(df1, df2, merge_on):\n","    merged_gf = df1[merge_on]\n","    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n","    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n","    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n","    return df1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:52:35.075641Z","iopub.status.busy":"2023-11-26T20:52:35.075337Z","iopub.status.idle":"2023-11-26T20:52:48.114594Z","shell.execute_reply":"2023-11-26T20:52:48.113714Z","shell.execute_reply.started":"2023-11-26T20:52:35.075615Z"},"trusted":true},"outputs":[],"source":["# Combine the converted df with the calendar df\n","df_conv = merge_by_concat_left(df_conv, df_cal[['d', 'wm_yr_wk']], ['d'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:53:02.535926Z","iopub.status.busy":"2023-11-26T20:53:02.535537Z","iopub.status.idle":"2023-11-26T20:53:02.557826Z","shell.execute_reply":"2023-11-26T20:53:02.556583Z","shell.execute_reply.started":"2023-11-26T20:53:02.535895Z"},"trusted":true},"outputs":[],"source":["# Check output of operation\n","df_conv.head()"]},{"cell_type":"markdown","metadata":{},"source":["Merge with the prices df to get the sales start date for each product. Then filter out all rows where there was no price available (the product didn't exist yet)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine the converted df with the prices df\n","df_conv = merge_by_concat_left(df_conv, df_prices, ['store_id', 'item_id', 'wm_yr_wk'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a new column is_available where when column sell_price is NaN the value is 0 (indicates item was not available) and otherwiese 1 (indicates the item was available that day)\n","df_conv['is_available'] = np.where(df_conv['sell_price'].isna(), 0, 1).astype(np.int8)\n","\n","# Also set column sales_amount to NaN when sell_price is NaN\n","df_conv['sell_price'].fillna(0, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check output of operation\n","df_conv.head()"]},{"cell_type":"markdown","metadata":{},"source":["<br>\n","<h2>2. Feature engineering</h2>\n","<a id='2'></a>\n","Feature engineering will be done here, and then can in the next step be merged with the df_conv dataframe and deleted from memory\n","- 1 days lag #float 64\n","- moving average for 7 and 28 days #float 64\n","- consecutive_zero_sales days\n","- zero_sales_available\n","\n","More possible:\n","- is there a price reduction?\n","- is there a price increase?\n","- adjust for inflation?\n","- consumer sentiment\n","- holiday\n","- weather"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform feature engineering\n","def feature_engineering(df_conv): \n","    ################## lag 1 day sales amount ##############################################################################\n","    df_conv['sales_amount_lag_1'] = df_conv['sales_amount'].shift(NUM_ITEMS)\n","\n","    # Now the first value of every entry in the sales_amount_lag_1 column is NaN, because there is no value for the first day of the time series, so we will replace these with the mode (most frequent value) of the sales_amount column\n","    mode_sales_amount = df_conv.groupby('id')['sales_amount'].agg(lambda x: x.mode()[0])\n","\n","    # Replace the first day's sales_amount_lag_1 for each item with the mode_sales_amount value\n","    df_conv.loc[df_conv['d'] == 'd_1', 'sales_amount_lag_1'] = df_conv.loc[df_conv['d'] == 'd_1', 'id'].map(mode_sales_amount)\n","    ########################################################################################################################\n","\n","    ################## moving average 7 and 28 days #########################\n","    df_conv['sales_amount_moving_avg_7'] = df_conv.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=7).mean())\n","    df_conv['sales_amount_moving_avg_7'] = df_conv['sales_amount_moving_avg_7'].fillna(method='bfill')\n","\n","    df_conv['sales_amount_moving_avg_28'] = df_conv.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=28).mean())\n","    df_conv['sales_amount_moving_avg_28'] = df_conv['sales_amount_moving_avg_28'].fillna(method='bfill')\n","    #########################################################################\n","\n","    ################## days consecutive zero sales and if an entry means that this is a zero sale  #########################\n","    # Step 1: Mark zero sales days where item is available\n","    df_conv['zero_sales_available'] = np.where((df_conv['sales_amount'] == 0) & (df_conv['is_available'] == 1), 1, 0).astype(np.int8)\n","\n","    # Function to apply to each group\n","    def calculate_consecutive_zeros(group):\n","        # Step 2: Identify change points to reset the count for consecutive zeros\n","        group['block'] = (group['zero_sales_available'] == 0).cumsum().astype(np.int16)\n","        \n","        # Step 3: Count consecutive zeros within each block\n","        group['consecutive_zero_sales'] = group.groupby('block').cumcount()\n","        \n","        # Reset count where 'zero_sales_available' is 0, as these are not zero sales days or the item is not available\n","        group['consecutive_zero_sales'] = np.where(group['zero_sales_available'] == 1, group['consecutive_zero_sales'], 0).astype(np.int16)\n","        \n","        return group\n","\n","    # Apply the function to each item group\n","    df_conv = df_conv.groupby('id', group_keys=False).apply(calculate_consecutive_zeros)\n","\n","    # Drop the 'block' column because no longer needed\n","    del df_conv['block']\n","\n","    return df_conv\n","########################################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_conv = feature_engineering(df_conv)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# delete df_prices from memory because no longer needed\n","del df_prices\n","del df_conv['wm_yr_wk']\n","df_conv.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Base steps are done, now lets save to pickle file\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_2.pkl')\n","del df_conv"]},{"cell_type":"markdown","metadata":{},"source":["<br>\n","<h2>3. Create the third dataframe with the calendar information</h2>\n","<a id='2'></a>\n","Now lets get the calendar information and do some date feature engineering."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read df_conv from pickle file \n","df_conv = pd.read_pickle(src_dir + 'processed_data/' + 'df_2.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Retrieve only identifying columns of df_conv to keep df small and to merge with df_conv\n","df_conv = df_conv[['id', 'd']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pick all useful columns for merge with df_conv\n","icols = ['date',\n","         'd',\n","         'wday',\n","         'event_name_1',\n","         'event_type_1',\n","         'event_name_2',\n","         'event_type_2',\n","         'snap_CA',\n","         'snap_TX',\n","         'snap_WI']\n","\n","# Now merge the df_conv with useful features from the prices df\n","df_conv = merge_by_concat_left(df_conv, df_cal[icols], ['d'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the date column to datetime format\n","df_conv['date'] = pd.to_datetime(df_conv['date'])\n","\n","# Do feature engineering on the df_conv dataframe; more features are possible\n","df_conv['week']  = df_conv['date'].dt.isocalendar().week.astype(np.int8)\n","df_conv['month'] = df_conv['date'].dt.month.astype(np.int8)\n","df_conv['year']  = df_conv['date'].dt.year.astype(np.int16)\n","df_conv['weekend'] = (df_conv['wday']<=2).astype(np.int8) # check with binary for weekend\n","\n","df_conv['day_month'] = df_conv['date'].dt.day.astype(np.int8)\n","df_conv['week_month'] = np.ceil(df_conv['day_month'] / 7).astype(np.int8) #check which week of the month, tried vectorized approach\n","df_conv['max_week_month'] = np.ceil(df_conv['date'].dt.days_in_month / 7).astype(np.int8) # Calculate the maximum week number for each row/date\n","df_conv['week_month_norm'] = (df_conv['week_month'] - 1) / (df_conv['max_week_month'] - 1).astype(np.float16) # Normalize week_month\n","df_conv.drop(['max_week_month', 'day_month', 'week_month'], axis=1, inplace=True) # delete columns max_week_month, day_month and week_month because no longer needed\n","\n","# convert all days strings to integers to reduce memory usage\n","df_conv['d'] = df_conv['d'].str.replace('d_', '').astype(np.int16)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["######################################## Convert to cyclical data of date time ########################################\n","# Convert cyclical features with sin and cos\n","df_conv['month_sin'] = np.sin(2 * np.pi * df_conv['month']/12).astype(np.float16)\n","df_conv['month_cos'] = np.cos(2 * np.pi * df_conv['month']/12).astype(np.float16)\n","df_conv.drop('month', axis=1, inplace=True)\n","\n","df_conv['wday_sin'] = np.sin(2 * np.pi * df_conv['wday']/7).astype(np.float16)\n","df_conv['wday_cos'] = np.cos(2 * np.pi * df_conv['wday']/7).astype(np.float16)\n","df_conv.drop('wday', axis=1, inplace=True)\n","\n","df_conv['week_sin'] = np.sin(2 * np.pi * df_conv['week']/53).astype(np.float32)\n","df_conv['week_cos'] = np.cos(2 * np.pi * df_conv['week']/53).astype(np.float32)\n","df_conv.drop('week', axis=1, inplace=True)\n","\n","# Normalize day of month\n","df_conv['total_days_in_month'] = df_conv['date'].dt.days_in_month\n","df_conv['mday_normalized'] = ((df_conv['date'].dt.day - 1) / (df_conv['total_days_in_month'] - 1)).astype(np.float32)\n","\n","# Normalized day of the year\n","df_conv['tm_dy_norm'] = (df_conv['date'].dt.dayofyear / df_conv['date'].dt.is_leap_year.map({True: 366, False: 365})).astype(np.float32)\n","df_conv.drop(['date', 'total_days_in_month'], axis=1, inplace=True)\n","\n","# Create a continous, normalized day number\n","df_conv['day_continuous_normalized'] = (df_conv['d'] / 1969).astype(np.float32)  # 1969 train, val and test days in total\n","\n","# Normalize linear data like year\n","min_year = df_conv['year'].min()\n","max_year = df_conv['year'].max()\n","df_conv['year_normalized'] = ((df_conv['year'] - min_year) / (max_year - min_year)).astype(np.float16)\n","df_conv.drop('year', axis=1, inplace=True)\n","########################################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save to pickle and delete df_cal from memory because no longer needed\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_3.pkl')\n","del df_cal\n","del df_conv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now the date column with the old format can be deleted from the df_conv dataframe\n","df_conv = pd.read_pickle(src_dir + 'processed_data/' + 'df_2.pkl')\n","del df_conv['d']\n","del df_conv['id']\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_2.pkl')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
