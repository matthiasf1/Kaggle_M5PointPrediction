{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1><center>Final Assignement</center></h1>\n","\n","Student number: 200420653\n","\n","<a class=\"anchor\" id=\"0\"></a>\n","## Table of content\n","1. [Introduction and objectives](#1)<br>\n","    1.1 [Introduction](#1.1)<br>\n","\n","2. [Data](#2)<br>\n","    2.1 [Data sources](#2.1)<br>\n","    2.2 [Data cleaning](#2.2)<br>\n","    2.3 [Data preparation](#2.3)<br>\n","\n","4. [Summary](#4)<br>"]},{"cell_type":"markdown","metadata":{},"source":["**Doings:** \n","<br>\n","\n","Code\n","### - zeigen, dass Vorgehensweise dem 7 Schritte Ansatz von Chollet entspricht\n","- check if after first transformations a reset_index(true) is needed\n","- plotting the data to get some insights\n","- exploratory data analysis like here: https://medium.com/@trilok_/the-m5-competition-forecasting-in-store-sales-39c2e6baa7dd\n","- show picture of formula for RMSSE from directory ../res/misc/Formula.png\n","- Feature Selection: Reduce the dimensionality of data by selecting only the most important features - may involve domain knowledge or techniques like PCA or feature importance scoring.\n","- merge df with weekdays\n","- only read in data in dataframes that is needed. In case something not needed, delete it\n","-create classes; use special skills like the super class or an inheritance from another class; ask gpt which special skills could make sense\n","- If I reduce the size of the df and hash the values to integers the question remains if the models predictions would have been better without that conversion + check if float32 conversion decreases accuracy of models\n","- Use scalene to measure the performance of the code\n","- clarify how the 7 step approach from chollet book is used\n","- dataframe: hash id's to decrease size of df drastically. Could speed up all computations\n","- maybe normalization makes sense but beware of fact that only reading operations are done on the data\n","- it seems like I have use tensorflow, so keep in mind that it is used somewhere. Maybe just keras works\n","\n","Report\n","- correct citation style with ACM and Mendeley\n","\n","Information\n","- sales_amount values are originally d_1, d_2 and so on and either int16 or int8, so should not be a float value"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:31.740721Z","iopub.status.busy":"2023-11-26T20:51:31.740270Z","iopub.status.idle":"2023-11-26T20:51:32.890826Z","shell.execute_reply":"2023-11-26T20:51:32.889907Z","shell.execute_reply.started":"2023-11-26T20:51:31.740681Z"},"trusted":true},"outputs":[],"source":["#Import data handling libraries\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Set variables\n","NUM_ITEMS = 30490"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:32.893727Z","iopub.status.busy":"2023-11-26T20:51:32.892858Z","iopub.status.idle":"2023-11-26T20:51:32.901413Z","shell.execute_reply":"2023-11-26T20:51:32.900136Z","shell.execute_reply.started":"2023-11-26T20:51:32.893685Z"},"trusted":true},"outputs":[],"source":["#Specify directories\n","# code_env = 'kaggle'\n","code_env = 'local'\n","# code_env = 'aws'\n","\n","\n","if code_env=='local':\n","    ###local###\n","    #get parent folder of current directory\n","    parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n","\n","    #Directory resources\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'\n","    prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n","\n","if code_env=='kaggle':\n","    ###On Kaggle###\n","    res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n","    prc_dir = '/kaggle/input/processed-data/'\n","\n","if code_env=='aws':\n","    parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n","    res_dir = parent_dir + '/res/'\n","    src_dir = parent_dir + '/src/'"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:32.904091Z","iopub.status.busy":"2023-11-26T20:51:32.903157Z","iopub.status.idle":"2023-11-26T20:51:43.752634Z","shell.execute_reply":"2023-11-26T20:51:43.751552Z","shell.execute_reply.started":"2023-11-26T20:51:32.904048Z"},"trusted":true},"outputs":[],"source":["# Import the provided csv files\n","df_cal = pd.read_csv(res_dir + 'calendar.csv')\n","df_prices = pd.read_csv(res_dir + 'sell_prices.csv')\n","df_train_eval = pd.read_csv(res_dir + 'sales_train_evaluation.csv')\n","#df_train_val = pd.read_csv(res_dir + 'sales_train_validation.csv')\n","#df_sample_subm = pd.read_csv(res_dir + 'sample_submission.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.784407Z","iopub.status.busy":"2023-11-26T20:51:43.784078Z","iopub.status.idle":"2023-11-26T20:51:43.794336Z","shell.execute_reply":"2023-11-26T20:51:43.793427Z","shell.execute_reply.started":"2023-11-26T20:51:43.784381Z"},"trusted":true},"outputs":[],"source":["#Inspect dataframes\n","# print('\\nCalendar dataframe: ')\n","# print(df_cal.head())\n","# print('\\nPrices dataframe: ')\n","# print(df_prices.head())\n","# print('\\nTrain evaluation dataframe: ')\n","# print(df_train_eval.head())\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.796360Z","iopub.status.busy":"2023-11-26T20:51:43.795756Z","iopub.status.idle":"2023-11-26T20:51:43.808072Z","shell.execute_reply":"2023-11-26T20:51:43.807340Z","shell.execute_reply.started":"2023-11-26T20:51:43.796326Z"},"trusted":true},"outputs":[],"source":["#Helper function to reduce memory usage of dataframes\n","def reduce_df_mem_usage(df, df_name):\n","    \"\"\" \n","    Helper function to iterate all columns of given dataframe and check and set for smallest dtype to reduce memory usage\n","    Taken and adapted from the widely used function which is available for instance here: \n","    https://www.kaggle.com/code/gemartin/load-data-reduce-memory-usage/notebook\n","    There is a flaw in the integer section in the publicly available function which is that it possibly introduces rounding errors when converting to a smaller dtype\n","    \"\"\"\n","\n","    #Print original memory usage\n","    print('Dataframe ' + df_name + ' is being processed...')\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","\n","    #Iterate through each column\n","    for col in df.columns:\n","        #Get dtype\n","        col_type = df[col].dtype\n","        \n","        #try except throw block to try if the following code works, otherwise skip this loop\n","        try:\n","\n","            #If type is not an object, therefore numerical, get biggest and smallest values\n","            if col_type != object:\n","                c_min = df[col].min()\n","                c_max = df[col].max()\n","                \n","                #If type is int\n","                if str(col_type)[:3] == 'int':\n","                    #If min value is greater than min value of given dtype and max value is smaller than max value of given dtype -> adjust dtype\n","                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                        df[col] = df[col].astype(np.int8)\n","                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                        df[col] = df[col].astype(np.int16)\n","                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                        df[col] = df[col].astype(np.int32)\n","                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                        df[col] = df[col].astype(np.int64)\n","                #Downcasting of float values leads to rounding errors -> as precision is crucial for float values, no downcasting is performed\n","\n","            #If none of the above, then assumption that  finite set of possible values -> convert to category which is internally stored as int but when queried returns the string\n","            else:\n","                df[col] = df[col].astype('category')\n","        \n","        except:\n","            pass\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    print('---------------------------------------------------\\n')\n","\n","    return df"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:02:33.016464Z","iopub.status.busy":"2023-11-26T10:02:33.016127Z","iopub.status.idle":"2023-11-26T10:02:35.531633Z","shell.execute_reply":"2023-11-26T10:02:35.530083Z","shell.execute_reply.started":"2023-11-26T10:02:33.016436Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataframe df_prices is being processed...\n","Memory usage of dataframe is 208.77 MB\n","Memory usage of dataframe after optimization is: 84.90 MB\n","Decreased by 59.3%\n","---------------------------------------------------\n","\n","Dataframe df_cal is being processed...\n","Memory usage of dataframe is 0.21 MB\n","Memory usage of dataframe after optimization is: 0.19 MB\n","Decreased by 8.7%\n","---------------------------------------------------\n","\n","Dataframe df_train_eval is being processed...\n","Memory usage of dataframe is 452.91 MB\n","Memory usage of dataframe after optimization is: 96.30 MB\n","Decreased by 78.7%\n","---------------------------------------------------\n","\n"]}],"source":["#Read in all data used later on\n","df_prices = reduce_df_mem_usage(df_prices, 'df_prices')\n","df_cal = reduce_df_mem_usage(df_cal, 'df_cal')\n","df_train_eval = reduce_df_mem_usage(df_train_eval, 'df_train_eval')\n","\n","#delete dataframe from memory or just don't read in\n","#del df_sample_subm\n","#get name of dataframe"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>item_id</th>\n","      <th>dept_id</th>\n","      <th>cat_id</th>\n","      <th>store_id</th>\n","      <th>state_id</th>\n","      <th>d_1</th>\n","      <th>d_2</th>\n","      <th>d_3</th>\n","      <th>d_4</th>\n","      <th>...</th>\n","      <th>d_1932</th>\n","      <th>d_1933</th>\n","      <th>d_1934</th>\n","      <th>d_1935</th>\n","      <th>d_1936</th>\n","      <th>d_1937</th>\n","      <th>d_1938</th>\n","      <th>d_1939</th>\n","      <th>d_1940</th>\n","      <th>d_1941</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HOBBIES_1_001_CA_1_evaluation</td>\n","      <td>HOBBIES_1_001</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>HOBBIES_1_002</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HOBBIES_1_003_CA_1_evaluation</td>\n","      <td>HOBBIES_1_003</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HOBBIES_1_004_CA_1_evaluation</td>\n","      <td>HOBBIES_1_004</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HOBBIES_1_005_CA_1_evaluation</td>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>30485</th>\n","      <td>FOODS_3_823_WI_3_evaluation</td>\n","      <td>FOODS_3_823</td>\n","      <td>FOODS_3</td>\n","      <td>FOODS</td>\n","      <td>WI_3</td>\n","      <td>WI</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30486</th>\n","      <td>FOODS_3_824_WI_3_evaluation</td>\n","      <td>FOODS_3_824</td>\n","      <td>FOODS_3</td>\n","      <td>FOODS</td>\n","      <td>WI_3</td>\n","      <td>WI</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30487</th>\n","      <td>FOODS_3_825_WI_3_evaluation</td>\n","      <td>FOODS_3_825</td>\n","      <td>FOODS_3</td>\n","      <td>FOODS</td>\n","      <td>WI_3</td>\n","      <td>WI</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>30488</th>\n","      <td>FOODS_3_826_WI_3_evaluation</td>\n","      <td>FOODS_3_826</td>\n","      <td>FOODS_3</td>\n","      <td>FOODS</td>\n","      <td>WI_3</td>\n","      <td>WI</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30489</th>\n","      <td>FOODS_3_827_WI_3_evaluation</td>\n","      <td>FOODS_3_827</td>\n","      <td>FOODS_3</td>\n","      <td>FOODS</td>\n","      <td>WI_3</td>\n","      <td>WI</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>30490 rows Ã— 1947 columns</p>\n","</div>"],"text/plain":["                                  id        item_id    dept_id   cat_id  \\\n","0      HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n","1      HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES   \n","2      HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES   \n","3      HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n","4      HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n","...                              ...            ...        ...      ...   \n","30485    FOODS_3_823_WI_3_evaluation    FOODS_3_823    FOODS_3    FOODS   \n","30486    FOODS_3_824_WI_3_evaluation    FOODS_3_824    FOODS_3    FOODS   \n","30487    FOODS_3_825_WI_3_evaluation    FOODS_3_825    FOODS_3    FOODS   \n","30488    FOODS_3_826_WI_3_evaluation    FOODS_3_826    FOODS_3    FOODS   \n","30489    FOODS_3_827_WI_3_evaluation    FOODS_3_827    FOODS_3    FOODS   \n","\n","      store_id state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  \\\n","0         CA_1       CA    0    0    0    0  ...       2       4       0   \n","1         CA_1       CA    0    0    0    0  ...       0       1       2   \n","2         CA_1       CA    0    0    0    0  ...       1       0       2   \n","3         CA_1       CA    0    0    0    0  ...       1       1       0   \n","4         CA_1       CA    0    0    0    0  ...       0       0       0   \n","...        ...      ...  ...  ...  ...  ...  ...     ...     ...     ...   \n","30485     WI_3       WI    0    0    2    2  ...       1       0       3   \n","30486     WI_3       WI    0    0    0    0  ...       0       0       0   \n","30487     WI_3       WI    0    6    0    2  ...       0       0       1   \n","30488     WI_3       WI    0    0    0    0  ...       1       1       1   \n","30489     WI_3       WI    0    0    0    0  ...       1       2       0   \n","\n","       d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n","0           0       0       0       3       3       0       1  \n","1           1       1       0       0       0       0       0  \n","2           0       0       0       2       3       0       1  \n","3           4       0       1       3       0       2       6  \n","4           2       1       0       0       2       1       0  \n","...       ...     ...     ...     ...     ...     ...     ...  \n","30485       0       1       1       0       0       1       1  \n","30486       0       0       0       1       0       1       0  \n","30487       2       0       1       0       1       0       2  \n","30488       4       6       0       1       1       1       0  \n","30489       5       4       0       2       2       5       1  \n","\n","[30490 rows x 1947 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df_train_eval"]},{"cell_type":"markdown","metadata":{},"source":["The final submission dataframe should also contain the validation data. The df_train_eval already contains this information so it will be extracted and transformed into the suitable output format. It will then be written out to a pkl file and read in and concatenated with the predictions from the model in the next notebook"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/w1/_96k6h412s5d15f14hdtbbyc0000gn/T/ipykernel_6175/3692366094.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_val.rename(columns=new_column_names, inplace=True)\n","/var/folders/w1/_96k6h412s5d15f14hdtbbyc0000gn/T/ipykernel_6175/3692366094.py:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_val['id'] = df_val['id'].str.replace('evaluation', 'validation')\n"]}],"source":["# get column id and all columns from d_1914 to d_1941\n","df_val = df_train_eval.iloc[:, [0] + list(range(-28, 0))]\n","# rename day columns to F_1 to F_28\n","# Generate the new column names for columns 1 to 29\n","new_column_names = {df_val.columns[i]: 'F' + str(i) for i in range(1, 29)}\n","\n","# Rename the columns in the DataFrame\n","df_val.rename(columns=new_column_names, inplace=True)\n","\n","# Replace string evaluation with validation in the id column\n","df_val['id'] = df_val['id'].str.replace('evaluation', 'validation')\n","\n","# write out\n","df_val.to_pickle(src_dir + 'processed_data/' + 'df_1.pkl')\n","\n","# delete\n","del df_val"]},{"cell_type":"markdown","metadata":{},"source":["In the following I will create 3 dataframes that will be stored in pickle format so that unused dataframes can be deleted from memory and the information is clearly separated.\n","The dataframes are:\n","- 1. df_conv: base grid with the main train information that contains the conversion of the sales data from wide to long format and merge of the sales price\n","- 2. calendar: contains calendar data and some generated date features\n","- 3. XXX: Feature engineering\n","\n","First convert from wide to long format of the training dataframe. Right now all sales days are in columns. We want them in rows for processing in ML / DL models."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:51:43.809854Z","iopub.status.busy":"2023-11-26T20:51:43.808988Z","iopub.status.idle":"2023-11-26T20:52:35.029576Z","shell.execute_reply":"2023-11-26T20:52:35.028394Z","shell.execute_reply.started":"2023-11-26T20:51:43.809826Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataframe df_conv is being processed...\n","Memory usage of dataframe is 1017.24 MB\n","Memory usage of dataframe after optimization is: 678.68 MB\n","Decreased by 33.3%\n","---------------------------------------------------\n","\n"]}],"source":["# Convert from wide to long format\n","df_conv = pd.melt(df_train_eval,\n","                    id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n","                    var_name='d', \n","                    value_name='sales_amount')\n","\n","# delete df_train_eval from memory because no longer needed\n","del df_train_eval \n","\n","# Reduce memory usage of df_conv\n","df_conv = reduce_df_mem_usage(df_conv, 'df_conv')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# create a dataframe for the next 28 days such that a slice of all 30490 items for day 1 will be copied, the columns remain the same and only for day a new value with d_1942 - d_1969 will be generated with a NaN value for sales_amount\n","df_future_base = df_conv[df_conv['d']=='d_1'].copy()\n","# set sales_amount to NaN\n","df_future_base['sales_amount'] = np.nan\n","#create an empty dataframe that will hold the future 28 day values\n","df_future = pd.DataFrame()\n","#loop 28 times to concat 28 days\n","for i in range(28):\n","    df_future_base['d'] = 'd_' + str(1942 + i)\n","    df_future = pd.concat([df_future, df_future_base.copy()], axis=0)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Now concat the future 28 days to the original dataframe\n","df_conv = pd.concat([df_conv, df_future], axis=0)\n","df_conv.reset_index(drop=True, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["Now we have the issue, that 0 values in the sales_amount column could mean that there were no sales, it could also mean that the sales start of the product was later and the product was just not available.\n","The sales start date can be infered from the price df. If there is no sales week for a product then that most likely means it wasn't available for sale.\n","I will first try to delete all rows from df_conv where there were no weeks available for from the price df. This will be done by adding the week information from prices through a left join. Then the week information is available to make a right join on the prices table and thereby filter out all rows where there was no sales week available (the product didn't exist yet).\n","\n","The reason why I didn't convert float types is that they loose precision. That is absolutely crucial for sales prediction. That is why now the dataframe is too large for the merge operation in the following. Even 30GB RAM machines run out of memory on this computation as it require much more memory than the original ~680MB for df_conv and ~85MB for Prices. The solution for me is using DASK which handles operations on dataframes in smaller chunks."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["####### Cited from the 1st solution (exact citing) #######\n","## Merging by concat to not lose dtypes\n","def merge_by_concat_left(df1, df2, merge_on):\n","    merged_gf = df1[merge_on]\n","    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n","    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n","    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n","    return df1"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:52:35.075641Z","iopub.status.busy":"2023-11-26T20:52:35.075337Z","iopub.status.idle":"2023-11-26T20:52:48.114594Z","shell.execute_reply":"2023-11-26T20:52:48.113714Z","shell.execute_reply.started":"2023-11-26T20:52:35.075615Z"},"trusted":true},"outputs":[],"source":["# Combine the converted df with the calendar df\n","df_conv = merge_by_concat_left(df_conv, df_cal[['d', 'wm_yr_wk']], ['d'])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T20:53:02.535926Z","iopub.status.busy":"2023-11-26T20:53:02.535537Z","iopub.status.idle":"2023-11-26T20:53:02.557826Z","shell.execute_reply":"2023-11-26T20:53:02.556583Z","shell.execute_reply.started":"2023-11-26T20:53:02.535895Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>item_id</th>\n","      <th>dept_id</th>\n","      <th>cat_id</th>\n","      <th>store_id</th>\n","      <th>state_id</th>\n","      <th>d</th>\n","      <th>sales_amount</th>\n","      <th>wm_yr_wk</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HOBBIES_1_001_CA_1_evaluation</td>\n","      <td>HOBBIES_1_001</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>HOBBIES_1_002</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HOBBIES_1_003_CA_1_evaluation</td>\n","      <td>HOBBIES_1_003</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HOBBIES_1_004_CA_1_evaluation</td>\n","      <td>HOBBIES_1_004</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HOBBIES_1_005_CA_1_evaluation</td>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              id        item_id    dept_id   cat_id store_id  \\\n","0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n","1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n","2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n","3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n","4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n","\n","  state_id    d  sales_amount  wm_yr_wk  \n","0       CA  d_1           0.0     11101  \n","1       CA  d_1           0.0     11101  \n","2       CA  d_1           0.0     11101  \n","3       CA  d_1           0.0     11101  \n","4       CA  d_1           0.0     11101  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Check output of operation\n","df_conv.head()"]},{"cell_type":"markdown","metadata":{},"source":["Merge with the prices df to get the sales start date for each product. Then filter out all rows where there was no price available (the product didn't exist yet)."]},{"cell_type":"markdown","metadata":{},"source":["Feature engineering for prices can be done here, and then can in the next step be merged with the df_conv dataframe and deleted from memory\n","- Promotions\n","- Average sales per month\n","- Explicit weighting with sales volume per store, country and so on because the final evaluation is being done with WRMSSE \n","- Weight more recent sales higher than older sales\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Combine the converted df with the prices df\n","df_conv = merge_by_concat_left(df_conv, df_prices, ['store_id', 'item_id', 'wm_yr_wk'])"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Create a new column is_available where when column sell_price is NaN the value is 0 (indicates item was not available) and otherwiese 1 (indicates the item was available that day)\n","df_conv['is_available'] = np.where(df_conv['sell_price'].isna(), 0, 1).astype(np.int8)\n","\n","# Also set column sales_amount to NaN when sell_price is NaN\n","#df_conv['sales_amount'] = np.where(df_conv['sell_price'].isna(), 0, df_conv['sales_amount'])\n","df_conv['sell_price'].fillna(0, inplace=True)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>item_id</th>\n","      <th>dept_id</th>\n","      <th>cat_id</th>\n","      <th>store_id</th>\n","      <th>state_id</th>\n","      <th>d</th>\n","      <th>sales_amount</th>\n","      <th>wm_yr_wk</th>\n","      <th>sell_price</th>\n","      <th>is_available</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HOBBIES_1_001_CA_1_evaluation</td>\n","      <td>HOBBIES_1_001</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HOBBIES_1_002_CA_1_evaluation</td>\n","      <td>HOBBIES_1_002</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HOBBIES_1_003_CA_1_evaluation</td>\n","      <td>HOBBIES_1_003</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HOBBIES_1_004_CA_1_evaluation</td>\n","      <td>HOBBIES_1_004</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HOBBIES_1_005_CA_1_evaluation</td>\n","      <td>HOBBIES_1_005</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>d_1</td>\n","      <td>0.0</td>\n","      <td>11101</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              id        item_id    dept_id   cat_id store_id  \\\n","0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n","1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n","2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n","3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n","4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n","\n","  state_id    d  sales_amount  wm_yr_wk  sell_price  is_available  \n","0       CA  d_1           0.0     11101         0.0             0  \n","1       CA  d_1           0.0     11101         0.0             0  \n","2       CA  d_1           0.0     11101         0.0             0  \n","3       CA  d_1           0.0     11101         0.0             0  \n","4       CA  d_1           0.0     11101         0.0             0  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Check output of operation\n","df_conv.head()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Perform feature engineering\n","\"\"\"\n","#####- these columns have to be update in the next notebook based on the predictions made by the model #####\n","- 1 days lag #float 64\n","- moving average for 7 and 28 days #float 64\n","- is there a price reduction?\n","- is there a price increase?\n","- adjust for inflation?\n","- consumer sentiment\n","- holiday\n","- weather\n","- \n","\"\"\"\n","def feature_engineering(df): \n","    ################## lag 1 day sales amount ##############################################################################\n","    df_conv['sales_amount_lag_1'] = df_conv['sales_amount'].shift(NUM_ITEMS)\n","\n","    # Now the first value of every entry in the sales_amount_lag_1 column is NaN, because there is no value for the first day of the time series, so we will replace these with the mode (most frequent value) of the sales_amount column\n","    mode_sales_amount = df_conv.groupby('id')['sales_amount'].agg(lambda x: x.mode()[0])\n","\n","    # Replace the first day's sales_amount_lag_1 for each item with the mode_sales_amount value\n","    df_conv.loc[df_conv['d'] == 'd_1', 'sales_amount_lag_1'] = df_conv.loc[df_conv['d'] == 'd_1', 'id'].map(mode_sales_amount)\n","    ########################################################################################################################\n","\n","    ################## moving average 7 and 28 days #########################\n","    df_conv['sales_amount_moving_avg_7'] = df_conv.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=7).mean())\n","    df_conv['sales_amount_moving_avg_7'] = df_conv['sales_amount_moving_avg_7'].fillna(method='bfill')\n","\n","    df_conv['sales_amount_moving_avg_28'] = df_conv.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=28).mean())\n","    df_conv['sales_amount_moving_avg_28'] = df_conv['sales_amount_moving_avg_28'].fillna(method='bfill')\n","    #########################################################################\n","\n","    ################## days consecutive zero sales and if an entry means that this is a zero sale  #########################\n","    # Step 1: Mark zero sales days where item is available\n","    df_conv['zero_sales_available'] = np.where((df_conv['sales_amount'] == 0) & (df_conv['is_available'] == 1), 1, 0).astype(np.int8)\n","\n","    # Function to apply to each group\n","    def calculate_consecutive_zeros(group):\n","        # Step 2: Identify change points to reset the count for consecutive zeros\n","        group['block'] = (group['zero_sales_available'] == 0).cumsum().astype(np.int16)\n","        \n","        # Step 3: Count consecutive zeros within each block\n","        group['consecutive_zero_sales'] = group.groupby('block').cumcount()\n","        \n","        # Reset count where 'zero_sales_available' is 0, as these are not zero sales days or the item is not available\n","        group['consecutive_zero_sales'] = np.where(group['zero_sales_available'] == 1, group['consecutive_zero_sales'], 0).astype(np.int16)\n","        \n","        return group\n","\n","    # Apply the function to each item group\n","    df_conv = df_conv.groupby('id', group_keys=False).apply(calculate_consecutive_zeros)\n","\n","    # Drop the 'block' column because no longer needed\n","    del df_conv['block']\n","\n","    return df\n","########################################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_conv = feature_engineering(df_conv)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 60034810 entries, 0 to 60034809\n","Data columns (total 16 columns):\n"," #   Column                      Dtype   \n","---  ------                      -----   \n"," 0   id                          category\n"," 1   item_id                     category\n"," 2   dept_id                     category\n"," 3   cat_id                      category\n"," 4   store_id                    category\n"," 5   state_id                    category\n"," 6   d                           object  \n"," 7   sales_amount                float64 \n"," 8   wm_yr_wk                    int16   \n"," 9   sell_price                  float64 \n"," 10  is_available                int8    \n"," 11  sales_amount_lag_1          float64 \n"," 12  sales_amount_moving_avg_7   float64 \n"," 13  sales_amount_moving_avg_28  float64 \n"," 14  zero_sales_available        int8    \n"," 15  consecutive_zero_sales      int16   \n","dtypes: category(6), float64(5), int16(2), int8(2), object(1)\n","memory usage: 5.9+ GB\n"]}],"source":["df_conv.info()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# df_conv[(df_conv['id']== 'HOBBIES_1_001_CA_1_evaluation') & (df_conv['is_available'] == 1)].head(50)\n","# df_conv.head()\n","# df_conv_test[df_conv_test['id']== 'HOBBIES_1_001_CA_1_evaluation'].head(50)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# delete df_prices from memory because no longer needed\n","del df_prices\n","del df_conv['wm_yr_wk']\n","df_conv.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Base steps are done, now lets save to pickle file\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_2.pkl')\n","del df_conv"]},{"cell_type":"markdown","metadata":{},"source":["Before we move on to the merge the infomation from the calendar to the df_conv dataframe, it makes sense to perform some feature engineering based on the prices information as long as the df_conv is smaller without the calendar information."]},{"cell_type":"markdown","metadata":{},"source":["### Create the second dataframe with the calendar information\n","Now lets get the calendar information and do some feature engineering for a third dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# read df_conv from pickle file \n","df_conv = pd.read_pickle(src_dir + 'processed_data/' + 'df_2.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Retrieve only identifying columns of df_conv to keep df small and to merge with df_conv\n","df_conv = df_conv[['id', 'd']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pick all useful columns for merge with df_conv\n","icols = ['date',\n","         'd',\n","         'wday',\n","         'event_name_1',\n","         'event_type_1',\n","         'event_name_2',\n","         'event_type_2',\n","         'snap_CA',\n","         'snap_TX',\n","         'snap_WI']\n","\n","# Now merge the df_conv with useful features from the prices df\n","df_conv = merge_by_concat_left(df_conv, df_cal[icols], ['d'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the date column to datetime format\n","df_conv['date'] = pd.to_datetime(df_conv['date'])\n","\n","# Do feature engineering on the df_conv dataframe; more features are possible\n","# df_conv['mday'] = df_conv['date'].dt.day.astype(np.int8)\n","df_conv['week'] = df_conv['date'].dt.isocalendar().week.astype(np.int8)\n","df_conv['month'] = df_conv['date'].dt.month.astype(np.int8)\n","df_conv['year'] = df_conv['date'].dt.year.astype(np.int16)\n","# df_conv['tm_y'] = (df_conv['tm_y'] - df_conv['tm_y'].min()).astype(np.int8)\n","# df_conv['tm_wm'] = df_conv['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n","\n","# df_conv['tm_dw'] = df_conv['date'].dt.dayofweek.astype(np.int8) \n","# df_conv['tm_w_end'] = (df_conv['tm_dw']>=5).astype(np.int8)\n","\n","df_conv['d'] = df_conv['d'].str.replace('d_', '').astype(np.int16)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["######################################## Convert to cyclical data of date time ########################################\n","# Should be moved to first script\n","# Convert cyclical features with sin and cos\n","df_conv['month_sin'] = np.sin(2 * np.pi * df_conv['month']/12).astype('float16')\n","df_conv['month_cos'] = np.cos(2 * np.pi * df_conv['month']/12).astype('float16')\n","df_conv.drop('month', axis=1, inplace=True)\n","\n","df_conv['wday_sin'] = np.sin(2 * np.pi * df_conv['wday']/7).astype('float16')\n","df_conv['wday_cos'] = np.cos(2 * np.pi * df_conv['wday']/7).astype('float16')\n","df_conv.drop('wday', axis=1, inplace=True)\n","\n","df_conv['week_sin'] = np.sin(2 * np.pi * df_conv['week']/53).astype('float32')\n","df_conv['week_cos'] = np.cos(2 * np.pi * df_conv['week']/53).astype('float32')\n","df_conv.drop('week', axis=1, inplace=True)\n","\n","# Normalize day of month\n","df_conv['total_days_in_month'] = df_conv['date'].dt.days_in_month\n","df_conv['mday_normalized'] = ((df_conv['date'].dt.day - 1) / (df_conv['total_days_in_month'] - 1)).astype('float32')\n","df_conv.drop(['date', 'total_days_in_month'], axis=1, inplace=True)\n","\n","# Create a continous, normalized day number\n","df_conv['day_continuous_normalized'] = (df_conv['d'] / 1969).astype('float32')  # 1969 train, val and test days in total\n","\n","# Normalize linear data like year\n","min_year = df_conv['year'].min()\n","max_year = df_conv['year'].max()\n","df_conv['year_normalized'] = ((df_conv['year'] - min_year) / (max_year - min_year)).astype('float16')\n","df_conv.drop('year', axis=1, inplace=True)\n","########################################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save to pickle and delete df_cal from memory because no longer needed\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_3.pkl')\n","del df_cal\n","del df_conv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Now the date column with the old format can be deleted from the df_conv dataframe\n","df_conv = pd.read_pickle(src_dir + 'processed_data/' + 'df_2.pkl')\n","del df_conv['d']\n","del df_conv['id']\n","df_conv.to_pickle(src_dir + 'processed_data/' + 'df_2.pkl')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":1236839,"sourceId":18599,"sourceType":"competition"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
