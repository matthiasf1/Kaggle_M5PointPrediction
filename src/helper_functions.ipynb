{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Final Assignement</center></h1>\n",
    "\n",
    "This is the helper function file to support and enable outsourcing of the code to a separate file. The main file will be used to run the code and the helper file will be used to store the functions. This ipynb will be written out to a .py file and then imported into the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helper_functions.py\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def detect_environment():\n",
    "    # Check for Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        print(\"Environment: kaggle\")\n",
    "        return 'kaggle'\n",
    "    # Check for AWS\n",
    "    try:\n",
    "        response = requests.get('http://169.254.169.254/latest/meta-data/', timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Environment: aws\")\n",
    "            return 'aws'\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "    # Default to local\n",
    "    print(\"Environment: local\")\n",
    "    return 'local'\n",
    "\n",
    "def get_paths(env):\n",
    "    if env=='local':\n",
    "        ###local###\n",
    "        #get parent folder of current directory\n",
    "        parent_dir = '/Users/mf/Desktop/CS/Studies/7_Final_Project/Kaggle_M5PointPrediction'\n",
    "        #Directory resources\n",
    "        res_dir = parent_dir + '/res/'\n",
    "        src_dir = parent_dir + '/src/'\n",
    "        prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n",
    "        sub_dir = src_dir + 'submissions/' # Directory to save submission files\n",
    "        CONFIG_DIR = src_dir + 'models/configs/' # Directory to save model configurations\n",
    "        CSV_PATH = CONFIG_DIR + 'config.csv'\n",
    "\n",
    "    if env=='kaggle':\n",
    "        ###On Kaggle###\n",
    "        parent_dir = None\n",
    "        res_dir = '/kaggle/input/m5-forecasting-accuracy/'\n",
    "        prc_dir = '/kaggle/input/processed-data/'\n",
    "        src_dir = '/kaggle/working/'\n",
    "        sub_dir = src_dir + 'submissions/'\n",
    "        CONFIG_DIR = src_dir + 'configs/'\n",
    "        CSV_PATH = CONFIG_DIR + 'config.csv'\n",
    "\n",
    "    if env=='aws':\n",
    "        parent_dir = '/home/ubuntu/projects/Kaggle_M5PointPrediction'\n",
    "        res_dir = parent_dir + '/res/'\n",
    "        src_dir = parent_dir + '/src/'\n",
    "        prc_dir = src_dir + 'processed_data/' # Processed data directory with pickled dataframes\n",
    "        sub_dir = src_dir + 'submissions/' # Directory to save submission files\n",
    "        CONFIG_DIR = src_dir + 'hyperparameter_tuning'\n",
    "        CSV_PATH = CONFIG_DIR + '/hyperparameter_search_results.csv'\n",
    "    \n",
    "    dirs = {\n",
    "        'parent_dir': parent_dir,\n",
    "        'res_dir': res_dir,\n",
    "        'src_dir': src_dir,\n",
    "        'prc_dir': prc_dir,\n",
    "        'sub_dir': sub_dir,\n",
    "        'CONFIG_DIR': CONFIG_DIR,\n",
    "        'CSV_PATH': CSV_PATH\n",
    "    }\n",
    "    return dirs\n",
    "\n",
    "\n",
    "# Perform feature engineering\n",
    "\"\"\"\n",
    "#####- these columns have to be update in the next notebook based on the predictions made by the model #####\n",
    "- 1 days lag #float 64\n",
    "- moving average for 7 and 28 days #float 64\n",
    "- ..\n",
    "\"\"\"\n",
    "def feature_engineering(df, num_block_items): \n",
    "    ################## lag 1 day sales amount ##############################################################################\n",
    "    # After shifting the first days values are NAN but not important as we skip them because we start with the second day\n",
    "    df['sales_amount_lag_1'] = df['sales_amount'].shift(num_block_items)\n",
    "    ########################################################################################################################\n",
    "\n",
    "    ################## moving average 7 and 28 days #########################\n",
    "    df['sales_amount_moving_avg_7'] = df.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=7).mean())\n",
    "    df['sales_amount_moving_avg_7'] = df['sales_amount_moving_avg_7'].fillna(method='bfill')\n",
    "\n",
    "    df['sales_amount_moving_avg_28'] = df.groupby('id')['sales_amount'].transform(lambda x: x.rolling(window=28).mean())\n",
    "    df['sales_amount_moving_avg_28'] = df['sales_amount_moving_avg_28'].fillna(method='bfill')\n",
    "    #########################################################################\n",
    "\n",
    "    ################# days consecutive zero sales and if an entry means that this is a zero sale  #########################\n",
    "    # Step 1: Mark zero sales days where item is available\n",
    "    df['zero_sales_available'] = np.where((df['sales_amount'] == 0) & (df['is_available'] == 1), 1, 0).astype(np.int8)\n",
    "\n",
    "    # Function to apply to each group\n",
    "    def calculate_consecutive_zeros(group):\n",
    "        # Step 2: Identify change points to reset the count for consecutive zeros\n",
    "        group['block'] = (group['zero_sales_available'] == 0).cumsum().astype(np.int16)\n",
    "        \n",
    "        # Step 3: Count consecutive zeros within each block\n",
    "        group['consecutive_zero_sales'] = group.groupby('block').cumcount()\n",
    "        \n",
    "        # Reset count where 'zero_sales_available' is 0, as these are not zero sales days or the item is not available\n",
    "        group['consecutive_zero_sales'] = np.where(group['zero_sales_available'] == 1, group['consecutive_zero_sales'], 0).astype(np.int16)\n",
    "        \n",
    "        return group\n",
    "\n",
    "    # Apply the function to each item group\n",
    "    df = df.groupby('id', group_keys=False).apply(calculate_consecutive_zeros)\n",
    "\n",
    "    # Drop the 'block' column because no longer needed\n",
    "    del df['block']\n",
    "\n",
    "    return df\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    try:\n",
    "        # Plot training & validation loss values\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "    except:\n",
    "        print('No history to plot')\n",
    "\n",
    "\n",
    "def save_model(model, model_name, src_dir):\n",
    "    # Save the model to a specified directory\n",
    "    if CODE_ENV=='local':\n",
    "        model.save(src_dir + 'models/' + model_name + '.h5')\n",
    "        \n",
    "    if CODE_ENV=='kaggle':\n",
    "        model.save('/kaggle/working/' + model_name + '.h5')\n",
    "\n",
    "    if CODE_ENV=='aws':\n",
    "        model.save(src_dir + 'models/' + model_name + '.h5')\n",
    "\n",
    "\n",
    "def load_model(model_name, src_dir):\n",
    "    # Start from here if you want to load the model\n",
    "    # Load the model from a specified directory\n",
    "    if CODE_ENV=='local':\n",
    "        model = load_model(src_dir + 'models/' + model_name + '.h5', custom_objects={'rmse': rmse})\n",
    "\n",
    "    if CODE_ENV=='kaggle':\n",
    "        model = load_model('/kaggle/input/v1-model/' + model_name + '.h5', custom_objects={'rmse': rmse})\n",
    "\n",
    "    if CODE_ENV=='aws':\n",
    "        model = load_model(src_dir + 'models/' + model_name + '.h5', custom_objects={'rmse': rmse})\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_bsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
